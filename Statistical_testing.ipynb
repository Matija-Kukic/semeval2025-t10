{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8769cd0-b67e-4b86-bb95-7bc07a9146c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkukic/projektR2025/merged_data/subtask1.parquet\n",
      "/home/mkukic/projektR2025/merged_data/subtask1_test.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "wd = Path.cwd()\n",
    "wd = wd / 'merged_data'\n",
    "sub1 = str(wd) + '/subtask1.parquet'\n",
    "sub2 = str(wd) + '/subtask1_test.parquet'\n",
    "print(sub1)\n",
    "print(sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6283a1-273c-423e-a2c8-1de569308f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(sub1)\n",
    "df2 = pd.read_parquet(sub2)\n",
    "#print(df2.tail())\n",
    "#print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2622a0-0f60-41c8-9236-48f522240c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df2], ignore_index=True)\n",
    "#df = df[:100]\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9459f45a-0b1d-4709-b2e6-61eb8f3c2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def labelNum(row):\n",
    "    if row['class1'] == 'Antagonist':\n",
    "        return int(0)\n",
    "    if row['class1'] == 'Innocent':\n",
    "        return int(1)\n",
    "    if row['class1'] == 'Protagonist':\n",
    "        return int(2)\n",
    "def cleanText(row):\n",
    "    text = str(row['text'])\n",
    "    #text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('\\n',' ').replace('  ', ' ')\n",
    "    return text\n",
    "df['label1'] = df.apply(labelNum,axis=1)\n",
    "df['input'] = df.apply(cleanText,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3ec00e-32dd-4905-8ab2-88c8034b68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelNum2(row):\n",
    "    labels2 = [0 for _ in range(22)]\n",
    "    if row['label1'] == 2:\n",
    "        #labels2 = [0 for _ in range(6)]\n",
    "        if 'Guardian' in row['classes2']:\n",
    "            labels2[0] = 1\n",
    "        if 'Martyr' in row['classes2']:\n",
    "            labels2[1] = 1\n",
    "        if 'Peacemaker' in row['classes2']:\n",
    "            labels2[2] = 1\n",
    "        if 'Rebel' in row['classes2']:\n",
    "            labels2[3] = 1\n",
    "        if 'Underdog' in row['classes2']:\n",
    "            labels2[4] = 1\n",
    "        if 'Virtuous' in row['classes2']:\n",
    "            labels2[5] = 1\n",
    "    elif row['label1'] == 0:\n",
    "        #labels2 = [0 for _ in range(12)]\n",
    "        if 'Instigator' in row['classes2']:\n",
    "           labels2[6] = 1\n",
    "        if 'Conspirator' in row['classes2']:\n",
    "            labels2[7] = 1\n",
    "        if 'Tyrant' in row['classes2']:\n",
    "            labels2[8] = 1\n",
    "        if  'Foreign Adversary' in row['classes2']:\n",
    "            labels2[9] = 1\n",
    "        if 'Traitor' in row['classes2']:\n",
    "            labels2[10] = 1\n",
    "        if 'Spy' in row['classes2']:\n",
    "            labels2[11] = 1\n",
    "        if 'Saboteur' in row['classes2']:\n",
    "            labels2[12] = 1\n",
    "        if 'Corrupt' in row['classes2']:\n",
    "            labels2[13] = 1\n",
    "        if 'Incompetent' in row['classes2']:\n",
    "            labels2[14] = 1\n",
    "        if 'Terrorist' in row['classes2']:\n",
    "            labels2[15] = 1\n",
    "        if 'Deceiver' in row['classes2']:\n",
    "            labels2[16] = 1\n",
    "        if 'Bigot' in row['classes2']:\n",
    "            labels2[17] = 1\n",
    "    elif row['label1'] == 1:\n",
    "        #labels2 = [0 for _ in range(4)]\n",
    "        if 'Forgotten' in row['classes2']:\n",
    "            labels2[18] = 1\n",
    "        if 'Exploited' in row['classes2']:\n",
    "            labels2[19] = 1\n",
    "        if 'Victim' in row['classes2']:\n",
    "            labels2[20] = 1\n",
    "        if 'Scapegoat' in row['classes2']:\n",
    "            labels2[21] = 1\n",
    "    return labels2\n",
    "\n",
    "df['label2'] = df.apply(labelNum2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7330538d-a45f-4bea-9264-394f0181935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_substring_start_end(text, substring):\n",
    "    # Use re.finditer to find all occurrences of the substring in the text\n",
    "    matches = re.finditer(re.escape(substring), text)\n",
    "\n",
    "    # Collect the start and end indices of all matches\n",
    "    positions = [(match.start(), match.end()) for match in matches]\n",
    "\n",
    "    return positions\n",
    "def adjust_start_end(row):\n",
    "    org_text,cl_text,start,end,entity = str(row['text']),str(row['input']),int(row['start']),int(row['end']),str(row['entity'])\n",
    "    ss1 = find_all_substring_start_end(org_text,entity)\n",
    "    ss2 = find_all_substring_start_end(cl_text,entity)\n",
    "    #print(ss1,ss2)\n",
    "    #print(row['text'][start:end])\n",
    "    a = 0\n",
    "    for i in range(len(ss1)):\n",
    "        if abs((ss1[i][0] - start) + (ss1[i][1] - end) ) <= 2:\n",
    "            a = i\n",
    "            break\n",
    "    if org_text[ss1[a][0]:ss1[a][1]] != cl_text[ss2[a][0]:ss2[a][1]]:\n",
    "        print(\"ERROR!\")\n",
    "    return ss2[a][0],ss2[a][1]\n",
    "df['new_start_end'] = df.apply(adjust_start_end,axis=1)\n",
    "#print(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e31d8de-9ff2-4f09-877e-7f34c29e4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTokensToInput(row):\n",
    "    inp = row['input']\n",
    "    start,end = row['new_start_end']\n",
    "    #print(start,end)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    token_input = inp[:start] + \"[SPAN_START] \" + inp[start:end] + \" [SPAN_END]\" + inp[end:]\n",
    "    return token_input\n",
    "\n",
    "df['span_input'] = df.apply(addTokensToInput,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40df3dc2-5476-4a40-8578-2e631b2cd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upStartEnd(row):\n",
    "    start,end = row['new_start_end']\n",
    "    start += len(\"[SPAN_START] \")\n",
    "    end += len(\"[SPAN_START] \")\n",
    "    return start,end\n",
    "\n",
    "df['new_start_end'] = df.apply(upStartEnd,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25703182-333e-41d4-b7af-bfdbc1b8bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkukic/.conda/envs/projektR2025/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=22,problem_type=\"multi_label_classification\").to(device)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['span_input'], padding=True, truncation=True,max_length=8192,return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b024895a-e5bc-4588-b223-895d8a8ceb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(250004, 768, padding_idx=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTokens = {\n",
    "    \"additional_special_tokens\": [\"[SPAN_START]\", \"[SPAN_END]\"]\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(extraTokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a51b1b8-9599-4e30-94b3-5a453be626e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[ : , ['span_input', 'label1', 'label2', 'new_start_end', 'entity']]\n",
    "data['tokenized']=data.apply(preprocess_function,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "722817fd-e02b-486a-8246-35a344a8c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes(row):\n",
    "    off_mask = row['tokenized']['offset_mapping']\n",
    "    start,end = row['new_start_end'][0],row['new_start_end'][1]\n",
    "    inds = list()\n",
    "    for p in range(len(off_mask)):\n",
    "        if off_mask[p][0] >= start and off_mask[p][1] <= end:\n",
    "            if p != len(off_mask)-1:\n",
    "                inds.append(p)\n",
    "    #if len(inds) > 1:\n",
    "        #print(\"GREATER THAN 1\")\n",
    "    if len(inds) == 0:\n",
    "        print(start,end)\n",
    "    return inds\n",
    "data['indexes'] = data.apply(indexes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d03ed1-aaf3-47f0-93e1-4b6649556534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3506 3506 3506\n"
     ]
    }
   ],
   "source": [
    "data['list'] = data['tokenized'].apply(lambda x: x['input_ids'])\n",
    "data['attention'] = data['tokenized'].apply(lambda x: x['attention_mask'])\n",
    "ids = data['list']\n",
    "att = data['attention']\n",
    "indexes = data['indexes']\n",
    "tids = list()\n",
    "tatt = list()\n",
    "print(len(ids),len(att),len(indexes))\n",
    "for i in range(len(ids)):\n",
    "    tids.append(torch.tensor(ids[i]))\n",
    "    tatt.append(torch.tensor(att[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d4f2a3-5476-4ae6-946b-92f5067bb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_ids = list()\n",
    "sliced_ntids = list()\n",
    "sliced_att = list()\n",
    "key_inds = list()\n",
    "key_ids = list()\n",
    "\n",
    "def slices(index,size,context_size):\n",
    "    if (size<context_size):\n",
    "        return 0,size\n",
    "    lower_c = int(context_size/2-1)\n",
    "    upper_c = int(context_size/2)\n",
    "    #print(lower_c,upper_c)\n",
    "    if index < lower_c:\n",
    "        return 0,context_size\n",
    "    elif index >= lower_c:\n",
    "        if index + upper_c > size:\n",
    "            return index-(context_size-(size-index)), size\n",
    "        else:\n",
    "            return index-lower_c,index+upper_c+1\n",
    "\n",
    "\n",
    "for i in range(len(tids)):\n",
    "    slower,supper = slices(indexes[i][0],len(tids[i]),510)\n",
    "    #key_tid = tids[i][indexes[i][0]]\n",
    "    pid = ids[i][slower:supper]\n",
    "    key_inds.append([])\n",
    "    for j in indexes[i]:\n",
    "        key_id = ids[i][j]\n",
    "        if key_id not in pid:\n",
    "           print(len(ids[i]),key_id,slower,supper,indexes[i])\n",
    "        key_inds[i].append(pid.index(key_id))\n",
    "    apid = tids[i][slower:supper]\n",
    "    apatt = tatt[i][slower:supper]\n",
    "    if 0 not in pid:\n",
    "        apid = torch.cat((torch.tensor([0]),apid),dim=0)\n",
    "        apatt = torch.cat((torch.tensor([1]),apatt),dim=0)\n",
    "    if 2 not in pid:\n",
    "        apid = torch.cat((apid,torch.tensor([2])),dim=0)\n",
    "        apatt = torch.cat((apatt,torch.tensor([1])),dim=0)\n",
    "    sliced_ids.append(apid)\n",
    "    sliced_att.append(apatt)\n",
    "\n",
    "Min = 10000\n",
    "Max = 0\n",
    "ind2 = 0\n",
    "for i in range(len(indexes)):\n",
    "    if len(sliced_ids[i]) < Min:\n",
    "        Min = len(sliced_ids[i])\n",
    "        ind2 = i\n",
    "\n",
    "    if len(sliced_ids[i]) > Max:\n",
    "        Max = len(sliced_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d4515f7-ef53-45e1-a2d9-7cac1f14d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_ids = list()\n",
    "att_mask = list()\n",
    "for ten,att in zip(sliced_ids,sliced_att):\n",
    "    if len(ten) < 512:\n",
    "        padding_length = 512 - len(ten)\n",
    "        padding_tensor = torch.full((padding_length,), tokenizer.pad_token_id, dtype=ten.dtype)\n",
    "        padding_tensor2 = torch.full((padding_length,), 0, dtype=att.dtype)\n",
    "        ten = torch.cat((ten,padding_tensor),dim=0)\n",
    "        att = torch.cat((att,padding_tensor2),dim=0)\n",
    "    input_ids.append(ten)\n",
    "    att_mask.append(att)\n",
    "inputIds = torch.stack(input_ids)\n",
    "attMask = torch.stack(att_mask)\n",
    "\n",
    "inputIds_np = inputIds.numpy()\n",
    "attMask_np = attMask.numpy()\n",
    "y1 = data['label1'].values\n",
    "y2 = data['label2'].values\n",
    "lang = df['lang'].tolist()\n",
    "lang = np.array(lang)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f675bcd-7cd7-4461-bc5c-a8e29d2b8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_ids, X_test_ids, X_train_mask, X_test_mask, y1_train, y1_test, y2_train, y2_test,lang_train,lang_test = train_test_split(\n",
    "#    inputIds_np, attMask_np, y1, y2,lang, test_size=0.2, random_state=42, shuffle=True\n",
    "#)\n",
    "\n",
    "import numpy as np\n",
    "#y2_train = np.array(y2_train.tolist(), dtype=np.int8)\n",
    "#y2_test = np.array(y2_test.tolist(), dtype=np.int8)\n",
    "\n",
    "#X_train_ids = torch.tensor(X_train_ids, dtype=torch.long).to(device)\n",
    "#X_test_ids = torch.tensor(X_test_ids, dtype=torch.long).to(device)\n",
    "#X_train_mask = torch.tensor(X_train_mask, dtype=torch.long).to(device)\n",
    "#X_test_mask = torch.tensor(X_test_mask, dtype=torch.long).to(device)\n",
    "#y1_train = torch.tensor(y1_train, dtype=torch.long).to(device)\n",
    "#y1_test = torch.tensor(y1_test, dtype=torch.long).to(device)\n",
    "#y2_train = torch.tensor(y2_train, dtype=torch.long).to(device)\n",
    "#y2_test = torch.tensor(y2_test, dtype=torch.long).to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#train_dataset = TensorDataset(X_train_ids, X_train_mask, y1_train, y2_train)\n",
    "#test_dataset = TensorDataset(X_test_ids, X_test_mask, y1_test, y2_test )\n",
    "\n",
    "# Create DataLoaders\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True) #shuffle=True provides data shuffle for batches in different epochs\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323f76be-59cb-4e38-b2b3-2567a3acce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class HierarchicalNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_parent_classes, num_subcategory_classes,hidden_size):\n",
    "        super(HierarchicalNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "\n",
    "        # Parent class output head\n",
    "        self.parent_fc = nn.Linear(hidden_size, num_parent_classes)\n",
    "\n",
    "        # Subcategory output head (conditional on parent class)\n",
    "        self.subcategory_fc = nn.Linear(hidden_size, num_subcategory_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gelu = nn.GELU()\n",
    "        x = self.fc1(x)\n",
    "        x = gelu(x)\n",
    "\n",
    "        #parent_output = self.parent_fc(x)  # Parent class logits\n",
    "        subcategory_output = self.subcategory_fc(x)  # Subcategory logits\n",
    "\n",
    "        return subcategory_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb219243-9bd4-44fa-b1e8-e05cb584ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#classifier = nn.Linear(model.config.hidden_size * 2, 22).to(device)\n",
    "#classifier = HierarchicalNN(model.config.hidden_size * 2,3,12, model.config.hidden_size * 2).to(device)\n",
    "#optimizer = AdamW([\n",
    "#    {'params': model.parameters(),'lr':2e-5},  # Lower learning rate for XLM-RoBERTa\n",
    "#    {'params': classifier.parameters(),'lr':1e-3}     # Higher learning rate for the classifier\n",
    "#])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c5ef570-4c0e-411a-9109-9e7e09c0648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(labels,parent,mask):\n",
    "    \n",
    "    # Create an empty tensor to store the results\n",
    "    result = labels.clone()\n",
    "\n",
    "    # Loop through the batch and apply the corresponding tensor from result_dict\n",
    "    for i in range(labels.shape[0]):\n",
    "        idx = parent[i].item()  # Get the index (0, 1, or 2)\n",
    "        mask2 = mask[idx]  # Apply the corresponding tensor from result_dict\n",
    "\n",
    "        result[i][~mask2] = 0 \n",
    "\n",
    "    return result\n",
    "mask = {}\n",
    "mask[2] = torch.cat([torch.zeros(6, dtype=torch.bool), torch.ones(16, dtype=torch.bool)])\n",
    "mask[0] = torch.cat([torch.ones(6, dtype=torch.bool), torch.zeros(12, dtype=torch.bool), torch.ones(4, dtype=torch.bool)])\n",
    "mask[1] = torch.cat([torch.ones(18, dtype=torch.bool), torch.zeros(4, dtype=torch.bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93b62a36-25ff-499c-8a54-78d3d2741f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the confusion matrix in the end\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "final_preds = np.empty((0, 12), dtype=np.int8)\n",
    "final_labels = np.empty((0, 12), dtype=np.int8)\n",
    "span_start_token_id = tokenizer.convert_tokens_to_ids('[SPAN_START]')\n",
    "span_end_token_id = tokenizer.convert_tokens_to_ids('[SPAN_END]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "995c22ec-5c04-429e-8b86-cae5ecf79812",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_model = []\n",
    "accuracies_baseline =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bac3f78-7e5c-4799-b3b9-648fdc08acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396951af-bf06-4975-823f-77627ed72eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing seed: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:24<00:00,  1.84s/it, loss=0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "Training loss: 0.7442, Training accuracy: 0.1205\n",
      "Train Micro Precision: 0.2678, Recall: 0.1633, F1: 0.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6810, Test accuracy: 0.2322\n",
      "Test Micro Precision: 0.5378, Recall: 0.3096, F1: 0.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:25<00:00,  1.85s/it, loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6791, Training accuracy: 0.2981\n",
      "Train Micro Precision: 0.5387, Recall: 0.4052, F1: 0.4625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6644, Test accuracy: 0.3091\n",
      "Test Micro Precision: 0.5564, Recall: 0.4032, F1: 0.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:25<00:00,  1.85s/it, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "Training loss: 0.6424, Training accuracy: 0.4112\n",
      "Train Micro Precision: 0.5864, Recall: 0.5964, F1: 0.5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6736, Test accuracy: 0.3561\n",
      "Test Micro Precision: 0.4980, Recall: 0.4848, F1: 0.4913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "Training loss: 0.6149, Training accuracy: 0.5185\n",
      "Train Micro Precision: 0.6635, Recall: 0.7321, F1: 0.6961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6903, Test accuracy: 0.3604\n",
      "Test Micro Precision: 0.4987, Recall: 0.4993, F1: 0.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "Training loss: 0.5925, Training accuracy: 0.6216\n",
      "Train Micro Precision: 0.7265, Recall: 0.8203, F1: 0.7706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7011, Test accuracy: 0.4003\n",
      "Test Micro Precision: 0.5397, Recall: 0.5191, F1: 0.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "Training loss: 0.5758, Training accuracy: 0.7357\n",
      "Train Micro Precision: 0.8072, Recall: 0.8947, F1: 0.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7127, Test accuracy: 0.3675\n",
      "Test Micro Precision: 0.5007, Recall: 0.4717, F1: 0.4858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "Training loss: 0.5641, Training accuracy: 0.8074\n",
      "Train Micro Precision: 0.8601, Recall: 0.9289, F1: 0.8932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.11it/s, loss=0.919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7591, Test accuracy: 0.4274\n",
      "Test Micro Precision: 0.5159, Recall: 0.5125, F1: 0.5142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5587, Training accuracy: 0.8509\n",
      "Train Micro Precision: 0.8896, Recall: 0.9493, F1: 0.9185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8012, Test accuracy: 0.4259\n",
      "Test Micro Precision: 0.5136, Recall: 0.5217, F1: 0.5176\n",
      "Testing seed: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "Training loss: 0.7450, Training accuracy: 0.1059\n",
      "Train Micro Precision: 0.2484, Recall: 0.1524, F1: 0.1889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6924, Test accuracy: 0.1880\n",
      "Test Micro Precision: 0.7143, Recall: 0.2026, F1: 0.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6821, Training accuracy: 0.2650\n",
      "Train Micro Precision: 0.5301, Recall: 0.3661, F1: 0.4331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6828, Test accuracy: 0.3219\n",
      "Test Micro Precision: 0.4947, Recall: 0.4301, F1: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "Training loss: 0.6428, Training accuracy: 0.3848\n",
      "Train Micro Precision: 0.5764, Recall: 0.5712, F1: 0.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6884, Test accuracy: 0.3547\n",
      "Test Micro Precision: 0.4966, Recall: 0.4784, F1: 0.4874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "Training loss: 0.5930, Training accuracy: 0.6227\n",
      "Train Micro Precision: 0.7320, Recall: 0.8163, F1: 0.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7259, Test accuracy: 0.4046\n",
      "Test Micro Precision: 0.5127, Recall: 0.5268, F1: 0.5197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "Training loss: 0.5723, Training accuracy: 0.7304\n",
      "Train Micro Precision: 0.8034, Recall: 0.8826, F1: 0.8411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7871, Test accuracy: 0.4174\n",
      "Test Micro Precision: 0.5592, Recall: 0.4693, F1: 0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "Training loss: 0.5591, Training accuracy: 0.8160\n",
      "Train Micro Precision: 0.8664, Recall: 0.9304, F1: 0.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8005, Test accuracy: 0.3932\n",
      "Test Micro Precision: 0.5184, Recall: 0.4967, F1: 0.5073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5521, Training accuracy: 0.8734\n",
      "Train Micro Precision: 0.9052, Recall: 0.9542, F1: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8176, Test accuracy: 0.3960\n",
      "Test Micro Precision: 0.5267, Recall: 0.4771, F1: 0.5007\n",
      "Testing seed: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "Training loss: 0.7466, Training accuracy: 0.1248\n",
      "Train Micro Precision: 0.2553, Recall: 0.1590, F1: 0.1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6799, Test accuracy: 0.1952\n",
      "Test Micro Precision: 0.5765, Recall: 0.2737, F1: 0.3712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6791, Training accuracy: 0.2964\n",
      "Train Micro Precision: 0.5465, Recall: 0.3906, F1: 0.4556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6689, Test accuracy: 0.3034\n",
      "Test Micro Precision: 0.5390, Recall: 0.3761, F1: 0.4431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "Training loss: 0.6156, Training accuracy: 0.5392\n",
      "Train Micro Precision: 0.6714, Recall: 0.7250, F1: 0.6972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6778, Test accuracy: 0.3647\n",
      "Test Micro Precision: 0.5392, Recall: 0.4812, F1: 0.5086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "Training loss: 0.5885, Training accuracy: 0.6558\n",
      "Train Micro Precision: 0.7568, Recall: 0.8410, F1: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7081, Test accuracy: 0.3846\n",
      "Test Micro Precision: 0.5204, Recall: 0.5136, F1: 0.5170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "Training loss: 0.5736, Training accuracy: 0.7432\n",
      "Train Micro Precision: 0.8143, Recall: 0.8913, F1: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7297, Test accuracy: 0.3932\n",
      "Test Micro Precision: 0.5130, Recall: 0.5136, F1: 0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5557, Training accuracy: 0.8598\n",
      "Train Micro Precision: 0.9020, Recall: 0.9458, F1: 0.9234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7819, Test accuracy: 0.3789\n",
      "Test Micro Precision: 0.5218, Recall: 0.4812, F1: 0.5007\n",
      "Testing seed: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "Training loss: 0.7570, Training accuracy: 0.0599\n",
      "Train Micro Precision: 0.1896, Recall: 0.0866, F1: 0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7034, Test accuracy: 0.1567\n",
      "Test Micro Precision: 0.7053, Recall: 0.1761, F1: 0.2818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6962, Training accuracy: 0.2175\n",
      "Train Micro Precision: 0.5330, Recall: 0.2875, F1: 0.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6826, Test accuracy: 0.2749\n",
      "Test Micro Precision: 0.5784, Recall: 0.3443, F1: 0.4316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "Training loss: 0.6569, Training accuracy: 0.3463\n",
      "Train Micro Precision: 0.5665, Recall: 0.4895, F1: 0.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6894, Test accuracy: 0.3476\n",
      "Test Micro Precision: 0.5748, Recall: 0.3837, F1: 0.4602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "Training loss: 0.6339, Training accuracy: 0.4269\n",
      "Train Micro Precision: 0.6039, Recall: 0.6163, F1: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6816, Test accuracy: 0.3276\n",
      "Test Micro Precision: 0.4855, Recall: 0.4612, F1: 0.4730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "Training loss: 0.6099, Training accuracy: 0.5407\n",
      "Train Micro Precision: 0.6795, Recall: 0.7395, F1: 0.7082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6991, Test accuracy: 0.3462\n",
      "Test Micro Precision: 0.4680, Recall: 0.4704, F1: 0.4692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "Training loss: 0.5737, Training accuracy: 0.7322\n",
      "Train Micro Precision: 0.8106, Recall: 0.8893, F1: 0.8481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7420, Test accuracy: 0.3832\n",
      "Test Micro Precision: 0.5163, Recall: 0.5191, F1: 0.5177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5586, Training accuracy: 0.8270\n",
      "Train Micro Precision: 0.8695, Recall: 0.9391, F1: 0.9029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7845, Test accuracy: 0.4288\n",
      "Test Micro Precision: 0.5295, Recall: 0.5191, F1: 0.5242\n",
      "Testing seed: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "Training loss: 0.7403, Training accuracy: 0.1377\n",
      "Train Micro Precision: 0.2748, Recall: 0.1829, F1: 0.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7004, Test accuracy: 0.1752\n",
      "Test Micro Precision: 0.4526, Recall: 0.3112, F1: 0.3688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6726, Training accuracy: 0.2971\n",
      "Train Micro Precision: 0.5329, Recall: 0.4154, F1: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6917, Test accuracy: 0.2778\n",
      "Test Micro Precision: 0.4935, Recall: 0.3524, F1: 0.4112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "Training loss: 0.6379, Training accuracy: 0.4290\n",
      "Train Micro Precision: 0.6069, Recall: 0.6154, F1: 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6899, Test accuracy: 0.3191\n",
      "Test Micro Precision: 0.4791, Recall: 0.4561, F1: 0.4673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "Training loss: 0.6104, Training accuracy: 0.5253\n",
      "Train Micro Precision: 0.6654, Recall: 0.7346, F1: 0.6983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7001, Test accuracy: 0.3462\n",
      "Test Micro Precision: 0.4861, Recall: 0.4894, F1: 0.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7498, Test accuracy: 0.3704\n",
      "Test Micro Precision: 0.4859, Recall: 0.5027, F1: 0.4941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "Training loss: 0.5697, Training accuracy: 0.7447\n",
      "Train Micro Precision: 0.8175, Recall: 0.8943, F1: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7768, Test accuracy: 0.3675\n",
      "Test Micro Precision: 0.4938, Recall: 0.4761, F1: 0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "Training loss: 0.5613, Training accuracy: 0.8103\n",
      "Train Micro Precision: 0.8617, Recall: 0.9310, F1: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7859, Test accuracy: 0.3946\n",
      "Test Micro Precision: 0.5244, Recall: 0.4867, F1: 0.5048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5538, Training accuracy: 0.8652\n",
      "Train Micro Precision: 0.9041, Recall: 0.9471, F1: 0.9251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8165, Test accuracy: 0.3889\n",
      "Test Micro Precision: 0.5007, Recall: 0.4694, F1: 0.4846\n",
      "Testing seed: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:25<00:00,  1.85s/it, loss=0.583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6760, Training accuracy: 0.2974\n",
      "Train Micro Precision: 0.5585, Recall: 0.4018, F1: 0.4674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6815, Test accuracy: 0.3177\n",
      "Test Micro Precision: 0.5124, Recall: 0.4297, F1: 0.4674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "Training loss: 0.6417, Training accuracy: 0.4019\n",
      "Train Micro Precision: 0.5835, Recall: 0.5735, F1: 0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6927, Test accuracy: 0.3248\n",
      "Test Micro Precision: 0.4416, Recall: 0.4870, F1: 0.4632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "Training loss: 0.6135, Training accuracy: 0.5107\n",
      "Train Micro Precision: 0.6554, Recall: 0.7134, F1: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6837, Test accuracy: 0.3476\n",
      "Test Micro Precision: 0.5340, Recall: 0.4701, F1: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "Training loss: 0.5893, Training accuracy: 0.6262\n",
      "Train Micro Precision: 0.7357, Recall: 0.8300, F1: 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7260, Test accuracy: 0.3789\n",
      "Test Micro Precision: 0.5012, Recall: 0.5404, F1: 0.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "Training loss: 0.5736, Training accuracy: 0.7268\n",
      "Train Micro Precision: 0.8009, Recall: 0.8897, F1: 0.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7566, Test accuracy: 0.3604\n",
      "Test Micro Precision: 0.5213, Recall: 0.4935, F1: 0.5070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "Training loss: 0.5630, Training accuracy: 0.8017\n",
      "Train Micro Precision: 0.8560, Recall: 0.9227, F1: 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7543, Test accuracy: 0.3490\n",
      "Test Micro Precision: 0.4907, Recall: 0.4792, F1: 0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5562, Training accuracy: 0.8538\n",
      "Train Micro Precision: 0.8925, Recall: 0.9455, F1: 0.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:20<00:00,  2.10it/s, loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7596, Test accuracy: 0.3889\n",
      "Test Micro Precision: 0.5201, Recall: 0.4896, F1: 0.5044\n",
      "Testing seed: 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "Training loss: 0.7401, Training accuracy: 0.1252\n",
      "Train Micro Precision: 0.2894, Recall: 0.1759, F1: 0.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6897, Test accuracy: 0.2293\n",
      "Test Micro Precision: 0.5085, Recall: 0.3548, F1: 0.4180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.85s/it, loss=0.741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "Training loss: 0.6764, Training accuracy: 0.2860\n",
      "Train Micro Precision: 0.5419, Recall: 0.4009, F1: 0.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6822, Test accuracy: 0.3305\n",
      "Test Micro Precision: 0.5507, Recall: 0.4350, F1: 0.4860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "Training loss: 0.6412, Training accuracy: 0.3937\n",
      "Train Micro Precision: 0.5781, Recall: 0.5840, F1: 0.5810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6805, Test accuracy: 0.3248\n",
      "Test Micro Precision: 0.4765, Recall: 0.5204, F1: 0.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "Training loss: 0.6146, Training accuracy: 0.5150\n",
      "Train Micro Precision: 0.6544, Recall: 0.7266, F1: 0.6886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6840, Test accuracy: 0.3590\n",
      "Test Micro Precision: 0.5042, Recall: 0.5493, F1: 0.5258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "Training loss: 0.5906, Training accuracy: 0.6323\n",
      "Train Micro Precision: 0.7347, Recall: 0.8218, F1: 0.7758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7111, Test accuracy: 0.4117\n",
      "Test Micro Precision: 0.5507, Recall: 0.5348, F1: 0.5427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "Training loss: 0.5709, Training accuracy: 0.7414\n",
      "Train Micro Precision: 0.8144, Recall: 0.8933, F1: 0.8520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7207, Test accuracy: 0.4145\n",
      "Test Micro Precision: 0.5486, Recall: 0.5269, F1: 0.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "Training loss: 0.5664, Training accuracy: 0.7721\n",
      "Train Micro Precision: 0.8394, Recall: 0.9042, F1: 0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7585, Test accuracy: 0.4359\n",
      "Test Micro Precision: 0.5473, Recall: 0.5322, F1: 0.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [05:26<00:00,  1.86s/it, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "Training loss: 0.5555, Training accuracy: 0.8402\n",
      "Train Micro Precision: 0.8857, Recall: 0.9368, F1: 0.9105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:21<00:00,  2.09it/s, loss=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7544, Test accuracy: 0.4231\n",
      "Test Micro Precision: 0.5361, Recall: 0.4980, F1: 0.5163\n"
     ]
    }
   ],
   "source": [
    "seeds = [1,11,42,199,33,85,102]\n",
    "import random\n",
    "import os\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "for seed in seeds:\n",
    "    print(f\"Testing seed: {seed}\")\n",
    "    set_seed_everywhere(seed)\n",
    "\n",
    "    X_train_ids, X_test_ids, X_train_mask, X_test_mask, y1_train, y1_test, y2_train, y2_test,lang_train,lang_test = train_test_split(\n",
    "        inputIds_np, attMask_np, y1, y2,lang, test_size=0.2, random_state=seed, shuffle=True\n",
    "    )\n",
    "    \n",
    "    y2_train = np.array(y2_train.tolist(), dtype=np.int8)\n",
    "    y2_test = np.array(y2_test.tolist(), dtype=np.int8)\n",
    "    \n",
    "    X_train_ids = torch.tensor(X_train_ids, dtype=torch.long).to(device)\n",
    "    X_test_ids = torch.tensor(X_test_ids, dtype=torch.long).to(device)\n",
    "    X_train_mask = torch.tensor(X_train_mask, dtype=torch.long).to(device)\n",
    "    X_test_mask = torch.tensor(X_test_mask, dtype=torch.long).to(device)\n",
    "    y1_train = torch.tensor(y1_train, dtype=torch.long).to(device)\n",
    "    y1_test = torch.tensor(y1_test, dtype=torch.long).to(device)\n",
    "    y2_train = torch.tensor(y2_train, dtype=torch.long).to(device)\n",
    "    y2_test = torch.tensor(y2_test, dtype=torch.long).to(device)\n",
    "    \n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_ids, X_train_mask, y1_train, y2_train)\n",
    "    test_dataset = TensorDataset(X_test_ids, X_test_mask, y1_test, y2_test )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True) #shuffle=True provides data shuffle for batches in different epochs\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=22,problem_type=\"multi_label_classification\").to(device)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    classifier = HierarchicalNN(model.config.hidden_size * 2,3,22, model.config.hidden_size * 2).to(device)\n",
    "    optimizer = AdamW([\n",
    "        {'params': model.parameters(),'lr':2e-5},  # Lower learning rate for XLM-RoBERTa\n",
    "        {'params': classifier.parameters(),'lr':1e-3}     # Higher learning rate for the classifier\n",
    "    ])\n",
    "    num_epochs = 8\n",
    "    debug = 0\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        model.train()\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "        correct_parents = 0\n",
    "        total_parents = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "    \n",
    "        train_preds = np.empty((0, 22), dtype=np.int8)\n",
    "        train_labels = np.empty((0, 22), dtype=np.int8)\n",
    "        \n",
    "        train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        for batch in train_progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            parents = batch[2].to(device)\n",
    "            labels = batch[3].to(device)\n",
    "            batch_size = input_ids.size(0)\n",
    "    \n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels.float(), output_hidden_states=True)\n",
    "    \n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "    \n",
    "            entity_representations = []\n",
    "    \n",
    "            for i in range(batch_size):\n",
    "                ind_start = torch.nonzero(input_ids[i] == span_start_token_id).squeeze()\n",
    "                ind_end = torch.nonzero(input_ids[i] == span_end_token_id).squeeze()\n",
    "                start_ten = hidden_states[i,ind_start]\n",
    "                end_ten = hidden_states[i,ind_end]\n",
    "                #if debug == 0:\n",
    "                    #print (ind_start,ind_end)\n",
    "                    #print(start_ten.shape,end_ten.shape)\n",
    "                rep = torch.cat((hidden_states[i,ind_start],hidden_states[i,ind_end]),dim=0)\n",
    "                entity_representations.append(rep)\n",
    "            \n",
    "    \n",
    "            #entity_representations = []\n",
    "    \n",
    "            #start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
    "            #end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
    "    \n",
    "            # check that span is valid and has non-zero length\n",
    "            #valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
    "    \n",
    "            #valid_start_indices = start_indices[valid_spans]\n",
    "            #valid_end_indices = end_indices[valid_spans]\n",
    "    \n",
    "            \n",
    "            \n",
    "            # extract entity tokens for every sample in batch\n",
    "            #for i in range(batch_size):\n",
    "                #entity_tokens = hidden_states[i, valid_start_indices[i]]\n",
    "                #entity_representations.append(entity_tokens)\n",
    "            \n",
    "            #if epoch == 0:\n",
    "            #    print(entity_representations)\n",
    "            \n",
    "            entity_representations = torch.stack(entity_representations, dim=0)\n",
    "            \n",
    "            \n",
    "            #parent_log,\n",
    "            child_log = classifier(entity_representations)\n",
    "            child_log2 = apply_mask(child_log,parents,mask)\n",
    "            zero_ten = torch.zeros((input_ids.size(0), 22), dtype=torch.float32).to(device)\n",
    "            #if debug == 0:\n",
    "                #print(child_log,zero_ten,input_ids.size(0))\n",
    "                #print(entity_representations.shape)\n",
    "                #debug+=1\n",
    "            \n",
    "            #loss = criterion(parent_log, parents) + criterion2(child_log,labels.float()) + 2 * criterion2(child_log2,zero_ten) \n",
    "            loss = criterion2(child_log,labels.float()) + 2 * criterion2(child_log2,zero_ten)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = (torch.sigmoid(child_log) > 0.25).int()\n",
    "            train_preds = np.vstack([train_preds,preds.cpu().numpy()])\n",
    "            train_labels = np.vstack([train_labels,labels.cpu().numpy()])\n",
    "            correct_predictions += ((preds == labels.int()).all(dim=1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            #if debug == 0:\n",
    "                #print(parent_log,child_log,preds,labels)\n",
    "                #debug+=1\n",
    "            #preds_parents = torch.argmax(parent_log, dim=-1)\n",
    "            #correct_parents += (preds_parents == parents).sum().item()\n",
    "            #total_parents += labels.size(0)\n",
    "    \n",
    "            train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        #parent_train_acc = correct_parents / total_parents\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        #print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}, Parent Train acc: {parent_train_acc:.4f}\")\n",
    "        print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(train_labels, train_preds, average='micro')\n",
    "        print(f\"Train Micro Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        classifier.eval()\n",
    "        test_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        correct_parents = 0\n",
    "        total_parents = 0\n",
    "        test_progress_bar = tqdm(test_dataloader, desc=f\"Test Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "        test_preds = np.empty((0, 22), dtype=np.int8)\n",
    "        test_labels = np.empty((0, 22), dtype=np.int8)\n",
    "    \n",
    "        final_preds = np.empty((0, 22), dtype=np.int8)\n",
    "        final_labels = np.empty((0, 22), dtype=np.int8)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_progress_bar:\n",
    "    \n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                parents = batch[2].to(device)\n",
    "                labels = batch[3].to(device)\n",
    "    \n",
    "                batch_size = input_ids.size(0)\n",
    "    \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels.float(), output_hidden_states=True)\n",
    "    \n",
    "                hidden_states = outputs.hidden_states[-1]\n",
    "    \n",
    "                entity_representations = []\n",
    "    \n",
    "                for i in range(batch_size):\n",
    "                    ind_start = torch.nonzero(input_ids[i] == span_start_token_id).squeeze()\n",
    "                    ind_end = torch.nonzero(input_ids[i] == span_end_token_id).squeeze()\n",
    "                    start_ten = hidden_states[i,ind_start]\n",
    "                    end_ten = hidden_states[i,ind_end]\n",
    "                    rep = torch.cat((hidden_states[i,ind_start],hidden_states[i,ind_end]),dim=0)\n",
    "                    entity_representations.append(rep)\n",
    "                \n",
    "                #start_mask = (input_ids == span_start_token_id)\n",
    "                #end_mask = (input_ids == span_end_token_id)\n",
    "    \n",
    "                #entity_representations = []\n",
    "    \n",
    "                #start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
    "                #end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
    "    \n",
    "                #valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
    "    \n",
    "                #valid_start_indices = start_indices[valid_spans]\n",
    "                #valid_end_indices = end_indices[valid_spans]\n",
    "    \n",
    "                # extract entity tokens for every sample in batch\n",
    "                #for i in range(batch_size):\n",
    "                #    entity_tokens = hidden_states[i, valid_start_indices[i]]\n",
    "                #    entity_representations.append(entity_tokens)\n",
    "    \n",
    "                entity_representations = torch.stack(entity_representations, dim=0)\n",
    "    \n",
    "                #parent_log,\n",
    "                child_log = classifier(entity_representations)\n",
    "                child_log2 = apply_mask(child_log,parents,mask)\n",
    "                #loss = criterion(parent_log, parents) + criterion2(child_log,labels.float()) + criterion2(child_log2,labels.float()) \n",
    "                loss = criterion2(child_log,labels.float()) + 2 * criterion2(child_log2,labels.float()) \n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                preds = (torch.sigmoid(child_log) > 0.25).int()\n",
    "                correct_predictions += ((preds == labels.int()).all(dim=1)).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "    \n",
    "                test_preds = np.vstack([test_preds,preds.cpu().numpy()])\n",
    "                test_labels = np.vstack([test_labels,labels.cpu().numpy()])\n",
    "                \n",
    "                #preds_parents = torch.argmax(parent_log, dim=-1)\n",
    "                #correct_parents += (preds_parents == parents).sum().item()\n",
    "                #total_parents += labels.size(0)\n",
    "    \n",
    "                test_progress_bar.set_postfix({'loss': loss.item()})\n",
    "                \n",
    "                #final_preds = np.vstack([final_preds,preds.cpu().numpy()])\n",
    "                #final_labels = np.vstack([final_labels,labels.cpu().numpy()])\n",
    "            \n",
    "        \n",
    "        avg_test_loss = test_loss / len(test_dataloader)\n",
    "        test_accuracy = correct_predictions / total_predictions\n",
    "        #parent_test_accuracy = correct_parents / total_parents\n",
    "        #print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}, Parent Test accuracy: {parent_test_accuracy:.4f}\")\n",
    "        print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='micro')\n",
    "        print(f\"Test Micro Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        if epoch == num_epochs-1:\n",
    "            accuracies_model.append(test_accuracy)\n",
    "    del model\n",
    "    del classifier\n",
    "    del train_preds\n",
    "    del train_labels\n",
    "    del optimizer\n",
    "    \n",
    "    del X_train_ids \n",
    "    del X_test_ids \n",
    "    del X_train_mask \n",
    "    del X_test_mask \n",
    "    del y1_train \n",
    "    del y1_test \n",
    "    del y2_train \n",
    "    del y2_test \n",
    "    del train_dataset \n",
    "    del test_dataset\n",
    "    del train_dataloader \n",
    "    del test_dataloader\n",
    "    del loss \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3becdfdd-ab95-443d-bcf2-d156dd192b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42592592592592593, 0.396011396011396, 0.3789173789173789, 0.4287749287749288, 0.3888888888888889, 0.3888888888888889, 0.4230769230769231]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6433db56-212e-4a34-ad61-8c3b5c349b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open (or create) a file in write mode ('w')\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    # Write each item in the list to the file\n",
    "    for item in accuracies_model:\n",
    "        file.write(f\"{item}\\n\")  # Writing each item on a new line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
