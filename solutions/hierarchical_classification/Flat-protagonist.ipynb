{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346cf6f9-e23b-4cb5-8be9-566ffdbea71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# sub1 = 'drive/My Drive/Colab Notebooks/subtask1.parquet'\n",
    "# print(sub1)\n",
    "\n",
    "from pathlib import Path\n",
    "wd = Path.cwd()\n",
    "wd = wd.parent.parent\n",
    "wd = wd / 'merged_data'\n",
    "sub1 = str(wd) + '/subtask1.parquet'\n",
    "sub2 = str(wd) + '/description_roles.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c231079a-e2fd-4afd-bd12-5897624ad96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(sub1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef05b8f-22b8-4948-9f23-a852c71d31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def labelNum(row):\n",
    "    if row['class1'] == 'Antagonist':\n",
    "        return int(0)\n",
    "    if row['class1'] == 'Innocent':\n",
    "        return int(1)\n",
    "    if row['class1'] == 'Protagonist':\n",
    "        return int(2)\n",
    "def cleanText(row):\n",
    "    text = str(row['text'])\n",
    "    #text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('\\n',' ').replace('  ', ' ')\n",
    "    return text\n",
    "df['label1'] = df.apply(labelNum,axis=1)\n",
    "df['input'] = df.apply(cleanText,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e167979a-96ad-46fa-afa4-0afbfcc76568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelNum2(row):\n",
    "    l = 0\n",
    "    if row['label1'] == 2:\n",
    "        l = 6\n",
    "    if row['label1'] == 1:\n",
    "        l = 4\n",
    "    if row['label1'] == 0:\n",
    "        l = 12\n",
    "    labels2 = [0 for _ in range(l)]\n",
    "    if row['label1'] == 2:\n",
    "        #labels2 = [0 for _ in range(6)]\n",
    "        if 'Guardian' in row['classes2']:\n",
    "            labels2[0] = 1\n",
    "        if 'Martyr' in row['classes2']:\n",
    "            labels2[1] = 1\n",
    "        if 'Peacemaker' in row['classes2']:\n",
    "            labels2[2] = 1\n",
    "        if 'Rebel' in row['classes2']:\n",
    "            labels2[3] = 1\n",
    "        if 'Underdog' in row['classes2']:\n",
    "            labels2[4] = 1\n",
    "        if 'Virtuous' in row['classes2']:\n",
    "            labels2[5] = 1\n",
    "    elif row['label1'] == 0:\n",
    "        #labels2 = [0 for _ in range(12)]\n",
    "        if 'Instigator' in row['classes2']:\n",
    "           labels2[0] = 1\n",
    "        if 'Conspirator' in row['classes2']:\n",
    "            labels2[1] = 1\n",
    "        if 'Tyrant' in row['classes2']:\n",
    "            labels2[2] = 1\n",
    "        if  'Foreign Adversary' in row['classes2']:\n",
    "            labels2[3] = 1\n",
    "        if 'Traitor' in row['classes2']:\n",
    "            labels2[4] = 1\n",
    "        if 'Spy' in row['classes2']:\n",
    "            labels2[5] = 1\n",
    "        if 'Saboteur' in row['classes2']:\n",
    "            labels2[6] = 1\n",
    "        if 'Corrupt' in row['classes2']:\n",
    "            labels2[7] = 1\n",
    "        if 'Incompetent' in row['classes2']:\n",
    "            labels2[8] = 1\n",
    "        if 'Terrorist' in row['classes2']:\n",
    "            labels2[9] = 1\n",
    "        if 'Deceiver' in row['classes2']:\n",
    "            labels2[10] = 1\n",
    "        if 'Bigot' in row['classes2']:\n",
    "            labels2[11] = 1\n",
    "    elif row['label1'] == 1:\n",
    "        #labels2 = [0 for _ in range(4)]\n",
    "        if 'Forgotten' in row['classes2']:\n",
    "            labels2[0] = 1\n",
    "        if 'Exploited' in row['classes2']:\n",
    "            labels2[1] = 1\n",
    "        if 'Victim' in row['classes2']:\n",
    "            labels2[2] = 1\n",
    "        if 'Scapegoat' in row['classes2']:\n",
    "            labels2[3] = 1\n",
    "    return labels2\n",
    "\n",
    "df['label2'] = df.apply(labelNum2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505e7057-7be0-4193-a21c-67544e078b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang                                                            BG\n",
      "art_name                                                BG_670.txt\n",
      "entity                                                       Запад\n",
      "start                                                          152\n",
      "end                                                            156\n",
      "class1                                                  Antagonist\n",
      "classes2              [Conspirator, Instigator, Foreign Adversary]\n",
      "text             Опитът на колективния Запад да „обезкърви Руси...\n",
      "label1                                                           0\n",
      "input            Опитът на колективния Запад да „обезкърви Руси...\n",
      "label2                        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "new_start_end                                           (151, 156)\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def find_all_substring_start_end(text, substring):\n",
    "    # Use re.finditer to find all occurrences of the substring in the text\n",
    "    matches = re.finditer(re.escape(substring), text)\n",
    "\n",
    "    # Collect the start and end indices of all matches\n",
    "    positions = [(match.start(), match.end()) for match in matches]\n",
    "\n",
    "    return positions\n",
    "def adjust_start_end(row):\n",
    "    org_text,cl_text,start,end,entity = str(row['text']),str(row['input']),int(row['start']),int(row['end']),str(row['entity'])\n",
    "    ss1 = find_all_substring_start_end(org_text,entity)\n",
    "    ss2 = find_all_substring_start_end(cl_text,entity)\n",
    "    #print(ss1,ss2)\n",
    "    #print(row['text'][start:end])\n",
    "    a = 0\n",
    "    for i in range(len(ss1)):\n",
    "        if abs((ss1[i][0] - start) + (ss1[i][1] - end) ) <= 2:\n",
    "            a = i\n",
    "            break\n",
    "    if org_text[ss1[a][0]:ss1[a][1]] != cl_text[ss2[a][0]:ss2[a][1]]:\n",
    "        print(\"ERROR!\")\n",
    "    return ss2[a][0],ss2[a][1]\n",
    "df['new_start_end'] = df.apply(adjust_start_end,axis=1)\n",
    "print(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb65ffb1-bf03-4351-a877-6450d5af4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTokensToInput(row):\n",
    "    inp = row['input']\n",
    "    start,end = row['new_start_end']\n",
    "    #print(start,end)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    token_input = inp[:start] + \"[SPAN_START] \" + inp[start:end] + \" [SPAN_END]\" + inp[end:]\n",
    "    return token_input\n",
    "\n",
    "df['span_input'] = df.apply(addTokensToInput,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b9f935-754a-4eb4-903a-5bd9f58bdc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upStartEnd(row):\n",
    "    start,end = row['new_start_end']\n",
    "    start += len(\"[SPAN_START] \")\n",
    "    end += len(\"[SPAN_START] \")\n",
    "    return start,end\n",
    "\n",
    "df['new_start_end'] = df.apply(upStartEnd,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49093e86-e800-41f9-b0fe-de177bca7aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lang</th>\n",
       "      <th>art_name</th>\n",
       "      <th>entity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>class1</th>\n",
       "      <th>classes2</th>\n",
       "      <th>text</th>\n",
       "      <th>label1</th>\n",
       "      <th>input</th>\n",
       "      <th>label2</th>\n",
       "      <th>new_start_end</th>\n",
       "      <th>span_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_855.txt</td>\n",
       "      <td>Руската федерация</td>\n",
       "      <td>1453</td>\n",
       "      <td>1469</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Virtuous, Guardian]</td>\n",
       "      <td>Русия забрани разпространението на десетки мед...</td>\n",
       "      <td>2</td>\n",
       "      <td>Русия забрани разпространението на десетки мед...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1]</td>\n",
       "      <td>(1458, 1475)</td>\n",
       "      <td>Русия забрани разпространението на десетки мед...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_561.txt</td>\n",
       "      <td>Унгария</td>\n",
       "      <td>717</td>\n",
       "      <td>723</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Virtuous]</td>\n",
       "      <td>Вратички на Борел за дефрагментиране на Европа...</td>\n",
       "      <td>2</td>\n",
       "      <td>Вратички на Борел за дефрагментиране на Европа...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>(729, 736)</td>\n",
       "      <td>Вратички на Борел за дефрагментиране на Европа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_751.txt</td>\n",
       "      <td>Путин</td>\n",
       "      <td>345</td>\n",
       "      <td>349</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Virtuous]</td>\n",
       "      <td>US военен: Путин ни изигра така, както Рейгън ...</td>\n",
       "      <td>2</td>\n",
       "      <td>US военен: Путин ни изигра така, както Рейгън ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>(356, 361)</td>\n",
       "      <td>US военен: Путин ни изигра така, както Рейгън ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_751.txt</td>\n",
       "      <td>Путин</td>\n",
       "      <td>1797</td>\n",
       "      <td>1801</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker]</td>\n",
       "      <td>US военен: Путин ни изигра така, както Рейгън ...</td>\n",
       "      <td>2</td>\n",
       "      <td>US военен: Путин ни изигра така, както Рейгън ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>(24, 29)</td>\n",
       "      <td>US военен: [SPAN_START] Путин [SPAN_END] ни из...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_839.txt</td>\n",
       "      <td>Европа</td>\n",
       "      <td>517</td>\n",
       "      <td>522</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Guardian]</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "      <td>2</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>(529, 535)</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_839.txt</td>\n",
       "      <td>България</td>\n",
       "      <td>1091</td>\n",
       "      <td>1098</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Virtuous]</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "      <td>2</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>(1103, 1111)</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_839.txt</td>\n",
       "      <td>Володимир Зеленски</td>\n",
       "      <td>1138</td>\n",
       "      <td>1155</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker]</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "      <td>2</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>(1150, 1168)</td>\n",
       "      <td>Посланикът на САЩ: България вече не е лесна ми...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>BG</td>\n",
       "      <td>A9_BG_5143.txt</td>\n",
       "      <td>Киев</td>\n",
       "      <td>608</td>\n",
       "      <td>611</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Underdog]</td>\n",
       "      <td>La Vanguardia: Украйна се отказа от надеждите ...</td>\n",
       "      <td>2</td>\n",
       "      <td>La Vanguardia: Украйна се отказа от надеждите ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>(620, 624)</td>\n",
       "      <td>La Vanguardia: Украйна се отказа от надеждите ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_4819.txt</td>\n",
       "      <td>В. Зеленски</td>\n",
       "      <td>152</td>\n",
       "      <td>162</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker]</td>\n",
       "      <td>Украинският президент В. Зеленски заяви, че пл...</td>\n",
       "      <td>2</td>\n",
       "      <td>Украинският президент В. Зеленски заяви, че пл...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>(164, 175)</td>\n",
       "      <td>Украинският президент В. Зеленски заяви, че пл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_4819.txt</td>\n",
       "      <td>Украйна</td>\n",
       "      <td>662</td>\n",
       "      <td>668</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker]</td>\n",
       "      <td>Украинският президент В. Зеленски заяви, че пл...</td>\n",
       "      <td>2</td>\n",
       "      <td>Украинският президент В. Зеленски заяви, че пл...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>(674, 681)</td>\n",
       "      <td>Украинският президент В. Зеленски заяви, че пл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_704.txt</td>\n",
       "      <td>Президента на Руската Федерация В.В.Путин</td>\n",
       "      <td>1003</td>\n",
       "      <td>1043</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker]</td>\n",
       "      <td>Всеки път, размишлявайки върху териториалната ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Всеки път, размишлявайки върху териториалната ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>(1014, 1055)</td>\n",
       "      <td>Всеки път, размишлявайки върху териториалната ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_704.txt</td>\n",
       "      <td>Русия</td>\n",
       "      <td>1149</td>\n",
       "      <td>1153</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker]</td>\n",
       "      <td>Всеки път, размишлявайки върху териториалната ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Всеки път, размишлявайки върху териториалната ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>(1160, 1165)</td>\n",
       "      <td>Всеки път, размишлявайки върху териториалната ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>62</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_541.txt</td>\n",
       "      <td>Русия</td>\n",
       "      <td>248</td>\n",
       "      <td>252</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Rebel]</td>\n",
       "      <td>въоръжението на НАТО не е в състояние да се пр...</td>\n",
       "      <td>2</td>\n",
       "      <td>въоръжението на НАТО не е в състояние да се пр...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>(259, 264)</td>\n",
       "      <td>въоръжението на НАТО не е в състояние да се пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_894.txt</td>\n",
       "      <td>Джон Стосел</td>\n",
       "      <td>465</td>\n",
       "      <td>475</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Rebel]</td>\n",
       "      <td>Зелената сделка: Бедност, мизерия и по-малко х...</td>\n",
       "      <td>2</td>\n",
       "      <td>Зелената сделка: Бедност, мизерия и по-малко х...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>(477, 488)</td>\n",
       "      <td>Зелената сделка: Бедност, мизерия и по-малко х...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>73</td>\n",
       "      <td>BG</td>\n",
       "      <td>BG_3753.txt</td>\n",
       "      <td>Русия</td>\n",
       "      <td>1125</td>\n",
       "      <td>1129</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Guardian]</td>\n",
       "      <td>Не жалят даже старците: Украйна принудително м...</td>\n",
       "      <td>2</td>\n",
       "      <td>Не жалят даже старците: Украйна принудително м...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>(1137, 1142)</td>\n",
       "      <td>Не жалят даже старците: Украйна принудително м...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index lang        art_name                                     entity  \\\n",
       "0      13   BG      BG_855.txt                          Руската федерация   \n",
       "1      16   BG      BG_561.txt                                    Унгария   \n",
       "2      17   BG      BG_751.txt                                      Путин   \n",
       "3      20   BG      BG_751.txt                                      Путин   \n",
       "4      26   BG      BG_839.txt                                     Европа   \n",
       "5      27   BG      BG_839.txt                                   България   \n",
       "6      28   BG      BG_839.txt                         Володимир Зеленски   \n",
       "7      42   BG  A9_BG_5143.txt                                       Киев   \n",
       "8      43   BG     BG_4819.txt                                В. Зеленски   \n",
       "9      44   BG     BG_4819.txt                                    Украйна   \n",
       "10     52   BG      BG_704.txt  Президента на Руската Федерация В.В.Путин   \n",
       "11     53   BG      BG_704.txt                                      Русия   \n",
       "12     62   BG      BG_541.txt                                      Русия   \n",
       "13     64   BG      BG_894.txt                                Джон Стосел   \n",
       "14     73   BG     BG_3753.txt                                      Русия   \n",
       "\n",
       "   start   end       class1              classes2  \\\n",
       "0   1453  1469  Protagonist  [Virtuous, Guardian]   \n",
       "1    717   723  Protagonist            [Virtuous]   \n",
       "2    345   349  Protagonist            [Virtuous]   \n",
       "3   1797  1801  Protagonist          [Peacemaker]   \n",
       "4    517   522  Protagonist            [Guardian]   \n",
       "5   1091  1098  Protagonist            [Virtuous]   \n",
       "6   1138  1155  Protagonist          [Peacemaker]   \n",
       "7    608   611  Protagonist            [Underdog]   \n",
       "8    152   162  Protagonist          [Peacemaker]   \n",
       "9    662   668  Protagonist          [Peacemaker]   \n",
       "10  1003  1043  Protagonist          [Peacemaker]   \n",
       "11  1149  1153  Protagonist          [Peacemaker]   \n",
       "12   248   252  Protagonist               [Rebel]   \n",
       "13   465   475  Protagonist               [Rebel]   \n",
       "14  1125  1129  Protagonist            [Guardian]   \n",
       "\n",
       "                                                 text  label1  \\\n",
       "0   Русия забрани разпространението на десетки мед...       2   \n",
       "1   Вратички на Борел за дефрагментиране на Европа...       2   \n",
       "2   US военен: Путин ни изигра така, както Рейгън ...       2   \n",
       "3   US военен: Путин ни изигра така, както Рейгън ...       2   \n",
       "4   Посланикът на САЩ: България вече не е лесна ми...       2   \n",
       "5   Посланикът на САЩ: България вече не е лесна ми...       2   \n",
       "6   Посланикът на САЩ: България вече не е лесна ми...       2   \n",
       "7   La Vanguardia: Украйна се отказа от надеждите ...       2   \n",
       "8   Украинският президент В. Зеленски заяви, че пл...       2   \n",
       "9   Украинският президент В. Зеленски заяви, че пл...       2   \n",
       "10  Всеки път, размишлявайки върху териториалната ...       2   \n",
       "11  Всеки път, размишлявайки върху териториалната ...       2   \n",
       "12  въоръжението на НАТО не е в състояние да се пр...       2   \n",
       "13  Зелената сделка: Бедност, мизерия и по-малко х...       2   \n",
       "14  Не жалят даже старците: Украйна принудително м...       2   \n",
       "\n",
       "                                                input              label2  \\\n",
       "0   Русия забрани разпространението на десетки мед...  [1, 0, 0, 0, 0, 1]   \n",
       "1   Вратички на Борел за дефрагментиране на Европа...  [0, 0, 0, 0, 0, 1]   \n",
       "2   US военен: Путин ни изигра така, както Рейгън ...  [0, 0, 0, 0, 0, 1]   \n",
       "3   US военен: Путин ни изигра така, както Рейгън ...  [0, 0, 1, 0, 0, 0]   \n",
       "4   Посланикът на САЩ: България вече не е лесна ми...  [1, 0, 0, 0, 0, 0]   \n",
       "5   Посланикът на САЩ: България вече не е лесна ми...  [0, 0, 0, 0, 0, 1]   \n",
       "6   Посланикът на САЩ: България вече не е лесна ми...  [0, 0, 1, 0, 0, 0]   \n",
       "7   La Vanguardia: Украйна се отказа от надеждите ...  [0, 0, 0, 0, 1, 0]   \n",
       "8   Украинският президент В. Зеленски заяви, че пл...  [0, 0, 1, 0, 0, 0]   \n",
       "9   Украинският президент В. Зеленски заяви, че пл...  [0, 0, 1, 0, 0, 0]   \n",
       "10  Всеки път, размишлявайки върху териториалната ...  [0, 0, 1, 0, 0, 0]   \n",
       "11  Всеки път, размишлявайки върху териториалната ...  [0, 0, 1, 0, 0, 0]   \n",
       "12  въоръжението на НАТО не е в състояние да се пр...  [0, 0, 0, 1, 0, 0]   \n",
       "13  Зелената сделка: Бедност, мизерия и по-малко х...  [0, 0, 0, 1, 0, 0]   \n",
       "14  Не жалят даже старците: Украйна принудително м...  [1, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   new_start_end                                         span_input  \n",
       "0   (1458, 1475)  Русия забрани разпространението на десетки мед...  \n",
       "1     (729, 736)  Вратички на Борел за дефрагментиране на Европа...  \n",
       "2     (356, 361)  US военен: Путин ни изигра така, както Рейгън ...  \n",
       "3       (24, 29)  US военен: [SPAN_START] Путин [SPAN_END] ни из...  \n",
       "4     (529, 535)  Посланикът на САЩ: България вече не е лесна ми...  \n",
       "5   (1103, 1111)  Посланикът на САЩ: България вече не е лесна ми...  \n",
       "6   (1150, 1168)  Посланикът на САЩ: България вече не е лесна ми...  \n",
       "7     (620, 624)  La Vanguardia: Украйна се отказа от надеждите ...  \n",
       "8     (164, 175)  Украинският президент В. Зеленски заяви, че пл...  \n",
       "9     (674, 681)  Украинският президент В. Зеленски заяви, че пл...  \n",
       "10  (1014, 1055)  Всеки път, размишлявайки върху териториалната ...  \n",
       "11  (1160, 1165)  Всеки път, размишлявайки върху териториалната ...  \n",
       "12    (259, 264)  въоръжението на НАТО не е в състояние да се пр...  \n",
       "13    (477, 488)  Зелената сделка: Бедност, мизерия и по-малко х...  \n",
       "14  (1137, 1142)  Не жалят даже старците: Украйна принудително м...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[ df ['class1'] == 'Protagonist']\n",
    "df = df.reset_index()\n",
    "df.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db09b44d-17d8-4d64-afca-4d3b1fc0a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=6,problem_type=\"multi_label_classification\").to(device)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['span_input'], padding=True, truncation=True,max_length=8192,return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f74da3e-c3d1-4351-bd85-dc1e6dd13d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(250004, 768, padding_idx=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTokens = {\n",
    "    \"additional_special_tokens\": [\"[SPAN_START]\", \"[SPAN_END]\"]\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(extraTokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6994eaf3-9c5e-4ea3-ab62-02d87e8c9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[ : , ['span_input', 'label1', 'label2', 'new_start_end', 'entity']]\n",
    "data['tokenized']=data.apply(preprocess_function,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3baeaf-532a-4034-a4bc-27340e9acd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes(row):\n",
    "    off_mask = row['tokenized']['offset_mapping']\n",
    "    start,end = row['new_start_end'][0],row['new_start_end'][1]\n",
    "    inds = list()\n",
    "    for p in range(len(off_mask)):\n",
    "        if off_mask[p][0] >= start and off_mask[p][1] <= end:\n",
    "            if p != len(off_mask)-1:\n",
    "                inds.append(p)\n",
    "    #if len(inds) > 1:\n",
    "        #print(\"GREATER THAN 1\")\n",
    "    if len(inds) == 0:\n",
    "        print(start,end)\n",
    "    return inds\n",
    "data['indexes'] = data.apply(indexes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "922c77a8-2fbf-47b3-a752-a694a1df1342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852 852 852\n"
     ]
    }
   ],
   "source": [
    "data['list'] = data['tokenized'].apply(lambda x: x['input_ids'])\n",
    "data['attention'] = data['tokenized'].apply(lambda x: x['attention_mask'])\n",
    "ids = data['list']\n",
    "att = data['attention']\n",
    "indexes = data['indexes']\n",
    "tids = list()\n",
    "tatt = list()\n",
    "print(len(ids),len(att),len(indexes))\n",
    "#print(ids)\n",
    "for i in range(len(ids)):\n",
    "    #print(i)\n",
    "    tids.append(torch.tensor(ids[i]))\n",
    "    tatt.append(torch.tensor(att[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb706b74-9194-4609-89fd-8498559c2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_ids = list()\n",
    "sliced_ntids = list()\n",
    "sliced_att = list()\n",
    "key_inds = list()\n",
    "key_ids = list()\n",
    "\n",
    "def slices(index,size,context_size):\n",
    "    if (size<context_size):\n",
    "        return 0,size\n",
    "    lower_c = int(context_size/2-1)\n",
    "    upper_c = int(context_size/2)\n",
    "    #print(lower_c,upper_c)\n",
    "    if index < lower_c:\n",
    "        return 0,context_size\n",
    "    elif index >= lower_c:\n",
    "        if index + upper_c > size:\n",
    "            return index-(context_size-(size-index)), size\n",
    "        else:\n",
    "            return index-lower_c,index+upper_c+1\n",
    "\n",
    "\n",
    "for i in range(len(tids)):\n",
    "    slower,supper = slices(indexes[i][0],len(tids[i]),510)\n",
    "    #key_tid = tids[i][indexes[i][0]]\n",
    "    pid = ids[i][slower:supper]\n",
    "    key_inds.append([])\n",
    "    for j in indexes[i]:\n",
    "        key_id = ids[i][j]\n",
    "        if key_id not in pid:\n",
    "           print(len(ids[i]),key_id,slower,supper,indexes[i])\n",
    "        key_inds[i].append(pid.index(key_id))\n",
    "    apid = tids[i][slower:supper]\n",
    "    apatt = tatt[i][slower:supper]\n",
    "    if 0 not in pid:\n",
    "        apid = torch.cat((torch.tensor([0]),apid),dim=0)\n",
    "        apatt = torch.cat((torch.tensor([1]),apatt),dim=0)\n",
    "    if 2 not in pid:\n",
    "        apid = torch.cat((apid,torch.tensor([2])),dim=0)\n",
    "        apatt = torch.cat((apatt,torch.tensor([1])),dim=0)\n",
    "    sliced_ids.append(apid)\n",
    "    sliced_att.append(apatt)\n",
    "\n",
    "Min = 10000\n",
    "Max = 0\n",
    "ind2 = 0\n",
    "for i in range(len(indexes)):\n",
    "    if len(sliced_ids[i]) < Min:\n",
    "        Min = len(sliced_ids[i])\n",
    "        ind2 = i\n",
    "\n",
    "    if len(sliced_ids[i]) > Max:\n",
    "        Max = len(sliced_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238eb37b-200e-4ae8-a64e-301ecbc020cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_ids = list()\n",
    "att_mask = list()\n",
    "for ten,att in zip(sliced_ids,sliced_att):\n",
    "    if len(ten) < 512:\n",
    "        padding_length = 512 - len(ten)\n",
    "        padding_tensor = torch.full((padding_length,), tokenizer.pad_token_id, dtype=ten.dtype)\n",
    "        padding_tensor2 = torch.full((padding_length,), 0, dtype=att.dtype)\n",
    "        ten = torch.cat((ten,padding_tensor),dim=0)\n",
    "        att = torch.cat((att,padding_tensor2),dim=0)\n",
    "    input_ids.append(ten)\n",
    "    att_mask.append(att)\n",
    "inputIds = torch.stack(input_ids)\n",
    "attMask = torch.stack(att_mask)\n",
    "\n",
    "inputIds_np = inputIds.numpy()\n",
    "attMask_np = attMask.numpy()\n",
    "y1 = data['label1'].values\n",
    "y2 = data['label2'].values\n",
    "lang = df['lang'].tolist()\n",
    "lang = np.array(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47437f84-c7b3-4438-8a3b-3ed050ccc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y1_train, y1_test, y2_train, y2_test,lang_train,lang_test = train_test_split(\n",
    "    inputIds_np, attMask_np, y1, y2,lang, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef407f4-490a-497f-841e-83e7be7aa4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y2_train = np.array(y2_train.tolist(), dtype=np.int8)\n",
    "y2_test = np.array(y2_test.tolist(), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7935e2d8-c0b0-457b-96bb-23627ccc15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ids = torch.tensor(X_train_ids, dtype=torch.long).to(device)\n",
    "X_test_ids = torch.tensor(X_test_ids, dtype=torch.long).to(device)\n",
    "X_train_mask = torch.tensor(X_train_mask, dtype=torch.long).to(device)\n",
    "X_test_mask = torch.tensor(X_test_mask, dtype=torch.long).to(device)\n",
    "y1_train = torch.tensor(y1_train, dtype=torch.long).to(device)\n",
    "y1_test = torch.tensor(y1_test, dtype=torch.long).to(device)\n",
    "y2_train = torch.tensor(y2_train, dtype=torch.long).to(device)\n",
    "y2_test = torch.tensor(y2_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e5efdde-e4b8-4e98-b321-dec52ce50456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_ids, X_train_mask, y1_train, y2_train)\n",
    "test_dataset = TensorDataset(X_test_ids, X_test_mask, y1_test, y2_test )\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True) #shuffle=True provides data shuffle for batches in different epochs\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b58b101-8c75-4c4c-bd99-d7f2afd517db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class HierarchicalNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_parent_classes, num_subcategory_classes,hidden_size):\n",
    "        super(HierarchicalNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        \n",
    "\n",
    "        # Parent class output head\n",
    "        self.parent_fc = nn.Linear(hidden_size, num_parent_classes)\n",
    "\n",
    "        # Subcategory output head (conditional on parent class)\n",
    "        self.subcategory_fc = nn.Linear(hidden_size, num_subcategory_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gelu = nn.GELU()\n",
    "        x = self.fc1(x)\n",
    "        x = gelu(x)\n",
    "\n",
    "        #parent_output = self.parent_fc(x)  # Parent class logits\n",
    "        subcategory_output = self.subcategory_fc(x)  # Subcategory logits\n",
    "\n",
    "        return subcategory_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6e6cc3f-f235-4391-b9e9-790b6653dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#classifier = nn.Linear(model.config.hidden_size * 2, 22).to(device)\n",
    "classifier = HierarchicalNN(model.config.hidden_size * 2,3,6, model.config.hidden_size * 2).to(device)\n",
    "optimizer = AdamW([\n",
    "    {'params': model.parameters(),'lr':2e-5},  # Lower learning rate for XLM-RoBERTa\n",
    "    {'params': classifier.parameters(),'lr':1e-3}     # Higher learning rate for the classifier\n",
    "])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e45b8d4-1af0-4563-91bf-43dae229a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: tensor([False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True]), 0: tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True]), 1: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "        False, False])}\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_batch= torch.Tensor([\n",
    "    #1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22\n",
    "    [1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "    [1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1]\n",
    "])\n",
    "test_parent = torch.Tensor([\n",
    "    [2],\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "mask = {}\n",
    "mask[2] = torch.cat([torch.zeros(6, dtype=torch.bool), torch.ones(16, dtype=torch.bool)])\n",
    "mask[0] = torch.cat([torch.ones(6, dtype=torch.bool), torch.zeros(12, dtype=torch.bool), torch.ones(4, dtype=torch.bool)])\n",
    "mask[1] = torch.cat([torch.ones(18, dtype=torch.bool), torch.zeros(4, dtype=torch.bool)])\n",
    "print(mask)\n",
    "def apply_mask(labels,parent,mask):\n",
    "    \n",
    "    # Create an empty tensor to store the results\n",
    "    result = labels.clone()\n",
    "\n",
    "    # Loop through the batch and apply the corresponding tensor from result_dict\n",
    "    for i in range(labels.shape[0]):\n",
    "        idx = parent[i].item()  # Get the index (0, 1, or 2)\n",
    "        mask2 = mask[idx]  # Apply the corresponding tensor from result_dict\n",
    "\n",
    "        result[i][~mask2] = 0 \n",
    "\n",
    "    return result\n",
    "print(apply_mask(test_batch,test_parent,mask))\n",
    "zero_ten = torch.zeros((16, 22), dtype=torch.float32).to(device)\n",
    "print(zero_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b6da954-1da2-4241-adc6-27709567687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the confusion matrix in the end\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "final_preds = np.empty((0, 6), dtype=np.int8)\n",
    "final_labels = np.empty((0, 6), dtype=np.int8)\n",
    "span_start_token_id = tokenizer.convert_tokens_to_ids('[SPAN_START]')\n",
    "span_end_token_id = tokenizer.convert_tokens_to_ids('[SPAN_END]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9f36079-7968-4211-ac19-1841c4bea782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training loss: 0.4224, Training accuracy: 0.0191\n",
      "Train Micro Precision: 0.2874, Recall: 0.8204, F1: 0.4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.50it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3506, Test accuracy: 0.0175\n",
      "Test Micro Precision: 0.3402, Recall: 0.9167, F1: 0.4962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Training loss: 0.3129, Training accuracy: 0.1483\n",
      "Train Micro Precision: 0.4034, Recall: 0.8591, F1: 0.5490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.58it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3048, Test accuracy: 0.0526\n",
      "Test Micro Precision: 0.4067, Recall: 0.8722, F1: 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:41<00:00,  1.03it/s, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Training loss: 0.2054, Training accuracy: 0.3348\n",
      "Train Micro Precision: 0.5333, Recall: 0.9171, F1: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.64it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3692, Test accuracy: 0.3450\n",
      "Test Micro Precision: 0.4760, Recall: 0.7722, F1: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Training loss: 0.1255, Training accuracy: 0.6373\n",
      "Train Micro Precision: 0.7202, Recall: 0.9420, F1: 0.8163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.56it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4391, Test accuracy: 0.4035\n",
      "Test Micro Precision: 0.5123, Recall: 0.6944, F1: 0.5896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Training loss: 0.0577, Training accuracy: 0.7988\n",
      "Train Micro Precision: 0.8341, Recall: 0.9862, F1: 0.9038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.57it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5384, Test accuracy: 0.4737\n",
      "Test Micro Precision: 0.5631, Recall: 0.6944, F1: 0.6219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Training loss: 0.0400, Training accuracy: 0.8972\n",
      "Train Micro Precision: 0.9231, Recall: 0.9779, F1: 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.55it/s, loss=0.467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5778, Test accuracy: 0.3977\n",
      "Test Micro Precision: 0.5625, Recall: 0.8000, F1: 0.6606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Training loss: 0.0336, Training accuracy: 0.8869\n",
      "Train Micro Precision: 0.9118, Recall: 0.9848, F1: 0.9469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.54it/s, loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5800, Test accuracy: 0.5146\n",
      "Test Micro Precision: 0.6029, Recall: 0.7000, F1: 0.6478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.0183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Training loss: 0.0229, Training accuracy: 0.9457\n",
      "Train Micro Precision: 0.9524, Recall: 0.9945, F1: 0.9730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.61it/s, loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5611, Test accuracy: 0.4561\n",
      "Test Micro Precision: 0.5721, Recall: 0.7056, F1: 0.6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:41<00:00,  1.03it/s, loss=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Training loss: 0.0290, Training accuracy: 0.9339\n",
      "Train Micro Precision: 0.9444, Recall: 0.9862, F1: 0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.55it/s, loss=0.479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6539, Test accuracy: 0.4971\n",
      "Test Micro Precision: 0.5838, Recall: 0.6389, F1: 0.6101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s, loss=0.00465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Training loss: 0.0175, Training accuracy: 0.9545\n",
      "Train Micro Precision: 0.9625, Recall: 0.9917, F1: 0.9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 10/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.57it/s, loss=0.706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6784, Test accuracy: 0.4795\n",
      "Test Micro Precision: 0.5591, Recall: 0.6833, F1: 0.6150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "debug = 0\n",
    "pred_list = list()\n",
    "labels_list = list()\n",
    "log_list =list()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    correct_parents = 0\n",
    "    total_parents = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    train_preds = np.empty((0, 6), dtype=np.int8)\n",
    "    train_labels = np.empty((0, 6), dtype=np.int8)\n",
    "    \n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        parents = batch[2].to(device)\n",
    "        labels = batch[3].to(device)\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        #print(labels.shape)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels.float(), output_hidden_states=True)\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        entity_representations = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            ind_start = torch.nonzero(input_ids[i] == span_start_token_id).squeeze()\n",
    "            ind_end = torch.nonzero(input_ids[i] == span_end_token_id).squeeze()\n",
    "            start_ten = hidden_states[i,ind_start]\n",
    "            end_ten = hidden_states[i,ind_end]\n",
    "            #if debug == 0:\n",
    "                #print (ind_start,ind_end)\n",
    "                #print(start_ten.shape,end_ten.shape)\n",
    "            rep = torch.cat((hidden_states[i,ind_start],hidden_states[i,ind_end]),dim=0)\n",
    "            entity_representations.append(rep)\n",
    "        \n",
    "\n",
    "        #entity_representations = []\n",
    "\n",
    "        #start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
    "        #end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # check that span is valid and has non-zero length\n",
    "        #valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
    "\n",
    "        #valid_start_indices = start_indices[valid_spans]\n",
    "        #valid_end_indices = end_indices[valid_spans]\n",
    "\n",
    "        \n",
    "        \n",
    "        # extract entity tokens for every sample in batch\n",
    "        #for i in range(batch_size):\n",
    "            #entity_tokens = hidden_states[i, valid_start_indices[i]]\n",
    "            #entity_representations.append(entity_tokens)\n",
    "        \n",
    "        #if epoch == 0:\n",
    "        #    print(entity_representations)\n",
    "        \n",
    "        entity_representations = torch.stack(entity_representations, dim=0)\n",
    "        \n",
    "        \n",
    "        #parent_log,\n",
    "        child_log = classifier(entity_representations)\n",
    "        #child_log2 = apply_mask(child_log,parents,mask)\n",
    "        #zero_ten = torch.zeros((input_ids.size(0), 22), dtype=torch.float32).to(device)\n",
    "        #if debug == 0:\n",
    "            #print(child_log,zero_ten,input_ids.size(0))\n",
    "            #print(entity_representations.shape)\n",
    "            #debug+=1\n",
    "        \n",
    "        #loss = criterion(parent_log, parents) + criterion2(child_log,labels.float()) + 2 * criterion2(child_log2,zero_ten) \n",
    "        loss = criterion2(child_log,labels.float()) #+ 2 * criterion2(child_log2,zero_ten)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = (torch.sigmoid(child_log) > 0.15).int()\n",
    "        train_preds = np.vstack([train_preds,preds.cpu().numpy()])\n",
    "        train_labels = np.vstack([train_labels,labels.cpu().numpy()])\n",
    "        correct_predictions += ((preds == labels.int()).all(dim=1)).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        #if debug == 0:\n",
    "            #print(parent_log,child_log,preds,labels)\n",
    "            #debug+=1\n",
    "        #preds_parents = torch.argmax(parent_log, dim=-1)\n",
    "        #correct_parents += (preds_parents == parents).sum().item()\n",
    "        #total_parents += labels.size(0)\n",
    "\n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    #parent_train_acc = correct_parents / total_parents\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    #print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}, Parent Train acc: {parent_train_acc:.4f}\")\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(train_labels, train_preds, average='micro')\n",
    "    print(f\"Train Micro Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    correct_parents = 0\n",
    "    total_parents = 0\n",
    "    test_progress_bar = tqdm(test_dataloader, desc=f\"Test Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    test_preds = np.empty((0, 6), dtype=np.int8)\n",
    "    test_labels = np.empty((0, 6), dtype=np.int8)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress_bar:\n",
    "\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            parents = batch[2].to(device)\n",
    "            labels = batch[3].to(device)\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels.float(), output_hidden_states=True)\n",
    "\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "            entity_representations = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                ind_start = torch.nonzero(input_ids[i] == span_start_token_id).squeeze()\n",
    "                ind_end = torch.nonzero(input_ids[i] == span_end_token_id).squeeze()\n",
    "                start_ten = hidden_states[i,ind_start]\n",
    "                end_ten = hidden_states[i,ind_end]\n",
    "                rep = torch.cat((hidden_states[i,ind_start],hidden_states[i,ind_end]),dim=0)\n",
    "                entity_representations.append(rep)\n",
    "            \n",
    "            #start_mask = (input_ids == span_start_token_id)\n",
    "            #end_mask = (input_ids == span_end_token_id)\n",
    "\n",
    "            #entity_representations = []\n",
    "\n",
    "            #start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
    "            #end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
    "\n",
    "            #valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
    "\n",
    "            #valid_start_indices = start_indices[valid_spans]\n",
    "            #valid_end_indices = end_indices[valid_spans]\n",
    "\n",
    "            # extract entity tokens for every sample in batch\n",
    "            #for i in range(batch_size):\n",
    "            #    entity_tokens = hidden_states[i, valid_start_indices[i]]\n",
    "            #    entity_representations.append(entity_tokens)\n",
    "\n",
    "            entity_representations = torch.stack(entity_representations, dim=0)\n",
    "\n",
    "            #parent_log,\n",
    "            child_log = classifier(entity_representations)\n",
    "            #child_log2 = apply_mask(child_log,parents,mask)\n",
    "            #loss = criterion(parent_log, parents) + criterion2(child_log,labels.float()) + criterion2(child_log2,labels.float()) \n",
    "            loss = criterion2(child_log,labels.float()) #+ 2 * criterion2(child_log2,labels.float()) \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(child_log) > 0.15).int()\n",
    "            correct_predictions += ((preds == labels.int()).all(dim=1)).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            test_preds = np.vstack([test_preds,preds.cpu().numpy()])\n",
    "            test_labels = np.vstack([test_labels,labels.cpu().numpy()])\n",
    "            \n",
    "            #preds_parents = torch.argmax(parent_log, dim=-1)\n",
    "            #correct_parents += (preds_parents == parents).sum().item()\n",
    "            #total_parents += labels.size(0)\n",
    "\n",
    "            test_progress_bar.set_postfix({'loss': loss.item()})\n",
    "            if epoch == num_epochs-1:\n",
    "                final_preds = np.vstack([final_preds,preds.cpu().numpy()])\n",
    "                final_labels = np.vstack([final_labels,labels.cpu().numpy()])\n",
    "                \n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    #parent_test_accuracy = correct_parents / total_parents\n",
    "    #print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}, Parent Test accuracy: {parent_test_accuracy:.4f}\")\n",
    "    print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='micro')\n",
    "    print(f\"Test Micro Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aecd0c2-e078-4c61-be18-d17b627e3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=0)\n",
    "bg_mask = lang_test == 'BG'\n",
    "hi_mask = lang_test == 'HI'\n",
    "en_mask = lang_test == 'EN'\n",
    "pt_mask = lang_test == 'PT'\n",
    "ru_mask = lang_test == 'RU'\n",
    "#print(bg_mask,hi_mask,en_mask,pt_mask,ru_mask)\n",
    "bg_pred = final_preds[bg_mask]\n",
    "bg_labels = final_labels[bg_mask]\n",
    "hi_pred = final_preds[hi_mask]\n",
    "hi_labels = final_labels[hi_mask]\n",
    "en_pred = final_preds[en_mask]\n",
    "en_labels = final_labels[en_mask]\n",
    "pt_pred = final_preds[pt_mask]\n",
    "pt_labels = final_labels[pt_mask]\n",
    "ru_pred = final_preds[ru_mask]\n",
    "ru_labels = final_labels[ru_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1848c07-cf04-47c0-80a8-4e364601941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN Micro Precision: 0.3333,Micro Recall: 0.4545,Micro F1: 0.3846\n",
      "EN accuracy: 0.3182\n",
      "BG Micro Precision: 0.6000,Micro Recall: 0.6000,Micro F1: 0.6000\n",
      "BG accuracy: 0.6000\n",
      "HI Micro Precision: 0.5300,Micro Recall: 0.6883,Micro F1: 0.5989\n",
      "HI accuracy: 0.3714\n",
      "PT Micro Precision: 0.6667,Micro Recall: 0.7600,Micro F1: 0.7103\n",
      "PT accuracy: 0.6042\n",
      "RU Micro Precision: 0.6957,Micro Recall: 0.7619,Micro F1: 0.7273\n",
      "RU accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(en_labels, en_pred, average='micro')\n",
    "print(f\"EN Micro Precision: {precision:.4f},Micro Recall: {recall:.4f},Micro F1: {f1:.4f}\")\n",
    "en_correct = np.sum(np.all(en_labels == en_pred,axis=1))\n",
    "en_total = en_labels.shape[0]\n",
    "total_acc = en_correct/en_total\n",
    "print(f\"EN accuracy: {total_acc:.4f}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(bg_labels, bg_pred, average='micro')\n",
    "print(f\"BG Micro Precision: {precision:.4f},Micro Recall: {recall:.4f},Micro F1: {f1:.4f}\")\n",
    "bg_correct = np.sum(np.all(bg_labels == bg_pred,axis=1))\n",
    "bg_total = bg_labels.shape[0]\n",
    "total_acc = bg_correct/bg_total\n",
    "print(f\"BG accuracy: {total_acc:.4f}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(hi_labels, hi_pred, average='micro')\n",
    "print(f\"HI Micro Precision: {precision:.4f},Micro Recall: {recall:.4f},Micro F1: {f1:.4f}\")\n",
    "hi_correct = np.sum(np.all(hi_labels == hi_pred,axis=1))\n",
    "hi_total = hi_labels.shape[0]\n",
    "total_acc = hi_correct/hi_total\n",
    "print(f\"HI accuracy: {total_acc:.4f}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(pt_labels, pt_pred, average='micro')\n",
    "print(f\"PT Micro Precision: {precision:.4f},Micro Recall: {recall:.4f},Micro F1: {f1:.4f}\")\n",
    "pt_correct = np.sum(np.all(pt_labels == pt_pred,axis=1))\n",
    "pt_total = pt_labels.shape[0]\n",
    "total_acc = pt_correct/pt_total\n",
    "print(f\"PT accuracy: {total_acc:.4f}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ru_labels, ru_pred, average='micro')\n",
    "print(f\"RU Micro Precision: {precision:.4f},Micro Recall: {recall:.4f},Micro F1: {f1:.4f}\")\n",
    "ru_correct = np.sum(np.all(ru_labels == ru_pred,axis=1))\n",
    "ru_total = ru_labels.shape[0]\n",
    "total_acc = ru_correct/ru_total\n",
    "print(f\"RU accuracy: {total_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
