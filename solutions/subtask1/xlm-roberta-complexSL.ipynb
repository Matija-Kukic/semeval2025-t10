{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbfc10c-7c17-442f-8b3a-0d07a72513d1",
   "metadata": {},
   "source": [
    "# Solution 2\n",
    "\n",
    "### Using xlm-roberta for vectorization and then a simple neural net for label classification\n",
    "\n",
    "#### Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf801245-b8c6-4a11-87f5-ba5088e8442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matijak/Documents/programiranje/projects/semeval/merged_data/subtask1.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "wd = Path.cwd()\n",
    "wd = wd.parent.parent\n",
    "wd = wd / 'merged_data' \n",
    "sub1 = str(wd) + '/subtask1.parquet'\n",
    "print(sub1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c900b5-2170-4712-b87d-5eb73adcfee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>art_name</th>\n",
       "      <th>entity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>class1</th>\n",
       "      <th>classes2</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>Запад</td>\n",
       "      <td>152</td>\n",
       "      <td>156</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Conspirator, Instigator, Foreign Adversary]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>САЩ</td>\n",
       "      <td>530</td>\n",
       "      <td>532</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>НАТО</td>\n",
       "      <td>535</td>\n",
       "      <td>538</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>Украйна</td>\n",
       "      <td>578</td>\n",
       "      <td>584</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Foreign Adversary]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>украински войници</td>\n",
       "      <td>633</td>\n",
       "      <td>649</td>\n",
       "      <td>Innocent</td>\n",
       "      <td>[Victim]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang    art_name             entity start  end      class1  \\\n",
       "0   BG  BG_670.txt              Запад   152  156  Antagonist   \n",
       "1   BG  BG_670.txt                САЩ   530  532  Antagonist   \n",
       "2   BG  BG_670.txt               НАТО   535  538  Antagonist   \n",
       "3   BG  BG_670.txt            Украйна   578  584  Antagonist   \n",
       "4   BG  BG_670.txt  украински войници   633  649    Innocent   \n",
       "\n",
       "                                       classes2  \\\n",
       "0  [Conspirator, Instigator, Foreign Adversary]   \n",
       "1                                  [Instigator]   \n",
       "2                                  [Instigator]   \n",
       "3                           [Foreign Adversary]   \n",
       "4                                      [Victim]   \n",
       "\n",
       "                                                text  \n",
       "0  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "1  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "2  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "3  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "4  Опитът на колективния Запад да „обезкърви Руси...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(sub1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d758d-c579-42fa-914b-9fd854e0ebca",
   "metadata": {},
   "source": [
    "#### Now lets clean article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3d00c4-6f4d-4850-be15-fdf7a80f660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang                                                       EN\n",
       "art_name                                     EN_UA_103861.txt\n",
       "entity                                                Chinese\n",
       "start                                                     791\n",
       "end                                                       797\n",
       "class1                                             Antagonist\n",
       "classes2                                                [Spy]\n",
       "text        The World Needs Peacemaker Trump Again \\n\\n by...\n",
       "label                                                       0\n",
       "input       The World Needs Peacemaker Trump Again  by Jef...\n",
       "Name: 448, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def labelNum(row):\n",
    "    if row['class1'] == 'Antagonist':\n",
    "        return int(0)\n",
    "    if row['class1'] == 'Innocent':\n",
    "        return int(1)\n",
    "    if row['class1'] == 'Protagonist':\n",
    "        return int(2)\n",
    "def cleanText(row):\n",
    "    text = str(row['text'])\n",
    "    #text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('\\n',' ').replace('  ', ' ')\n",
    "    return text\n",
    "df['label'] = df.apply(labelNum,axis=1)\n",
    "df['input'] = df.apply(cleanText,axis=1)\n",
    "df.loc[448]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f408c987-cee4-4310-97e9-549d19287c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang                                                       BG\n",
      "art_name                                           BG_670.txt\n",
      "entity                                                  Запад\n",
      "start                                                     152\n",
      "end                                                       156\n",
      "class1                                             Antagonist\n",
      "classes2         [Conspirator, Instigator, Foreign Adversary]\n",
      "text        Опитът на колективния Запад да „обезкърви Руси...\n",
      "label                                                       0\n",
      "input       Опитът на колективния Запад да „обезкърви Руси...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def find_all_substring_start_end(text, substring):\n",
    "    # Use re.finditer to find all occurrences of the substring in the text\n",
    "    matches = re.finditer(re.escape(substring), text)\n",
    "    \n",
    "    # Collect the start and end indices of all matches\n",
    "    positions = [(match.start(), match.end()) for match in matches]\n",
    "    \n",
    "    return positions\n",
    "def adjust_start_end(row):\n",
    "    org_text,cl_text,start,end,entity = str(row['text']),str(row['input']),int(row['start']),int(row['end']),str(row['entity'])\n",
    "    ss1 = find_all_substring_start_end(org_text,entity)\n",
    "    ss2 = find_all_substring_start_end(cl_text,entity)\n",
    "    #print(ss1,ss2)\n",
    "    #print(row['text'][start:end])\n",
    "    a = 0\n",
    "    for i in range(len(ss1)):\n",
    "        if abs((ss1[i][0] - start) + (ss1[i][1] - end) ) <= 2:\n",
    "            a = i\n",
    "            break\n",
    "    if org_text[ss1[a][0]:ss1[a][1]] != cl_text[ss2[a][0]:ss2[a][1]]:\n",
    "        print(\"ERROR!\")\n",
    "    return ss2[a][0],ss2[a][1]\n",
    "print(df.loc[0])\n",
    "df['new_start_end'] = df.apply(adjust_start_end,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790f5e9e-5e11-488f-9361-aafa6be270fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3).to(device)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['input'], padding=True, truncation=True,max_length=8192,return_offsets_mapping=True)\n",
    "\n",
    "data = df.loc[ : , ['input','label','new_start_end','entity']]\n",
    "data['tokenized']=data.apply(preprocess_function,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3201b7e0-58cf-4555-a772-6dd49a362762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 1089, 22617, 1669, 29, 47829, 2097, 32275, 69, 137, 197, 35359, 53335, 2827, 40053, 155, 135, 128601, 29, 12747, 226, 49, 94511, 137, 2687, 591, 7533, 135, 10099, 54293, 35, 25977, 245, 131732, 155, 35, 18777, 183, 159814, 153, 1089, 22617, 1669, 29, 47829, 2097, 32275, 69, 137, 197, 35359, 53335, 2827, 40053, 155, 135, 128601, 29, 12747, 226, 49, 94511, 137, 2687, 591, 7533, 135, 10099, 54293, 35, 25977, 245, 131732, 155, 35, 18777, 183, 159814, 4629, 69, 62086, 16846, 33318, 4, 3756, 77, 63084, 15258, 1669, 29, 92173, 59, 6208, 29, 6047, 39540, 197, 14114, 16641, 44267, 5, 61216, 193342, 43219, 84535, 2262, 36690, 45961, 213358, 222, 31458, 2549, 29, 45775, 59, 29, 103285, 245, 34078, 29, 40108, 47239, 303, 3512, 105, 22192, 4, 12434, 47853, 19737, 245, 6, 163308, 183, 109560, 205, 29, 40108, 135694, 25223, 650, 447, 3873, 8458, 63522, 5, 44, 123209, 24724, 2374, 205, 29, 40108, 4, 20292, 35, 4907, 155386, 74300, 4301, 61, 51192, 205, 49, 159814, 19173, 40053, 218, 1093, 7054, 212, 5, 7932, 12210, 255, 134845, 29, 176323, 168489, 1835, 49, 27068, 29, 61, 43473, 5672, 6367, 105, 1323, 4, 3756, 77, 107935, 226, 35400, 56560, 1200, 4306, 29, 19789, 183, 41374, 35, 61683, 166631, 111998, 89, 4, 8803, 35, 157716, 22524, 245, 38273, 69, 114, 70576, 53335, 2827, 40053, 19523, 42777, 19658, 2004, 17212, 447, 3873, 8458, 63522, 5, 44, 3159, 32104, 17214, 61, 120850, 336, 29, 7631, 3738, 14933, 12332, 4, 440, 52281, 67400, 969, 123807, 4, 114, 140670, 72653, 135, 10099, 54293, 35, 25977, 245, 131732, 830, 40594, 10797, 13582, 41374, 326, 812, 812, 26012, 13110, 5, 20, 417, 231945, 40053, 89053, 129, 35458, 28476, 740, 5901, 18777, 77, 6579, 167186, 197, 237621, 29, 175432, 245, 35, 77, 6579, 12053, 20684, 209016, 11373, 212, 74958, 4, 48554, 183, 159814, 35253, 33318, 4629, 69, 62086, 2004, 84, 5514, 2582, 218, 447, 3873, 8458, 63522, 5, 17345, 137189, 44, 14927, 23394, 11373, 212, 74958, 58, 49, 57231, 32561, 29, 164110, 49, 1537, 743, 67412, 66213, 15, 5221, 104809, 78455, 1114, 247, 5724, 4404, 12053, 20684, 29, 423, 13528, 71147, 129, 159069, 29, 180774, 35, 29, 5724, 40053, 77, 4404, 11004, 336, 212, 5, 124406, 1983, 155386, 63016, 26012, 62760, 40594, 1862, 3258, 164110, 4, 975, 11849, 218, 1714, 25067, 57650, 551, 33545, 59, 3670, 5, 10604, 328, 4334, 8005, 183, 1537, 35030, 49208, 77, 231238, 79971, 53051, 245, 13053, 183, 164110, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 4), (4, 6), (7, 9), (10, 18), (18, 21), (22, 27), (28, 30), (31, 32), (32, 33), (33, 36), (36, 39), (39, 41), (42, 47), (47, 48), (49, 50), (51, 57), (58, 60), (61, 67), (67, 69), (70, 71), (72, 76), (77, 78), (78, 80), (81, 84), (84, 88), (89, 90), (91, 93), (93, 95), (96, 97), (98, 100), (100, 101), (101, 104), (104, 105), (106, 107), (108, 113), (114, 116), (117, 124), (125, 128), (129, 130), (130, 133), (133, 135), (136, 138), (139, 147), (147, 150), (151, 156), (157, 159), (160, 161), (161, 162), (162, 165), (165, 168), (168, 170), (171, 176), (176, 177), (178, 179), (180, 186), (187, 189), (190, 196), (196, 198), (199, 200), (201, 205), (206, 207), (207, 209), (210, 213), (213, 217), (218, 219), (220, 222), (222, 224), (225, 226), (227, 229), (229, 230), (230, 233), (233, 234), (235, 236), (237, 242), (243, 245), (246, 253), (254, 258), (259, 261), (262, 268), (269, 274), (275, 279), (279, 280), (281, 284), (285, 287), (288, 295), (296, 302), (302, 304), (305, 307), (308, 314), (314, 315), (315, 319), (320, 322), (323, 327), (328, 330), (330, 331), (331, 333), (333, 336), (337, 345), (345, 346), (347, 351), (352, 358), (359, 364), (365, 371), (372, 376), (377, 379), (379, 381), (382, 388), (388, 389), (390, 395), (395, 398), (399, 401), (402, 409), (409, 410), (411, 413), (414, 422), (422, 423), (424, 432), (433, 435), (436, 439), (440, 444), (444, 445), (446, 448), (448, 449), (449, 451), (451, 452), (453, 456), (456, 460), (460, 465), (465, 466), (467, 468), (467, 476), (477, 479), (480, 485), (485, 487), (488, 490), (491, 494), (495, 498), (498, 500), (500, 501), (502, 503), (503, 505), (505, 508), (508, 511), (511, 512), (513, 514), (514, 517), (517, 520), (520, 522), (522, 524), (525, 527), (528, 531), (531, 532), (533, 537), (538, 539), (540, 545), (546, 553), (554, 558), (558, 562), (563, 565), (566, 571), (571, 573), (574, 575), (576, 583), (584, 589), (590, 595), (596, 597), (598, 601), (601, 603), (603, 605), (605, 606), (607, 611), (612, 616), (617, 619), (620, 627), (628, 630), (631, 640), (641, 646), (646, 648), (649, 650), (651, 656), (657, 659), (660, 662), (662, 665), (665, 668), (669, 671), (671, 672), (672, 674), (674, 675), (676, 679), (680, 682), (683, 688), (688, 690), (691, 698), (699, 703), (703, 705), (705, 709), (710, 712), (713, 717), (718, 720), (721, 732), (733, 734), (735, 745), (746, 752), (753, 763), (763, 764), (764, 765), (766, 771), (772, 773), (774, 778), (778, 782), (782, 783), (784, 788), (789, 791), (792, 794), (795, 799), (799, 802), (802, 804), (805, 810), (811, 815), (816, 821), (822, 828), (828, 830), (831, 835), (836, 837), (837, 839), (839, 842), (842, 845), (845, 846), (847, 848), (848, 849), (849, 852), (852, 856), (857, 859), (860, 867), (867, 869), (870, 872), (873, 877), (878, 883), (883, 886), (887, 890), (890, 891), (892, 895), (896, 902), (903, 904), (905, 908), (908, 912), (912, 913), (914, 916), (917, 923), (923, 926), (927, 928), (929, 931), (931, 933), (934, 935), (936, 938), (938, 939), (939, 942), (942, 944), (945, 950), (951, 954), (954, 958), (959, 970), (971, 973), (973, 975), (975, 977), (978, 984), (984, 988), (988, 989), (990, 991), (992, 993), (994, 1008), (1009, 1014), (1015, 1023), (1024, 1026), (1026, 1029), (1029, 1033), (1033, 1035), (1036, 1039), (1040, 1045), (1046, 1048), (1049, 1053), (1054, 1060), (1060, 1061), (1062, 1074), (1075, 1077), (1078, 1082), (1082, 1083), (1084, 1085), (1086, 1088), (1089, 1093), (1094, 1097), (1097, 1101), (1102, 1110), (1111, 1114), (1114, 1116), (1117, 1128), (1128, 1129), (1130, 1136), (1137, 1139), (1140, 1147), (1148, 1159), (1160, 1164), (1165, 1169), (1170, 1172), (1173, 1179), (1179, 1181), (1182, 1183), (1183, 1185), (1185, 1188), (1189, 1190), (1191, 1192), (1192, 1194), (1194, 1197), (1197, 1200), (1200, 1201), (1202, 1205), (1206, 1213), (1214, 1215), (1215, 1219), (1219, 1223), (1224, 1227), (1227, 1229), (1230, 1241), (1241, 1242), (1243, 1244), (1245, 1252), (1252, 1255), (1256, 1258), (1259, 1266), (1267, 1268), (1269, 1270), (1270, 1271), (1271, 1275), (1275, 1279), (1280, 1281), (1281, 1282), (1282, 1285), (1285, 1288), (1288, 1290), (1290, 1292), (1293, 1298), (1299, 1303), (1304, 1307), (1307, 1311), (1312, 1314), (1315, 1317), (1317, 1320), (1321, 1324), (1325, 1327), (1328, 1338), (1339, 1341), (1342, 1348), (1349, 1350), (1351, 1353), (1354, 1359), (1360, 1365), (1366, 1368), (1369, 1373), (1374, 1378), (1378, 1380), (1380, 1382), (1382, 1383), (1384, 1388), (1389, 1394), (1395, 1402), (1403, 1414), (1415, 1421), (1421, 1426), (1427, 1432), (1432, 1434), (1435, 1439), (1440, 1447), (1447, 1448), (1449, 1451), (1452, 1454), (1455, 1456), (1457, 1460), (1460, 1463), (1463, 1465), (1465, 1467), (1468, 1470), (1470, 1471), (1471, 1474), (1474, 1475), (1476, 1478), (1478, 1480), (1481, 1485), (1486, 1492), (1493, 1495), (1496, 1497), (1497, 1499), (1499, 1501), (1502, 1504), (1505, 1513), (1514, 1520), (1520, 1526), (1526, 1527), (1528, 1536), (1537, 1539), (1540, 1547), (1547, 1548), (0, 0)]}\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[0]['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79ca62-1259-4a16-b867-40da353e576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.loc[0]['tokenized'])\n",
    "def indexes(row):\n",
    "    off_mask = row['tokenized']['offset_mapping']\n",
    "    start,end = row['new_start_end'][0],row['new_start_end'][1]\n",
    "    inds = list()\n",
    "    for p in range(len(off_mask)):\n",
    "        if off_mask[p][0] >= start and off_mask[p][1] <= end:\n",
    "            if p != len(off_mask)-1:\n",
    "                inds.append(p)\n",
    "    #if len(inds) > 1:\n",
    "        #print(\"GREATER THAN 1\")\n",
    "    if len(inds) == 0:\n",
    "        print(start,end)\n",
    "    return inds\n",
    "data['indexes'] = data.apply(indexes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec11da4f-89c6-4618-95a2-d05c1f80a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input            The World Needs Peacemaker Trump Again  by Jef...\n",
      "label                                                            0\n",
      "new_start_end                                           (785, 792)\n",
      "entity                                                     Chinese\n",
      "tokenized              [input_ids, attention_mask, offset_mapping]\n",
      "indexes                                                      [180]\n",
      "Name: 448, dtype: object input            जयपुर में जलवायु परिवर्तन को लेकर स्टेट लेवल ट...\n",
      "label                                                            2\n",
      "new_start_end                                           (623, 636)\n",
      "entity                                               केन्द्र सरकार\n",
      "tokenized              [input_ids, attention_mask, offset_mapping]\n",
      "indexes                                                 [162, 163]\n",
      "Name: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[448],data.loc[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b9a4e2-900a-4d91-8be1-e0c328d1f3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2535 2535 2535\n",
      "tensor([     0,   1089,  22617,   1669,     29,  47829,   2097,  32275,     69,\n",
      "           137,    197,  35359,  53335,   2827,  40053,    155,    135, 128601,\n",
      "            29,  12747,    226,     49,  94511,    137,   2687,    591,   7533,\n",
      "           135,  10099,  54293,     35,  25977,    245, 131732,    155,     35,\n",
      "         18777,    183, 159814,    153,   1089,  22617,   1669,     29,  47829,\n",
      "          2097,  32275,     69,    137,    197,  35359,  53335,   2827,  40053,\n",
      "           155,    135, 128601,     29,  12747,    226,     49,  94511,    137,\n",
      "          2687,    591,   7533,    135,  10099,  54293,     35,  25977,    245,\n",
      "        131732,    155,     35,  18777,    183, 159814,   4629,     69,  62086,\n",
      "         16846,  33318,      4,   3756,     77,  63084,  15258,   1669,     29,\n",
      "         92173,     59,   6208,     29,   6047,  39540,    197,  14114,  16641,\n",
      "         44267,      5,  61216, 193342,  43219,  84535,   2262,  36690,  45961,\n",
      "        213358,    222,  31458,   2549,     29,  45775,     59,     29, 103285,\n",
      "           245,  34078,     29,  40108,  47239,    303,   3512,    105,  22192,\n",
      "             4,  12434,  47853,  19737,    245,      6, 163308,    183, 109560,\n",
      "           205,     29,  40108, 135694,  25223,    650,    447,   3873,   8458,\n",
      "         63522,      5,     44, 123209,  24724,   2374,    205,     29,  40108,\n",
      "             4,  20292,     35,   4907, 155386,  74300,   4301,     61,  51192,\n",
      "           205,     49, 159814,  19173,  40053,    218,   1093,   7054,    212,\n",
      "             5,   7932,  12210,    255, 134845,     29, 176323, 168489,   1835,\n",
      "            49,  27068,     29,     61,  43473,   5672,   6367,    105,   1323,\n",
      "             4,   3756,     77, 107935,    226,  35400,  56560,   1200,   4306,\n",
      "            29,  19789,    183,  41374,     35,  61683, 166631, 111998,     89,\n",
      "             4,   8803,     35, 157716,  22524,    245,  38273,     69,    114,\n",
      "         70576,  53335,   2827,  40053,  19523,  42777,  19658,   2004,  17212,\n",
      "           447,   3873,   8458,  63522,      5,     44,   3159,  32104,  17214,\n",
      "            61, 120850,    336,     29,   7631,   3738,  14933,  12332,      4,\n",
      "           440,  52281,  67400,    969, 123807,      4,    114, 140670,  72653,\n",
      "           135,  10099,  54293,     35,  25977,    245, 131732,    830,  40594,\n",
      "         10797,  13582,  41374,    326,    812,    812,  26012,  13110,      5,\n",
      "            20,    417, 231945,  40053,  89053,    129,  35458,  28476,    740,\n",
      "          5901,  18777,     77,   6579, 167186,    197, 237621,     29, 175432,\n",
      "           245,     35,     77,   6579,  12053,  20684, 209016,  11373,    212,\n",
      "         74958,      4,  48554,    183, 159814,  35253,  33318,   4629,     69,\n",
      "         62086,   2004,     84,   5514,   2582,    218,    447,   3873,   8458,\n",
      "         63522,      5,  17345, 137189,     44,  14927,  23394,  11373,    212,\n",
      "         74958,     58,     49,  57231,  32561,     29, 164110,     49,   1537,\n",
      "           743,  67412,  66213,     15,   5221, 104809,  78455,   1114,    247,\n",
      "          5724,   4404,  12053,  20684,     29,    423,  13528,  71147,    129,\n",
      "        159069,     29, 180774,     35,     29,   5724,  40053,     77,   4404,\n",
      "         11004,    336,    212,      5, 124406,   1983, 155386,  63016,  26012,\n",
      "         62760,  40594,   1862,   3258, 164110,      4,    975,  11849,    218,\n",
      "          1714,  25067,  57650,    551,  33545,     59,   3670,      5,  10604,\n",
      "           328,   4334,   8005,    183,   1537,  35030,  49208,     77, 231238,\n",
      "         79971,  53051,    245,  13053,    183, 164110,      5,      2],\n",
      "       device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "[180]\n"
     ]
    }
   ],
   "source": [
    "data['list'] = data['tokenized'].apply(lambda x: x['input_ids'])\n",
    "data['attention'] = data['tokenized'].apply(lambda x: x['attention_mask'])\n",
    "ids = data['list']\n",
    "att = data['attention']\n",
    "indexes = data['indexes']\n",
    "tids = list()\n",
    "tatt = list()\n",
    "print(len(ids),len(att),len(indexes))\n",
    "for i in range(len(ids)):\n",
    "    tids.append(torch.tensor(ids[i]).to(device))\n",
    "    tatt.append(torch.tensor(att[i]).to(device))\n",
    "print(tids[0],tatt[0])\n",
    "print(indexes[448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82c0d314-c958-4e7f-98fa-be7c77cebd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511 586 65 512 65 956 [1, 2]\n"
     ]
    }
   ],
   "source": [
    "sliced_ids = list()\n",
    "sliced_ntids = list()\n",
    "sliced_att = list()\n",
    "key_inds = list()\n",
    "key_ids = list()\n",
    "\n",
    "def slices(index,size,context_size):\n",
    "    if (size<context_size):\n",
    "        return 0,size\n",
    "    lower_c = int(context_size/2-1)\n",
    "    upper_c = int(context_size/2)\n",
    "    #print(lower_c,upper_c)\n",
    "    if index < lower_c:\n",
    "        return 0,context_size\n",
    "    elif index >= lower_c:\n",
    "        if index + upper_c > size:\n",
    "            return index-(context_size-(size-index)), size\n",
    "        else:\n",
    "            return index-lower_c,index+upper_c+1  \n",
    "\n",
    "\n",
    "for i in range(len(tids)):\n",
    "    slower,supper = slices(indexes[i][0],len(tids[i]),510)\n",
    "    #key_tid = tids[i][indexes[i][0]]\n",
    "    pid = ids[i][slower:supper]\n",
    "    key_inds.append([])\n",
    "    for j in indexes[i]: \n",
    "        key_id = ids[i][j]\n",
    "        if key_id not in pid:\n",
    "           print(len(ids[i]),key_id,slower,supper,indexes[i])\n",
    "        key_inds[i].append(pid.index(key_id))\n",
    "    apid = tids[i][slower:supper]\n",
    "    apatt = tatt[i][slower:supper]\n",
    "    if 0 not in pid:\n",
    "        apid = torch.cat((torch.tensor([0]).to(device),apid),dim=0)\n",
    "        apatt = torch.cat((torch.tensor([1]).to(device),apatt),dim=0)\n",
    "        for j in range(len(key_inds[i])):\n",
    "            key_inds[i][j] += 1\n",
    "    if 2 not in pid:\n",
    "        apid = torch.cat((apid,torch.tensor([2]).to(device)),dim=0)\n",
    "        apatt = torch.cat((apatt,torch.tensor([1]).to(device)),dim=0)\n",
    "    sliced_ids.append(apid)\n",
    "    sliced_att.append(apatt)\n",
    "\n",
    "Min = 10000\n",
    "Max = 0\n",
    "ind2 = 0\n",
    "for i in range(len(indexes)):\n",
    "    if len(sliced_ids[i]) < Min:\n",
    "        Min = len(sliced_ids[i])\n",
    "        ind2 = i\n",
    "        \n",
    "    if len(sliced_ids[i]) > Max:\n",
    "        Max = len(sliced_ids[i])\n",
    "print(len(sliced_ids[500]),len(tids[500]),Min,Max,len(tids[ind2]),ind2,key_inds[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf8ec797-6162-41a5-bbb9-e29015eb3acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(884, device='cuda:0'), tensor(59223, device='cuda:0')]\n",
      "['Al', 'Gore'] Al Gore\n"
     ]
    }
   ],
   "source": [
    "a = list()\n",
    "for i in key_inds[512]:\n",
    "    a.append(sliced_ids[512][i])\n",
    "print(a)\n",
    "print(tokenizer.batch_decode(a),df['entity'].loc[512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aaa4627-0e31-4344-8723-6ed53bf064fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1089, 22617,  ...,     1,     1,     1],\n",
      "        [    0,  1089, 22617,  ...,     1,     1,     1],\n",
      "        [    0,  1089, 22617,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 19559,   164,  ...,     5,     2,     1],\n",
      "        [    0, 19559,   164,  ...,     5,     2,     1],\n",
      "        [    0, 19559,   164,  ...,     5,     2,     1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids = list()\n",
    "att_mask = list()\n",
    "for ten,att in zip(sliced_ids,sliced_att):\n",
    "    if len(ten) < 512:\n",
    "        padding_length = 512 - len(ten)\n",
    "        padding_tensor = torch.full((padding_length,), tokenizer.pad_token_id, dtype=ten.dtype).to(device)\n",
    "        padding_tensor2 = torch.full((padding_length,), 0, dtype=att.dtype).to(device)\n",
    "        ten = torch.cat((ten,padding_tensor),dim=0)\n",
    "        att = torch.cat((att,padding_tensor2),dim=0)\n",
    "    input_ids.append(ten)\n",
    "    att_mask.append(att)\n",
    "inputIds = torch.stack(input_ids)\n",
    "attMask = torch.stack(att_mask)\n",
    "#print(input_ids[300],attMask[300],inputIds.shape,attMask.shape)\n",
    "print(inputIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eb62432-9ddb-4556-86f5-b0d6f87a8494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(67151, device='cuda:0'), tensor(59520, device='cuda:0')]\n",
      "['Boris', 'Johnson'] Boris Johnson\n"
     ]
    }
   ],
   "source": [
    "a = list()\n",
    "for i in key_inds[500]:\n",
    "    a.append(inputIds[500][i])\n",
    "print(a)\n",
    "print(tokenizer.batch_decode(a),df['entity'].loc[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21c4723a-8d9e-433d-ab0e-ed548e91e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "dataset = TensorDataset(inputIds, attMask)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1,shuffle=False)\n",
    "\n",
    "ind = 0\n",
    "\n",
    "vectors = []\n",
    "\n",
    "#print(input_ids_batch)\n",
    "for batch in dataloader:\n",
    "    #print(len(batch[0]),batch)\n",
    "    input_for_model = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\" : batch[1]\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**input_for_model,output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    last_hs = hidden_states[-1]\n",
    "    #print(last_hs.shape)\n",
    "    for i in range(len(last_hs)):\n",
    "        vectors.append([])\n",
    "        for j in range(len(key_inds[ind])):\n",
    "            vectors[len(vectors)-1].append(last_hs[i][key_inds[ind][j]])\n",
    "        ind+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ced5b10f-a6ca-41de-9f7c-96ad2f11a0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1.3687e-01,  6.0286e-02, -3.2900e-03,  1.4017e-02,  8.8945e-02,\n",
      "        -2.8998e-01, -7.5474e-02,  3.1370e-01,  7.2622e-02,  7.4450e-02,\n",
      "        -1.2063e-01,  7.7085e-03,  5.2945e-01, -2.2475e-01,  7.1912e-04,\n",
      "         8.5789e-02,  2.2363e-03, -2.1583e-02,  1.8685e-01, -7.6887e-02,\n",
      "        -1.8216e-01, -1.5473e-03, -3.6373e-02, -5.0588e-04,  1.9936e-01,\n",
      "        -6.4968e-02,  8.5755e-02, -2.8144e-02,  1.8071e-01, -1.0272e-01,\n",
      "         6.0687e-02,  1.2576e-02,  7.2960e-03,  9.9497e-02,  3.2894e-02,\n",
      "         1.6656e-01,  5.5868e-02,  1.4055e-02,  1.2616e-01,  2.1519e-03,\n",
      "         5.0026e-02, -5.1597e-02,  3.9367e-02,  4.6329e-02,  5.6186e-02,\n",
      "        -3.7047e-02,  9.8740e-03,  2.9654e-02,  6.1344e-02, -5.9128e-02,\n",
      "         5.2124e-02, -4.5550e-02,  8.4431e-03,  1.3068e-01,  2.5632e-02,\n",
      "        -1.0084e-01,  8.5307e-02, -5.6493e-02,  1.2085e-02,  1.7036e-01,\n",
      "         5.5647e-02,  3.5237e-02,  3.9696e-02, -1.4993e-01,  1.5731e-01,\n",
      "        -3.3519e-02, -1.3597e-01,  2.8188e-02, -7.5262e-02,  6.2445e-02,\n",
      "        -2.7698e-02, -2.2623e-01,  1.2248e-01, -8.4980e-02, -1.4709e-02,\n",
      "        -5.5233e-02,  3.4211e-02, -1.5360e-02, -1.1049e-02,  9.3987e-02,\n",
      "        -2.1947e-02, -1.0644e-01, -4.5745e-02, -8.7779e-02,  1.3121e-01,\n",
      "         6.7421e-02, -4.2748e-02,  4.1415e-02,  2.8197e-02,  3.5597e-01,\n",
      "        -4.3464e-02,  2.1372e-03,  2.2440e-01, -7.7559e-02,  1.6470e-01,\n",
      "        -2.2526e-02,  1.4945e-02,  1.0049e-01, -2.2038e-02, -4.7754e-01,\n",
      "         1.0774e-02, -3.9041e-02, -6.7682e-02,  6.1176e-03, -2.1412e-01,\n",
      "         8.1366e-04,  4.6042e-04, -4.8445e-02, -1.7453e-02,  2.4628e-02,\n",
      "         1.0616e-01, -3.6700e-02, -1.9546e-01, -1.2357e-01,  1.0519e-01,\n",
      "        -8.1591e-02,  2.9781e-02,  1.1343e-01,  4.2427e-02,  4.2816e-02,\n",
      "        -1.1092e-01, -1.8484e-01, -5.3283e-03,  6.6583e-02,  6.3397e-03,\n",
      "         2.1575e-02, -6.9175e-02, -4.5630e-02,  9.9569e-02,  2.2922e-01,\n",
      "         6.2623e-02, -2.6941e-01,  8.8732e-04, -1.4514e-02,  4.5064e-02,\n",
      "         1.1819e-01, -8.0564e-02,  2.2493e-02, -7.3017e-02,  3.9678e-01,\n",
      "        -1.4791e-01, -2.4413e-01,  1.7291e-02,  2.9503e-02,  7.8113e-03,\n",
      "         1.7556e+00,  9.2967e-03, -2.8933e-02, -1.3713e-01,  1.5973e-01,\n",
      "        -1.6621e-01,  3.5491e-01, -9.2678e-01, -1.9088e-02,  8.1395e-03,\n",
      "         7.6232e-02,  1.8292e-01, -1.3288e-02, -2.7426e-02, -4.5518e-02,\n",
      "         1.5330e-01,  1.9346e-02,  7.5208e-02, -1.4382e-04,  2.7264e-02,\n",
      "         1.2039e-01,  1.8413e-01, -5.1875e-03, -6.7662e-02,  6.2414e-02,\n",
      "        -1.3139e-03, -3.9978e-02, -7.7208e-02, -2.3076e-02, -4.2374e-02,\n",
      "        -1.4470e-01, -1.9809e-02,  1.7701e-01,  4.5680e-02, -9.6780e-03,\n",
      "         1.2704e-03, -5.1873e-02, -2.6822e-01,  4.7019e-02, -2.7752e-01,\n",
      "        -1.8216e-02,  3.5688e-02, -1.7290e-02,  5.2033e-02, -5.0807e-02,\n",
      "         3.4543e-02,  1.5512e-01,  1.1962e-02,  8.6893e-03, -4.5102e-02,\n",
      "         1.4789e-01,  1.0453e-02,  4.6082e-02, -7.7324e-02, -3.8459e-02,\n",
      "         4.8857e-02, -1.8419e-02, -2.7006e-01, -5.6990e-02, -5.0933e-02,\n",
      "        -1.5861e-02, -2.5021e-02,  3.5824e-01,  2.5064e-02,  7.0412e-02,\n",
      "        -1.7082e-02, -6.4999e-04,  8.1225e-03, -8.2772e-02,  2.4385e-02,\n",
      "        -1.7302e-01, -1.2460e-02,  1.9705e-01,  9.5960e-02,  1.7932e-02,\n",
      "        -5.1905e-02, -1.6691e-01,  3.8235e-02,  7.7851e-03, -4.0995e-03,\n",
      "         9.5989e-02, -1.6781e-02,  2.1999e-02,  4.2500e-02, -2.6031e-01,\n",
      "         2.5185e-02, -6.0203e-02,  6.2023e-02,  1.8933e-01, -1.3678e-02,\n",
      "         4.0014e-02,  2.1989e-01, -1.9716e-04,  4.6384e-02, -3.6229e-01,\n",
      "        -1.3055e-03,  4.6138e-02, -2.5966e-03, -5.6942e-03,  5.8138e-02,\n",
      "         1.4162e-01,  2.7097e-02,  1.1460e-01,  1.7973e-01, -5.5072e-02,\n",
      "         2.4266e-02,  5.4588e-02, -5.4734e-02, -6.6293e-02, -5.1020e-02,\n",
      "         1.8211e-01, -1.7928e-01, -1.0739e-01,  5.9679e-02,  1.1449e-01,\n",
      "         3.8435e-02,  9.5948e-02,  1.0435e-01,  1.5620e-01,  2.4683e-02,\n",
      "         1.7529e-01,  1.9214e-01,  7.3640e-02, -3.8258e-02, -6.1740e-02,\n",
      "         9.2704e-03, -7.1393e-02, -1.4858e-01,  1.3024e-01,  2.8525e-02,\n",
      "         2.2225e-02,  1.7714e-01, -2.0909e-01, -1.9900e-01, -2.8643e-02,\n",
      "         3.0988e-02, -5.4668e-02, -5.2948e-02, -3.4970e-02,  2.8294e-02,\n",
      "        -4.9069e-02, -9.4921e-02, -1.4935e-01,  7.8857e-04, -2.4037e-01,\n",
      "        -1.7410e-02,  7.3425e-02, -8.7995e-02, -3.3099e-02,  2.8218e-01,\n",
      "         1.1340e-01, -1.0030e-02, -5.1633e-02,  8.4984e-02,  5.2321e-02,\n",
      "        -8.7224e-04, -1.8087e-02,  1.6605e-01,  8.1785e-03,  2.2382e-03,\n",
      "        -3.2968e-02,  4.2232e-01, -1.6147e-03,  6.3730e-02, -5.4267e-05,\n",
      "        -1.9559e-02, -2.3288e-02, -1.4038e-01, -8.7644e-02, -5.6138e-02,\n",
      "         3.3034e-02, -7.2387e-02, -5.1269e-02, -3.5748e-03, -4.8515e-02,\n",
      "         2.0594e-01, -3.8272e-03, -5.4378e-02,  1.7157e-02,  7.0921e-02,\n",
      "         7.3343e-02, -1.8536e-02,  3.0798e-02, -9.2395e-02,  3.5048e-02,\n",
      "         4.5696e-02, -2.6734e-02,  9.3713e-03, -2.2567e-03, -5.4363e-02,\n",
      "         2.9919e-02,  2.8207e-02, -4.8692e-02,  1.9837e-01, -1.7772e-01,\n",
      "         8.6692e-02, -7.9071e-02, -3.1313e-01, -1.1169e-01, -7.3784e-02,\n",
      "        -1.5531e-02,  8.7787e-03, -2.2143e-02,  9.3168e-02,  5.4276e-02,\n",
      "         1.4120e-01, -7.6172e-02,  1.5807e-01, -9.9867e-02, -2.9552e-02,\n",
      "        -8.6383e-02, -1.3942e-01,  2.0907e-01,  6.0784e-02, -5.8873e-02,\n",
      "         2.3811e-02,  4.9516e-02,  7.4919e-03,  1.1985e-03,  2.9140e-03,\n",
      "         9.1913e-02,  1.0752e-01, -4.5657e-02,  9.5912e-03,  2.6275e-03,\n",
      "         1.5028e-01,  8.8305e-02,  3.5344e-02,  5.0524e-03,  7.5077e-02,\n",
      "         2.1491e-02,  3.9225e-03,  2.2317e-01, -1.1842e-02,  8.8838e-02,\n",
      "         4.3099e-02,  1.6939e-01, -4.9256e-02,  6.7586e-02,  6.9858e-02,\n",
      "        -8.1543e-02, -7.7864e-02, -2.4138e-01, -2.6680e-02, -6.2349e-03,\n",
      "         8.8018e-02, -5.3805e-01,  2.3024e-03, -1.8627e-02,  6.6336e-04,\n",
      "         2.6982e-01,  2.9370e-04, -1.4541e-01,  5.5454e-04, -6.0709e-02,\n",
      "         1.2060e-02, -1.4382e-02, -9.4404e-02,  5.6317e-02,  9.7113e-02,\n",
      "         3.3489e-02,  4.3177e-03,  1.8660e-05, -2.0812e-02, -7.2396e-03,\n",
      "         4.2762e-02,  7.3300e-02,  2.6870e-02, -5.0510e-02,  6.9187e-02,\n",
      "         1.5834e-02,  7.4371e-02,  2.2165e-03, -3.2238e-01,  3.6898e-02,\n",
      "        -1.1498e-01,  1.3330e-01,  6.6365e-03, -4.1122e-02,  2.2759e-02,\n",
      "         8.4579e-03, -3.6442e-02, -2.7024e-02,  1.3943e-02, -8.5632e-03,\n",
      "         3.2179e-02, -3.8633e-02,  4.6037e-02,  8.9698e-02,  5.7607e-02,\n",
      "        -1.0141e-01, -1.6042e-02,  8.0423e-02,  8.0085e-02,  1.3676e-01,\n",
      "        -7.4387e-02,  5.9370e-02, -2.0036e-02,  2.1591e-01, -8.0591e-03,\n",
      "        -1.2198e-01,  6.1549e-02,  3.5209e-03, -7.0831e-02, -2.8251e-02,\n",
      "        -2.1897e-02,  3.6565e-02,  5.8449e-02, -1.9369e-02,  4.9704e-02,\n",
      "        -1.9991e-01,  8.4714e-03,  4.0312e-02,  2.6804e-02, -1.6486e+00,\n",
      "         2.9783e-01,  8.9497e-02,  2.8147e-02,  1.5558e-01, -9.3397e-02,\n",
      "        -6.7490e-02, -1.6199e-02, -1.1011e-01,  1.2069e-01,  1.3962e-01,\n",
      "        -1.5759e-01, -2.7306e-02,  1.7282e-02, -1.9170e-01, -2.8695e-02,\n",
      "         8.8006e-03,  6.6301e-02, -1.6103e-01,  1.1311e-01,  4.0503e-02,\n",
      "         3.6763e-02, -3.6810e-02, -2.6206e-01, -1.4942e-02, -2.6456e-02,\n",
      "         1.0911e-01,  8.0810e-02,  3.3623e-01,  6.8785e-02, -7.3347e-02,\n",
      "         7.0560e-02,  3.4504e-02, -1.0052e-01,  2.7516e-02,  2.0973e-02,\n",
      "         3.5654e-02,  8.6995e-02, -2.2365e-01,  1.9649e-02,  4.7044e-02,\n",
      "         4.1647e-02,  5.1388e-02, -5.8907e-02, -5.5393e-03, -1.4853e-01,\n",
      "        -5.3563e-03, -1.8329e-02, -5.1172e-02, -1.9160e-03,  2.2313e-02,\n",
      "         5.9190e-01, -5.9004e-02, -5.1810e-02, -1.7442e-02,  1.6103e-01,\n",
      "         3.2708e-02, -2.7269e-02,  6.6193e-02,  3.4194e-02, -3.2171e-02,\n",
      "         3.8298e-01,  2.0056e-01, -1.7855e-02, -1.5899e-02,  4.0817e-02,\n",
      "        -5.0930e-02,  4.4115e-02,  5.5665e-02, -1.0097e-02,  2.0563e-01,\n",
      "        -1.1758e-01,  8.9060e-03, -6.7653e-02, -4.3860e-01,  6.7257e-02,\n",
      "        -9.1791e-02,  2.7190e-03,  8.2548e-03, -4.5514e-02,  4.0118e-02,\n",
      "         3.6717e-01,  6.0681e-03,  2.3727e-01, -2.1408e-02,  9.8249e-03,\n",
      "         7.9814e-02, -5.6198e-02, -1.5505e-02,  5.3600e-02, -1.3566e-02,\n",
      "         1.6035e-03, -1.8615e-01, -5.7694e-03, -8.6792e-02, -2.4545e-01,\n",
      "         1.7982e-01, -3.7838e-02,  8.0834e-02,  1.2627e-01, -1.4270e-02,\n",
      "         1.0050e-02, -7.9506e-03,  3.5044e-02, -1.3269e-02, -2.3376e-02,\n",
      "         3.0348e-02,  3.9665e-02,  2.2428e-02,  2.3934e-02,  1.1996e-01,\n",
      "        -9.9042e-02,  5.1589e-02,  8.3792e-03,  5.6825e-02,  2.2760e-01,\n",
      "         4.9929e-02,  1.9693e-02, -2.1445e-02, -1.0051e-03, -1.6564e-01,\n",
      "        -2.5288e-01,  3.6391e-03,  1.0185e-01, -4.7566e-03, -1.6778e-01,\n",
      "         3.2623e-03,  3.1840e-03,  2.3287e-01, -6.0260e+00,  1.2910e-02,\n",
      "         1.0942e-01,  2.0724e-01,  3.9028e-03, -7.3236e-02, -9.1204e-03,\n",
      "         8.0489e-02,  1.3074e-02,  1.6801e-01,  6.2650e-02, -2.4781e-01,\n",
      "         1.5190e-01,  2.2885e-02, -6.9861e-03, -5.1457e-02, -5.2643e-02,\n",
      "        -2.0757e-01,  6.6567e-02,  8.1626e-02,  5.0019e-02,  2.4582e-01,\n",
      "         2.3137e-01,  2.3810e-01,  1.4194e-01,  1.0459e-04,  4.8024e-02,\n",
      "         2.6420e-01,  3.0860e-02, -2.9732e-02, -7.2322e-02,  9.7596e-02,\n",
      "        -2.4496e-03,  1.4243e-03,  2.8599e-02, -5.0545e-02, -3.6812e-02,\n",
      "        -9.1237e-03,  1.0308e-01,  1.2688e-01, -4.1218e-02, -2.2722e-01,\n",
      "         6.0912e-02,  8.2090e-02, -4.4540e-02, -3.3393e-01,  3.6177e-01,\n",
      "        -2.9377e-02,  1.7713e-02, -1.0531e-01,  2.6477e-02,  6.2442e-02,\n",
      "        -8.4020e-02,  1.4910e-01, -8.4496e-02, -4.2592e-02, -3.8535e-04,\n",
      "        -2.1991e-02,  2.1739e-02,  1.4072e-02, -3.0323e-02, -2.7998e-02,\n",
      "         2.5741e-02,  7.6574e-02, -5.7899e-02,  1.1688e-01, -2.0753e-01,\n",
      "         4.4960e-02,  5.0156e-03, -6.7473e-03,  8.4359e-02,  1.2106e-02,\n",
      "        -4.0764e-01, -9.9564e-02, -4.4839e-02, -1.3950e-01,  7.5190e-02,\n",
      "        -1.1537e-02,  8.9399e-02,  6.0129e-02, -3.0538e-02,  1.1992e-01,\n",
      "         3.1842e-03, -1.0122e-02,  3.7471e-02, -1.4846e-01,  1.4140e-01,\n",
      "        -2.9747e-02, -2.8136e-02,  1.0317e-02, -1.3539e-01, -1.5016e-01,\n",
      "         2.9986e-01,  9.4693e-02,  1.1284e-01, -9.1553e-02, -5.7171e-02,\n",
      "        -2.1539e-02,  8.2399e-02, -2.1689e-02,  3.7769e-02,  1.4336e-01,\n",
      "        -5.7707e-02,  2.6121e-02,  9.1248e-02,  1.1093e-02, -2.6882e-01,\n",
      "         1.1112e-01,  2.8570e-02, -6.0406e-02, -8.1387e-02, -2.9731e-02,\n",
      "         5.9781e-02,  5.8555e-02,  5.1693e-02,  1.5685e-02,  1.4995e-01,\n",
      "         1.7611e-01, -1.3044e-01, -1.1177e-01,  4.7968e-02,  1.0973e-01,\n",
      "         3.0580e-02, -1.6860e-02,  3.0464e-01,  1.1686e-02,  7.4234e-02,\n",
      "        -1.6809e-01,  5.4882e-02,  4.2173e-02, -2.3221e-01,  3.4979e-02,\n",
      "         1.9955e-01,  6.1776e-02, -3.3941e-01, -3.5684e-01,  1.3922e-02,\n",
      "        -3.9732e-02, -4.9857e-03,  9.3370e-02,  1.3293e-01,  1.4216e-01,\n",
      "         1.0456e-01,  4.1934e-02, -1.1899e-01,  4.9319e-02,  2.3157e-01,\n",
      "        -3.9691e-03,  3.1844e-01, -5.0615e-02,  3.8360e-02, -1.8186e-01,\n",
      "         2.0538e-02,  1.7866e+01,  7.4334e-02,  9.4591e-02, -4.3587e-02,\n",
      "         3.4284e-02, -2.0323e-01, -3.3431e-02,  1.3048e-01, -4.8372e-01,\n",
      "         2.1883e-02,  5.5820e-02, -2.4124e-03, -5.8618e-02,  5.8479e-02,\n",
      "        -1.1680e-02,  8.4033e-02,  7.2687e-02, -8.6462e-02,  4.0160e-02,\n",
      "         2.0450e-02,  7.1194e-02,  3.5743e-02,  2.4790e-02,  2.1696e-01,\n",
      "        -7.2947e-02,  5.8202e-02,  2.0631e-01], device='cuda:0'), tensor([ 8.1047e-02,  7.6517e-02,  6.0304e-03,  5.2398e-02,  2.1350e-01,\n",
      "        -4.0156e-02, -4.3735e-02,  3.1448e-01,  9.2655e-02, -9.2090e-02,\n",
      "        -2.3044e-01,  2.6245e-02,  5.7882e-01, -3.1863e-01, -2.2419e-02,\n",
      "         4.4740e-02, -1.4199e-02, -5.9755e-03,  6.8040e-02, -1.5763e-02,\n",
      "         8.8582e-02,  1.0614e-02, -2.3555e-01,  9.9436e-03,  1.1733e-01,\n",
      "        -1.0494e-01,  7.6767e-02, -1.0352e-01, -3.3191e-03, -8.5925e-02,\n",
      "         4.0177e-02,  1.1305e-02, -2.6038e-02, -6.4500e-02, -5.7461e-02,\n",
      "         2.6094e-01,  7.7488e-02, -7.1281e-03,  2.6798e-01, -6.0397e-02,\n",
      "         3.4565e-02, -6.2729e-02, -4.8917e-02,  1.1755e-01, -1.3920e-03,\n",
      "        -4.5833e-03, -2.8609e-02, -2.3716e-02,  7.1606e-02, -3.9493e-02,\n",
      "         4.3158e-02, -2.9245e-01,  2.4300e-02, -1.8265e-02, -1.9677e-01,\n",
      "        -8.1168e-02,  9.9678e-02, -6.8600e-02,  8.7618e-02,  2.9071e-01,\n",
      "         1.0723e-01,  2.1746e-01,  1.0581e-03,  2.5737e-01, -1.5541e-01,\n",
      "        -2.7108e-02, -8.2315e-02,  1.7146e-02, -6.0813e-02,  6.1539e-02,\n",
      "         5.1954e-03, -9.9260e-02,  2.6458e-01,  2.1956e-01,  2.2071e-02,\n",
      "        -3.2739e-02, -7.4059e-03, -3.3048e-03,  3.0760e-02,  8.0316e-03,\n",
      "         3.4296e-02, -1.1837e-01,  9.6501e-03,  2.2268e-01,  2.6526e-01,\n",
      "         2.1638e-02, -3.4117e-02, -9.0561e-02,  6.7185e-02,  2.6524e-01,\n",
      "        -6.2014e-02, -9.3166e-03,  1.9723e-01, -2.7817e-02,  1.2467e-01,\n",
      "        -2.9969e-02,  1.1454e-02,  8.6899e-02, -3.9249e-02, -2.4232e-01,\n",
      "         7.2747e-02,  7.5041e-02, -8.0870e-02,  4.1968e-02, -3.5321e-02,\n",
      "         6.9348e-02, -4.5608e-03,  4.7670e-02, -4.4456e-03, -5.0248e-02,\n",
      "         3.7222e-02,  1.8880e-03, -1.8293e-01, -1.5372e-01,  5.7452e-02,\n",
      "         3.6083e-02,  8.0035e-03,  2.4371e-01,  3.8896e-02, -5.6765e-03,\n",
      "        -2.2972e-01, -3.4820e-02, -3.1153e-02,  4.9172e-02, -5.8518e-02,\n",
      "        -3.3998e-02, -2.9306e-02, -8.6788e-03,  6.8595e-02,  9.9372e-02,\n",
      "         7.7511e-02, -3.9946e-01, -2.8897e-02,  5.4168e-02, -1.2323e-02,\n",
      "         5.0971e-02,  7.5626e-03,  9.2684e-03, -1.1990e-01, -3.4211e-02,\n",
      "        -5.0956e-02,  6.2167e-02,  8.6782e-03,  1.3184e-02, -1.0196e-02,\n",
      "         1.2857e+00,  6.3103e-02,  4.6367e-03,  8.0960e-03,  1.7326e-01,\n",
      "        -2.2882e-01,  2.6613e-01, -7.7841e-01, -8.8247e-02, -1.9183e-02,\n",
      "         5.7476e-02, -7.1976e-02,  3.2107e-03, -7.3144e-02, -5.6937e-02,\n",
      "         1.7182e-01,  1.2788e-01,  1.2054e-01,  2.5422e-02,  2.8944e-01,\n",
      "         1.3143e-02,  8.7476e-02,  5.9769e-02, -9.0866e-02, -3.0917e-02,\n",
      "        -3.1915e-02,  3.4482e-02,  3.2792e-01,  2.9329e-02, -2.4546e-02,\n",
      "        -6.2955e-02, -3.1642e-02, -7.6182e-02,  2.1799e-02,  2.8215e-01,\n",
      "        -4.5248e-02, -5.0524e-02,  1.0312e-01,  3.7096e-03, -3.3113e-01,\n",
      "        -6.5451e-03, -3.1881e-03, -1.6013e-02, -5.3292e-02, -1.1409e-02,\n",
      "        -3.9488e-03, -3.9327e-02, -5.9887e-02,  8.8121e-03, -3.1059e-02,\n",
      "         1.3988e-01, -5.7465e-02,  6.1953e-02, -4.3348e-02, -6.2803e-02,\n",
      "        -9.6458e-03,  5.3449e-02, -1.3884e-01,  2.6421e-02, -5.3485e-02,\n",
      "        -4.0322e-02,  4.3009e-02,  2.7627e-01,  3.6782e-02,  1.5764e-01,\n",
      "        -3.7675e-02,  1.4539e-02,  3.3524e-02, -9.5996e-02,  6.9877e-02,\n",
      "        -8.3219e-02, -5.0818e-02,  2.1248e-01,  6.0498e-02,  3.8974e-02,\n",
      "        -2.3514e-02, -1.3213e-01, -2.7783e-01,  5.1397e-02,  3.7470e-02,\n",
      "         1.0721e-01, -1.5672e-02,  1.0655e-01,  5.8129e-02, -6.6466e-02,\n",
      "         2.7478e-02,  2.2863e-01, -2.4149e-02,  7.5828e-02, -3.4686e-02,\n",
      "         4.8277e-02,  8.2409e-02,  2.9049e-02,  1.0367e-01, -4.8115e-01,\n",
      "         5.6688e-02,  2.1711e-02,  3.1469e-02,  6.4853e-03,  1.5035e-01,\n",
      "         1.0166e-01,  1.5511e-02,  1.6065e-01, -1.5590e-01,  5.9343e-02,\n",
      "        -3.1065e-02, -2.7349e-03, -6.7431e-02, -1.6244e-01,  8.5319e-02,\n",
      "         7.2648e-03,  7.2285e-02,  9.5868e-03,  6.8394e-02,  4.1458e-01,\n",
      "         2.9041e-01,  3.4396e-02,  7.8606e-02,  9.8162e-02,  1.2110e-02,\n",
      "         7.0974e-02,  9.4173e-02,  1.4365e-01, -2.1725e-01,  1.0777e-02,\n",
      "        -4.6671e-02, -5.2641e-02, -7.9822e-02,  1.4863e-01,  2.2086e-02,\n",
      "        -4.7950e-02,  3.1344e-03, -2.1002e-01,  3.1003e-02,  1.2355e-02,\n",
      "         1.9488e-02, -2.5180e-01, -3.1077e-02, -6.2950e-02,  4.6207e-02,\n",
      "         8.1460e-02, -8.6125e-02, -1.0722e-01, -8.9750e-02, -2.6743e-04,\n",
      "        -1.0039e-01, -2.3607e-02, -5.9142e-03, -1.1704e-02,  2.8974e-01,\n",
      "         2.1882e-02, -5.5282e-02,  7.0383e-02,  8.4561e-03, -2.9887e-02,\n",
      "         4.4571e-03, -2.1062e-01, -1.1387e-01, -5.9623e-02, -2.8303e-02,\n",
      "         4.6151e-02,  4.6253e-01,  1.4632e-02, -1.3950e-01,  1.4897e-01,\n",
      "         1.7881e-02, -5.1051e-02, -1.6028e-01,  1.2699e-01, -1.5196e-01,\n",
      "        -1.6314e-02,  1.3684e-01, -1.5946e-02, -2.7821e-02, -1.0003e-01,\n",
      "         1.1695e-01, -8.3180e-03,  1.4416e-02,  1.9368e-01,  1.7479e-02,\n",
      "         9.1197e-03,  2.4974e-02,  8.4054e-03, -1.0430e-01, -5.5954e-03,\n",
      "         5.1109e-03, -1.0709e-01, -4.4023e-02, -6.1489e-02,  2.6348e-02,\n",
      "         7.6498e-02,  7.0703e-02, -5.4153e-02,  1.3841e-02,  1.0698e-01,\n",
      "        -4.3909e-02, -8.4957e-02,  1.5278e-01,  1.2073e-01, -5.1321e-02,\n",
      "        -7.2769e-04, -5.7261e-03,  3.3071e-01,  3.4708e-02,  3.5088e-02,\n",
      "         1.3126e-01, -1.4654e-02, -1.9121e-02,  1.4101e-01, -6.5217e-02,\n",
      "        -9.4861e-02, -2.8714e-01,  2.0269e-01,  1.3509e-01, -5.6561e-02,\n",
      "         3.5484e-02,  4.4501e-02,  1.5804e-03,  6.4803e-02, -4.0606e-02,\n",
      "         4.6527e-02,  1.0703e-01,  1.5388e-01,  1.7709e-02,  2.5505e-02,\n",
      "         2.8129e-01, -2.4582e-02, -7.5241e-02, -4.8137e-02,  4.7598e-02,\n",
      "         3.1940e-02, -4.8185e-02, -1.0888e-01, -2.2084e-02,  2.2565e-01,\n",
      "         3.0260e-02,  7.4244e-02, -2.4621e-02, -2.0396e-03,  3.1107e-02,\n",
      "        -5.7875e-02, -2.5030e-02,  2.8781e-01, -1.7207e-01, -8.0321e-03,\n",
      "         2.3872e-02, -2.1069e-01,  5.7281e-02, -1.2417e-01,  4.7609e-02,\n",
      "         1.9888e-01,  3.3462e-02, -8.9881e-02,  4.5122e-02, -1.4224e-01,\n",
      "         7.9002e-02, -2.9726e-02, -8.1674e-03, -2.0759e-02,  1.1695e-01,\n",
      "        -2.0023e-01, -1.8965e-01, -5.0871e-02, -3.6011e-03, -8.6287e-03,\n",
      "         7.8778e-02, -5.3821e-02,  4.3446e-02, -1.4042e-01,  4.0514e-02,\n",
      "         4.3890e-02,  3.8137e-02, -1.1687e-02, -1.8774e-01,  1.4362e-01,\n",
      "        -8.0265e-02,  8.7258e-03, -5.0708e-03, -6.1546e-02,  6.5874e-02,\n",
      "         6.7633e-02, -4.2298e-02,  3.4523e-02,  3.2274e-02, -5.0726e-02,\n",
      "         2.2735e-03,  1.0389e-03,  9.3175e-02,  8.0988e-02, -1.7919e-02,\n",
      "        -5.9244e-02,  3.4083e-02,  1.0999e-01,  4.0307e-02,  9.2032e-02,\n",
      "         5.4610e-02,  6.8489e-02,  3.5795e-02,  1.6234e-01,  3.6057e-03,\n",
      "        -7.5231e-02, -2.0563e-02,  3.1694e-02,  1.2557e-01, -5.8902e-02,\n",
      "        -1.3119e-02,  1.4096e-02,  8.7029e-02, -5.0061e-02,  5.9233e-02,\n",
      "        -1.1616e-01,  2.3807e-02,  9.7721e-02,  4.2463e-02, -1.2802e+00,\n",
      "         4.5594e-01, -7.7613e-02,  9.2521e-02, -1.1331e-01, -2.3426e-01,\n",
      "        -3.8160e-01, -3.3462e-03, -1.2436e-01,  1.5280e-01,  8.6417e-02,\n",
      "        -4.9638e-02, -3.8469e-02,  4.2506e-02, -2.5768e-04, -3.7161e-02,\n",
      "         3.1604e-02, -6.6463e-02,  6.9631e-03,  9.3255e-02, -3.8409e-02,\n",
      "        -1.0615e-02, -1.7596e-01, -1.8678e-01, -5.1064e-02,  3.4293e-02,\n",
      "         1.0209e-01, -2.7974e-01,  2.2730e-01,  6.1457e-03, -1.1336e-01,\n",
      "        -1.3473e-01,  5.4974e-03, -1.4665e-01,  6.3085e-02,  8.2647e-03,\n",
      "         1.2271e-01, -2.3846e-02, -4.2220e-01,  8.5595e-03,  2.2206e-02,\n",
      "         5.9233e-02,  9.9846e-02, -2.4066e-03, -5.2669e-02, -1.7564e-01,\n",
      "         7.5076e-02, -1.3381e-01, -4.0828e-03,  4.8789e-02, -1.3195e-02,\n",
      "         6.0221e-01, -2.5593e-02, -7.8968e-02, -2.1146e-02,  2.5701e-02,\n",
      "        -7.6371e-02, -3.7180e-02,  3.3422e-02,  4.8929e-02, -2.7642e-02,\n",
      "         2.5606e-01,  4.7374e-02, -6.9107e-03,  2.4202e-02, -4.8710e-03,\n",
      "        -4.7797e-02,  9.0536e-02,  9.2176e-02, -2.1212e-02,  1.4358e-01,\n",
      "        -6.4221e-02,  7.1833e-03, -1.9455e-02, -2.6611e-01, -1.9386e-01,\n",
      "         2.6649e-01, -5.2804e-02,  1.5339e-02, -2.0304e-01,  4.0899e-02,\n",
      "         3.5567e-01,  2.7159e-02,  2.3820e-01, -5.1595e-02,  1.1098e-02,\n",
      "         9.6149e-02,  2.1387e-02, -1.5055e-02,  2.6905e-02, -9.7637e-03,\n",
      "         2.2443e-02,  5.7448e-04,  6.7294e-05,  2.5399e-01, -8.4097e-02,\n",
      "        -8.1272e-02, -5.1048e-02,  4.9478e-02,  6.8127e-02,  1.1317e-03,\n",
      "         5.7450e-04, -1.0712e-02,  3.5640e-02,  8.1500e-04,  7.8349e-02,\n",
      "        -3.1761e-02,  1.3692e-02,  1.3797e-02,  4.7065e-02,  5.5511e-02,\n",
      "         7.6770e-02,  1.9078e-02, -2.5029e-02,  1.2592e-01, -2.8228e-03,\n",
      "         1.8123e-02, -1.1726e-02, -4.0938e-03,  1.1135e-01, -5.4311e-01,\n",
      "        -1.8636e-01,  6.7051e-02, -3.5766e-02,  4.7546e-02,  5.7965e-03,\n",
      "        -2.4166e-02,  6.6453e-02,  1.8493e-01, -5.8704e+00,  2.6685e-01,\n",
      "         7.9413e-02,  1.8917e-01,  3.6191e-02, -1.7434e-02,  5.9981e-03,\n",
      "         2.6433e-02,  1.3377e-02,  1.2234e-01,  6.6080e-02, -1.6292e-02,\n",
      "        -8.7636e-02,  4.4461e-02, -2.3600e-02,  6.7214e-02, -2.9799e-02,\n",
      "        -2.6311e-01, -5.0249e-02,  3.2787e-01,  7.0540e-02, -1.2514e-01,\n",
      "         6.5230e-02,  1.2158e-01,  7.3859e-02,  1.0280e-01, -8.1316e-02,\n",
      "         1.1645e-01,  4.6767e-02, -5.4525e-02, -1.3075e-01,  7.3443e-02,\n",
      "        -6.1513e-02,  4.5667e-02, -6.4321e-03, -8.2565e-02,  9.6607e-04,\n",
      "        -1.2899e-02, -4.0067e-02,  2.2062e-01, -5.2131e-02,  1.2788e-01,\n",
      "        -1.2649e-01,  2.8782e-02,  7.5406e-02, -1.3556e-01,  1.8498e-01,\n",
      "        -1.0423e-02, -4.0259e-03, -1.3471e-01,  2.4441e-01,  8.5059e-02,\n",
      "        -1.2357e-01,  1.5583e-01, -8.2786e-02, -7.2862e-03,  6.1344e-02,\n",
      "         3.2660e-02,  4.7294e-02, -1.5708e-02,  1.4613e-01,  9.0359e-02,\n",
      "        -2.8418e-02,  1.9007e-02, -4.6264e-02, -2.0277e-02, -6.5790e-02,\n",
      "        -1.2692e-02, -2.8684e-02, -7.3855e-03,  2.5210e-02,  8.1806e-02,\n",
      "        -8.9393e-02, -4.9987e-02, -2.6187e-02,  9.7525e-02,  2.7850e-02,\n",
      "        -6.1222e-02,  5.8535e-02, -8.4268e-02, -1.0817e-01,  2.4327e-02,\n",
      "        -8.2687e-03, -2.3368e-02,  8.8121e-02, -1.5000e-01,  1.5327e-01,\n",
      "        -4.3570e-02, -3.7496e-02,  3.4537e-02, -1.1007e-01, -1.0210e-01,\n",
      "         3.2947e-01,  6.9199e-02,  1.0139e-01, -3.3702e-03,  6.5856e-03,\n",
      "         2.4151e-02,  2.5768e-01, -5.0801e-03,  1.6401e-01, -1.3046e-03,\n",
      "        -2.4805e-02, -1.9962e-01,  5.3950e-02, -1.0641e-01, -3.0505e-01,\n",
      "         7.6158e-02,  4.6415e-02,  1.2670e-02, -8.6423e-02, -9.0204e-02,\n",
      "         4.6739e-02,  5.4010e-02,  8.6047e-02,  5.8762e-02,  5.5289e-02,\n",
      "        -9.8815e-03, -8.6927e-02, -3.1028e-01,  5.9826e-03,  2.6246e-01,\n",
      "        -3.6072e-03, -1.7293e-02, -3.1377e-01, -8.4158e-02,  4.9801e-02,\n",
      "        -1.1496e-01,  7.7817e-02,  6.6827e-03, -1.5663e-01,  3.8741e-02,\n",
      "         3.8183e-02,  3.6036e-02, -1.6887e-02, -3.0175e-01, -4.6946e-02,\n",
      "        -4.9646e-04,  5.1443e-02,  5.2105e-02,  1.5220e-01, -7.8310e-03,\n",
      "        -2.7707e-02,  7.3394e-03, -1.0147e-01,  4.2873e-02, -3.1715e-01,\n",
      "         1.2311e-02,  2.8120e-01,  1.7156e-02, -1.7203e-03, -2.7266e-01,\n",
      "        -3.7337e-02,  1.7922e+01,  4.8146e-02,  6.1149e-02,  4.3456e-02,\n",
      "         9.3302e-02, -2.2901e-01,  1.3256e-03, -5.7648e-02, -4.0595e-01,\n",
      "        -1.2382e-02,  5.6782e-02,  2.7268e-02,  2.9630e-02, -2.0628e-02,\n",
      "        -3.1840e-02,  3.8756e-03,  7.7852e-02, -1.7593e-02,  2.8851e-01,\n",
      "         2.5598e-02,  2.9920e-02,  1.3102e-01, -7.6504e-03,  4.5557e-01,\n",
      "         1.7605e-01, -8.0876e-03,  9.3638e-02], device='cuda:0')]\n",
      "[tensor([-5.8686e-02,  3.4657e-02, -3.6262e-02,  6.7395e-02,  1.6973e-01,\n",
      "        -4.4144e-02, -6.1284e-02,  2.5659e-01,  2.6240e-02,  3.1222e-02,\n",
      "         3.4324e-01, -4.1629e-03,  4.5309e-02, -6.0351e-02, -3.3048e-02,\n",
      "         5.5507e-02,  6.0345e-02, -6.3364e-02,  1.7767e-01,  4.9507e-02,\n",
      "        -6.1910e-02,  9.0025e-03, -2.0084e-02,  4.9632e-02, -3.6431e-02,\n",
      "         7.5115e-02, -3.9066e-02, -1.0489e-01, -2.8162e-02, -6.1412e-03,\n",
      "        -6.7616e-03, -9.6993e-03,  6.6534e-02,  1.0432e-02, -7.0586e-03,\n",
      "         1.3621e-01, -1.8189e-02,  3.3750e-02, -5.6847e-02,  7.6918e-02,\n",
      "         3.2925e-02,  1.0435e-02, -9.3532e-02,  3.6769e-02,  5.4276e-02,\n",
      "        -3.3575e-02,  1.7695e-02,  6.9353e-02,  3.7339e-02, -8.7500e-02,\n",
      "        -2.9894e-02,  2.2758e-01,  7.7331e-02,  1.9990e-01,  1.8427e-01,\n",
      "        -4.5861e-02,  4.0104e-02, -3.0493e-02,  5.1540e-02,  7.8686e-02,\n",
      "         8.1790e-02,  1.3621e-03, -4.0332e-03, -3.7186e-02,  1.4164e-01,\n",
      "        -1.4697e-02,  6.4026e-02,  2.7380e-02,  6.2387e-02, -9.7022e-02,\n",
      "         6.9384e-02, -7.8445e-02, -5.7939e-02, -1.2348e-01, -7.1397e-02,\n",
      "         1.2052e-02,  8.4463e-02, -8.6851e-03,  3.6717e-02, -2.0607e-02,\n",
      "        -7.8281e-02,  1.4873e-01,  6.4233e-03,  2.8095e-02,  1.4286e-01,\n",
      "         1.9436e-02, -4.2079e-02, -4.9162e-02,  8.2736e-02,  1.7390e-01,\n",
      "         6.8379e-02,  3.6963e-02,  1.6139e-01,  8.7656e-02, -4.9685e-03,\n",
      "         6.8431e-03,  2.6192e-02,  1.6474e-02,  3.7665e-02, -6.9183e-02,\n",
      "        -6.1743e-02,  9.2597e-02,  6.1583e-02,  4.1057e-01, -2.3185e-01,\n",
      "        -3.6186e-02, -2.9486e-02,  7.9501e-02,  8.3835e-02,  1.2186e-01,\n",
      "         1.2267e-01,  4.0320e-03,  3.4353e-01, -6.5496e-02,  1.9598e-01,\n",
      "         1.5400e-02, -7.7571e-02,  1.7984e-01,  9.6503e-02, -8.9794e-02,\n",
      "        -3.9626e-03, -9.5565e-02,  3.1898e-02,  1.2147e-02, -5.8875e-02,\n",
      "        -5.1287e-02, -1.0329e-01,  4.2649e-02,  3.0891e-02, -2.7330e-01,\n",
      "         2.0544e-02,  8.4731e-02, -2.4805e-02,  5.9370e-02,  6.1860e-02,\n",
      "        -5.3319e-02,  7.5700e-02,  9.6616e-02,  4.1858e-02,  1.2218e-01,\n",
      "         1.3713e-01, -2.1572e-01, -1.9354e-02, -1.0661e-01,  1.1859e-02,\n",
      "         2.7676e-01,  6.7816e-02, -6.9454e-02,  4.4493e-05,  2.1123e-01,\n",
      "        -7.7910e-02,  4.2684e-01, -1.1289e+00,  6.8413e-02, -3.2342e-02,\n",
      "         1.3034e-01, -1.2374e-02,  1.1863e-02, -1.7234e-01,  3.1796e-02,\n",
      "         1.3305e-01,  1.3466e-01, -1.1527e-02, -4.2703e-02, -1.2768e-01,\n",
      "         5.3290e-04, -1.7169e-02, -3.7716e-02,  2.3147e-03, -3.9741e-02,\n",
      "         2.8398e-02, -2.9436e-03,  4.6125e-02,  5.0067e-03,  3.1719e-02,\n",
      "         8.5233e-02,  6.2527e-02,  1.4512e-01, -4.3552e-02, -1.3788e-01,\n",
      "         1.0431e-01, -6.1307e-02,  2.5960e-02,  7.2223e-02,  1.2868e-01,\n",
      "        -1.5903e-02,  6.0214e-02, -5.4226e-02, -4.6949e-02, -8.3303e-02,\n",
      "        -1.1903e-03, -5.1007e-02, -1.3925e-01,  1.6333e-02, -9.3232e-03,\n",
      "         6.7208e-03, -2.9187e-02, -2.0512e-02,  8.7254e-02, -1.0959e-01,\n",
      "         2.9522e-02,  4.0622e-02,  1.4028e-01, -1.8232e-01, -1.1266e-01,\n",
      "        -1.2389e-01,  1.3171e-02,  2.2294e-02,  5.2658e-02,  2.6388e-02,\n",
      "         4.7115e-02, -8.8414e-02, -5.1040e-02,  1.0698e-03, -2.1457e-02,\n",
      "        -2.0475e-02, -5.4579e-02,  3.4204e-02,  6.9992e-02,  2.1090e-02,\n",
      "        -1.1336e-01, -2.7742e-02,  6.7070e-03,  4.4773e-02, -7.8142e-03,\n",
      "         1.5295e-02,  9.5341e-02, -3.4296e-02,  6.1573e-02, -6.6885e-02,\n",
      "         6.5403e-03,  9.4729e-02,  4.1080e-02, -6.4571e-02, -2.9622e-02,\n",
      "         2.8524e-02, -1.7332e-01, -8.2889e-02,  1.9123e-02,  3.8210e-01,\n",
      "        -3.9240e-02,  6.4786e-03,  2.4492e-02,  6.2677e-03,  1.9100e-01,\n",
      "         1.9786e-02,  1.1904e-01,  4.2306e-02, -1.2544e-01, -1.3017e-02,\n",
      "         1.0566e-01,  9.7551e-02, -2.6303e-02,  2.5565e-02, -5.8789e-02,\n",
      "        -1.5689e-01, -1.1780e-01,  3.6471e-02, -2.4619e-02,  1.0341e-01,\n",
      "        -6.0555e-02,  6.7055e-02,  1.2504e-02, -6.1417e-02,  5.3459e-02,\n",
      "         8.9101e-02,  3.5087e-01,  1.9062e-01, -7.2536e-02,  3.8883e-03,\n",
      "        -8.6112e-02,  8.4270e-02,  6.5885e-02, -3.2056e-02, -2.9802e-02,\n",
      "         3.7826e-02, -9.1945e-02,  6.8309e-02, -7.7586e-03, -3.2024e-02,\n",
      "         1.0589e-01,  5.8416e-02,  1.7370e-02,  8.7653e-02, -3.3105e-02,\n",
      "         6.8278e-02,  2.0173e-01, -2.1078e-02,  1.4272e-01, -3.2571e-02,\n",
      "        -6.5458e-02,  8.0639e-02,  4.2333e-03,  9.9761e-02,  9.0362e-02,\n",
      "        -2.5953e-02,  2.8209e-02, -3.1455e-03, -2.2201e-02,  9.5753e-02,\n",
      "        -2.9993e-02,  7.1991e-02, -2.7955e-02, -5.6980e-02,  1.1047e-01,\n",
      "         2.3150e-01,  3.9543e-01,  1.8553e-02, -2.1149e-01,  1.5101e-01,\n",
      "        -5.9988e-02, -1.0050e-02, -1.0577e-01, -1.6690e-01, -4.6637e-01,\n",
      "         7.6914e-02, -2.3580e-01,  1.9363e-01,  2.9340e-02,  1.9475e-02,\n",
      "         1.0418e-01,  1.0277e-02,  9.1464e-02, -3.2626e-02,  2.1809e-02,\n",
      "        -1.5393e-01,  5.3024e-02, -8.1621e-02, -1.1211e-01,  5.5002e-02,\n",
      "        -4.3588e-04, -3.5058e-02,  6.3378e-02,  1.6125e-01, -2.3030e-01,\n",
      "        -1.1444e-01,  7.8776e-03,  1.4700e-02,  5.8491e-02,  1.2484e-01,\n",
      "         7.3411e-02,  3.0119e-02,  2.1412e-01,  8.5773e-02,  1.2391e-01,\n",
      "        -3.9807e-02, -1.9400e-03,  1.5262e-01, -4.2454e-02,  2.4951e-02,\n",
      "         1.4073e-01, -1.2975e-02, -8.1054e-02, -1.5083e-01, -6.9377e-03,\n",
      "        -4.8851e-02,  9.2714e-02,  6.9336e-02, -7.6882e-03,  1.3259e-01,\n",
      "        -4.3739e-02, -7.4436e-02, -1.6071e-02,  6.5335e-02, -3.4002e-02,\n",
      "        -1.2368e-02, -1.3871e-01,  1.8495e-02,  9.0479e-02, -3.0778e-02,\n",
      "        -6.7138e-02,  2.1787e-02, -1.2775e-02, -3.4564e-02,  4.6455e-03,\n",
      "         1.8678e-02,  4.1192e-02, -2.7787e-01,  8.1934e-03,  1.6247e-01,\n",
      "         8.6759e-02,  5.3075e-02, -9.9176e-02, -1.6702e-02,  8.4432e-03,\n",
      "         4.4630e-03,  1.3298e-02,  2.3342e-01, -9.4504e-02, -3.4176e-02,\n",
      "         1.1553e-01, -6.7317e-02, -3.2904e-02,  2.6794e-02,  5.6571e-02,\n",
      "        -1.2092e-01,  7.4411e-02,  1.7189e-02,  1.8345e-02, -6.0869e-02,\n",
      "         7.3354e-02,  3.7071e-02, -1.0366e-01, -7.6934e-02,  1.7485e-02,\n",
      "         1.7236e-01,  5.7823e-02, -7.9561e-02,  5.2693e-02, -2.5710e-02,\n",
      "         9.9134e-02, -2.0931e-01, -2.9754e-02, -7.1109e-02,  5.1577e-02,\n",
      "        -4.5420e-02,  1.6401e-01,  4.5441e-04, -9.6231e-02,  8.0705e-02,\n",
      "         2.2679e-02, -1.3111e-02, -6.9968e-02,  6.7841e-03,  5.4353e-02,\n",
      "         2.9056e-02,  1.3730e-02,  4.1828e-02, -2.6436e-02,  4.1219e-02,\n",
      "         4.1020e-02,  6.3742e-02, -2.9188e-02,  6.5574e-02, -3.6503e-04,\n",
      "         1.3180e-01,  3.6114e-02, -1.2032e-02, -6.5718e-03, -4.3328e-02,\n",
      "        -5.9904e-03, -2.6730e-02, -1.1602e-02, -1.2298e-01,  1.1099e-02,\n",
      "         1.7373e-02, -8.8630e-03,  1.2329e-02, -6.2061e-02,  5.3122e-02,\n",
      "         2.7602e-02, -3.8662e-03,  9.3306e-02, -2.6043e-02, -4.8109e-02,\n",
      "         5.8065e-02, -2.0045e-02,  2.9694e-02,  6.3331e-02, -2.4524e+00,\n",
      "        -8.3201e-02, -4.2473e-02, -7.9881e-02,  6.6316e-02,  3.5572e-02,\n",
      "        -8.0040e-02,  1.1270e-02, -1.6094e-02,  2.7410e-02, -5.7041e-03,\n",
      "         4.6905e-02, -2.0145e-02,  8.8875e-02,  1.3080e-01,  6.2925e-02,\n",
      "         5.9472e-03,  7.2554e-02,  1.9620e-02,  1.8886e-02, -1.8658e-02,\n",
      "         1.1078e-01,  9.1579e-02, -4.3546e-02,  2.4646e-02,  6.1319e-02,\n",
      "        -1.9695e-02, -4.7405e-02,  2.1349e-01, -8.8513e-02, -6.4376e-02,\n",
      "        -1.9289e-01, -1.4572e-01,  3.7525e-02,  1.7814e-02, -2.5529e-02,\n",
      "         9.3560e-02, -4.3108e-02, -2.0264e-01, -2.1701e-01, -6.5573e-02,\n",
      "        -2.8701e-02,  1.0239e-02,  3.9768e-02,  3.2102e-02, -1.0140e-01,\n",
      "        -6.2599e-03, -2.0533e-01, -7.5654e-02, -3.7314e-02, -3.3561e-02,\n",
      "         1.2939e-01,  3.5082e-02, -8.1676e-02,  4.5991e-03,  3.3036e-02,\n",
      "         8.9205e-02,  1.4120e-02,  3.3902e-02,  1.3562e-02, -4.3991e-02,\n",
      "        -8.0012e-02,  1.0789e-02,  6.0130e-02,  2.5835e-02, -5.8779e-02,\n",
      "        -6.6417e-02,  1.1639e-01, -2.4928e-01, -6.8550e-02,  1.3081e-01,\n",
      "         9.3756e-02,  6.4134e-02,  1.2341e-01, -2.4714e-01,  1.4890e-01,\n",
      "        -7.8769e-02,  2.9589e-02, -9.9482e-03, -1.3936e-03,  4.1132e-02,\n",
      "         5.6917e-02,  4.7992e-03, -5.1279e-04, -9.9041e-03,  1.6431e-02,\n",
      "        -2.5330e-02,  3.4663e-02, -6.5176e-02, -2.7741e-02,  9.3396e-02,\n",
      "         1.6055e-02, -1.4824e-01, -1.2842e-01,  1.0477e-01, -1.9884e-01,\n",
      "         1.2958e-01, -4.1218e-02,  7.0921e-03,  2.3999e-02,  3.6222e-03,\n",
      "         1.4935e-01, -6.3045e-02, -9.6109e-03, -2.1407e-02,  3.4108e-02,\n",
      "        -3.4358e-02,  4.0311e-02,  3.5341e-02,  4.1186e-02, -1.3200e-02,\n",
      "         1.2097e-01, -2.6548e-02, -2.7633e-02, -2.6488e-01,  7.2424e-02,\n",
      "        -6.2561e-02,  3.7219e-02, -7.0129e-03,  9.6639e-02,  1.3336e-01,\n",
      "        -7.2225e-02, -5.4260e-02,  1.6926e-01,  1.2194e-01,  2.1040e-01,\n",
      "         2.0665e-02,  5.3655e-02,  9.0209e-02, -6.0660e+00,  1.9815e-01,\n",
      "         1.0067e-01,  4.0178e-02, -5.5103e-02, -5.4977e-02,  1.5293e-02,\n",
      "         6.0973e-03,  3.2590e-02, -2.1882e-02,  4.5758e-02, -1.3298e-01,\n",
      "        -7.5412e-02,  1.1064e-01, -5.0708e-02,  1.0400e-01,  6.2349e-02,\n",
      "         4.0958e-03,  7.2832e-03,  4.7970e-02,  2.0541e-02, -3.0015e-03,\n",
      "         1.0932e-01,  2.0290e-01,  7.4562e-03,  7.6181e-02, -2.0645e-02,\n",
      "        -6.3684e-02, -5.3366e-02, -5.8062e-02,  3.6212e-02, -8.6601e-03,\n",
      "        -4.0603e-02,  3.1207e-02, -6.1104e-02,  1.7797e-01, -7.4416e-02,\n",
      "         1.0416e-02,  5.1840e-02, -7.9046e-02, -2.8612e-02, -1.0040e-01,\n",
      "        -1.0599e-01,  2.3691e-02, -1.9678e-02, -1.8309e-01,  9.6531e-02,\n",
      "        -4.2041e-02, -4.0486e-02, -6.2793e-03,  1.0297e-01,  4.1180e-02,\n",
      "         1.4929e-01, -1.7037e-04, -1.4598e-02,  9.5555e-02,  1.0745e-01,\n",
      "        -4.3391e-02,  1.7046e-01,  1.3014e-02,  6.5711e-02, -2.2385e-01,\n",
      "        -7.8230e-03, -4.3115e-02,  2.5947e-02,  9.1146e-02,  2.4447e-01,\n",
      "         1.4860e-02, -1.2551e-02,  5.1229e-02,  4.8745e-02,  2.7759e-02,\n",
      "         1.3239e-01,  3.4289e-02, -6.3711e-02,  7.6113e-02,  6.1095e-02,\n",
      "         2.7420e-02,  4.9382e-02,  8.4234e-02,  7.9855e-02,  4.6882e-02,\n",
      "        -2.3556e-02,  5.6986e-02,  6.8099e-02, -2.5283e-02,  2.8102e-01,\n",
      "        -3.3596e-02,  3.5598e-02,  3.7739e-02,  8.2202e-02, -7.2463e-02,\n",
      "         5.6859e-02, -5.6959e-04,  3.2004e-02,  3.1234e-02, -1.3851e-01,\n",
      "        -3.9905e-02, -1.2660e-02,  4.0477e-02,  1.3361e-01,  4.4272e-02,\n",
      "        -7.5084e-02, -7.0893e-02,  6.5180e-02,  6.4691e-02, -4.7160e-01,\n",
      "         1.0119e-01,  1.1524e-02,  3.9679e-02, -8.2091e-02,  1.2555e-01,\n",
      "        -1.0526e-02,  6.6270e-02,  5.1852e-02,  7.7070e-02,  3.7613e-02,\n",
      "        -1.1787e-01, -1.9918e-02,  3.3708e-01,  2.9900e-02,  9.0657e-02,\n",
      "        -5.1976e-02, -1.4199e-02,  1.3357e-01,  1.1748e-02,  2.0530e-02,\n",
      "         9.9844e-02,  7.9010e-02,  5.9043e-03, -1.6186e-02, -3.2431e-02,\n",
      "         1.1646e-01, -3.2852e-02,  2.8255e-02,  1.7520e-01,  2.4886e-02,\n",
      "        -1.1699e-01, -8.2359e-03,  1.1723e-02,  2.5405e-01, -1.3690e-01,\n",
      "        -3.7795e-02, -1.5247e-01, -5.1454e-02,  9.1208e-03, -2.7137e-03,\n",
      "         5.5741e-02, -5.4926e-03, -7.5031e-02,  2.3432e-02,  3.5136e-02,\n",
      "         2.6945e-03,  1.7913e+01,  3.5015e-02,  3.4483e-02, -5.2864e-02,\n",
      "         8.1884e-04,  1.8391e-02,  7.6135e-03,  2.8166e-02, -1.3558e-01,\n",
      "        -4.2786e-03,  1.1684e-02,  7.9284e-02, -1.8770e-01, -5.4921e-02,\n",
      "         3.5950e-02,  1.4967e-01,  5.8822e-02,  1.5599e-03,  2.6414e-01,\n",
      "         3.7247e-02, -2.9759e-02,  9.4233e-02,  3.7950e-02,  1.1123e-01,\n",
      "         7.2338e-02,  3.1380e-02,  3.6135e-01], device='cuda:0')] [99]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[500])\n",
    "print(vectors[448],key_inds[448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce17ef64-6453-4a41-bae7-82d4972543f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2535\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2535, 768])\n"
     ]
    }
   ],
   "source": [
    "X = list()\n",
    "for i in range(len(vectors)):\n",
    "    Sum = 0\n",
    "    for j in range(len(vectors[i])):\n",
    "        Sum += vectors[i][j]\n",
    "    X.append(Sum)\n",
    "print(len(X))\n",
    "print(type(X))\n",
    "X = torch.stack(X)\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "800b6902-f985-499f-b310-49374feae1f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y[\u001b[38;5;241m1000\u001b[39m],X[\u001b[38;5;241m1000\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "y = data['label']\n",
    "y = torch.tensor(y).to(device)\n",
    "print(y[1000],X[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "413b9e52-1875-4e4c-8c07-6a0c9d18f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.6003e-02,  6.2943e-02, -2.2935e-03,  2.5985e-03,  3.4805e-02,\n",
      "        -2.1132e-01, -5.6319e-02,  4.2129e-01,  9.8496e-03,  1.2241e-01,\n",
      "         1.6987e-01, -6.3021e-02,  3.1096e-01,  1.2543e-01,  7.4294e-02,\n",
      "         7.3950e-02,  5.0596e-02,  5.2429e-03,  1.2735e-01,  7.1985e-02,\n",
      "         1.1167e-01,  1.3440e-01, -9.7626e-03,  1.7524e-01,  8.2241e-02,\n",
      "        -7.4658e-02, -2.0493e-02, -1.0414e-01,  1.7133e-01,  2.6297e-02,\n",
      "         6.4875e-02, -5.9976e-02,  1.0653e-02, -1.0010e-01, -1.9897e-02,\n",
      "         1.2702e-01,  3.1713e-02, -5.0852e-02, -2.1265e-01,  5.4170e-02,\n",
      "         3.9952e-02, -3.7870e-02,  6.7580e-02,  3.5146e-02,  6.4087e-02,\n",
      "         7.5251e-03, -2.8026e-02,  1.3864e-02, -1.9209e-02, -9.9757e-02,\n",
      "         2.3889e-02, -2.3436e-01,  7.5889e-02,  1.5338e-01, -1.0456e-01,\n",
      "        -4.8758e-02,  6.8041e-02, -3.7255e-02, -1.9131e-01,  1.7387e-01,\n",
      "         1.1611e-01,  3.2410e-02,  1.4867e-01,  1.4538e-02,  1.4758e-03,\n",
      "         3.4975e-02, -7.1496e-02, -3.0925e-03, -2.0447e-01,  6.4438e-03,\n",
      "         1.8459e-01,  2.0173e-01,  3.8581e-01,  4.1033e-02, -7.7871e-03,\n",
      "        -5.3512e-02, -2.0017e-02,  3.1333e-02,  8.2583e-03,  1.4883e-03,\n",
      "         2.9015e-02,  1.7497e-01,  1.3087e-02,  1.3803e-01,  7.6833e-03,\n",
      "        -5.0403e-02, -7.7533e-02, -2.2534e-02,  5.0733e-02,  2.6023e-01,\n",
      "        -5.6820e-03, -7.5975e-02,  2.3864e-01,  1.9444e-01,  1.4697e-01,\n",
      "        -1.7562e-02,  1.2949e-01,  9.7096e-02, -2.1048e-02,  6.7539e-02,\n",
      "         4.8430e-02, -2.2194e-01, -7.4832e-02, -4.7932e-02, -1.3306e-02,\n",
      "         8.0776e-03,  4.9455e-02, -5.3221e-02,  9.1321e-02,  1.7201e-03,\n",
      "         6.2414e-02, -8.6870e-02, -4.5440e-02, -2.7703e-02,  1.1203e-01,\n",
      "         6.6264e-02, -1.3985e-01,  1.1696e-01,  1.4913e-01,  1.1513e-01,\n",
      "         2.2604e-01,  1.0043e-02, -1.4550e-01,  7.1284e-02,  1.4013e-02,\n",
      "        -2.0556e-02, -1.7682e-02,  9.9141e-02, -1.0383e-02,  3.6589e-02,\n",
      "         1.8636e-03, -4.4713e-02, -2.4242e-02,  7.7096e-02,  2.1377e-03,\n",
      "         1.1892e-01, -9.5147e-03,  3.2695e-02, -1.0410e-01, -1.3842e-01,\n",
      "         9.3540e-02,  2.5066e-01, -4.3283e-02, -2.8803e-01, -3.3710e-02,\n",
      "         1.3252e+00,  1.4458e-02, -6.8581e-03,  8.6490e-02,  2.3419e-01,\n",
      "         7.0135e-02,  3.6405e-01, -1.0661e+00, -2.6604e-02, -5.2788e-02,\n",
      "         6.8121e-02, -5.1892e-01,  4.4556e-03, -1.3041e-01,  6.5401e-02,\n",
      "         2.0655e-02,  5.1388e-02,  1.5040e-01,  1.2535e-02,  2.5950e-01,\n",
      "         6.2147e-02,  6.5474e-02, -2.5350e-02, -9.6297e-02, -1.7802e-01,\n",
      "         1.8525e-03,  1.2164e-04,  2.2965e-01, -5.6657e-02, -1.1739e-02,\n",
      "        -1.6131e-02, -3.3717e-02, -2.2757e-01,  1.2763e-02,  1.5230e-01,\n",
      "        -9.3413e-02, -1.2251e-02,  9.2504e-02,  3.6148e-02, -1.9429e-01,\n",
      "         4.4885e-02, -1.1696e-02,  1.0658e-02, -8.7272e-02, -3.3784e-02,\n",
      "        -2.2827e-03,  1.0845e-01, -1.4137e-01,  1.0168e-01, -8.3807e-02,\n",
      "        -6.7897e-02,  6.4866e-02,  4.0212e-02, -1.9247e-02, -5.3637e-02,\n",
      "         7.5289e-02,  7.5113e-02,  2.1001e-01, -1.6080e-01, -1.6858e-01,\n",
      "        -1.4934e-01, -9.6953e-02,  1.9007e-01, -3.0545e-03,  3.9162e-03,\n",
      "         7.5978e-02,  1.2350e-01,  1.6267e-01,  1.2440e-02,  2.4405e-02,\n",
      "        -1.7243e-01, -5.6709e-02,  1.8542e-01,  1.4156e-01,  6.1558e-02,\n",
      "        -3.4650e-02, -1.9166e-01, -2.6547e-01,  1.0956e-01, -1.7164e-02,\n",
      "        -1.1793e-02,  9.0906e-03, -4.7403e-04,  6.3305e-02, -1.0539e-01,\n",
      "         2.3174e-02,  1.0505e-01,  1.1081e-01, -1.9631e-01, -5.3019e-02,\n",
      "        -1.5869e-01,  2.4343e-02, -9.1027e-03,  1.8694e-02, -5.0092e-01,\n",
      "        -2.9829e-02,  7.4379e-02,  6.5352e-02, -4.0456e-02,  1.4499e-01,\n",
      "         1.1927e-01, -4.1584e-02,  4.6589e-02,  1.8344e-01, -2.7470e-02,\n",
      "         1.5532e-01,  6.6628e-02,  5.5955e-02,  1.3076e-01,  9.6062e-03,\n",
      "         2.3146e-01, -1.5682e-01,  2.4305e-02, -1.5124e-01,  5.3305e-02,\n",
      "        -3.0754e-02,  2.2757e-02,  7.6375e-02,  8.0179e-02,  2.6386e-02,\n",
      "        -1.0004e-01, -2.4504e-01, -7.6613e-02,  2.0346e-01, -7.1507e-02,\n",
      "        -1.7249e-01, -9.8052e-02, -5.3877e-02, -5.5961e-02, -2.1477e-02,\n",
      "         8.9196e-03,  5.9132e-02,  4.7248e-02,  1.8173e-01,  8.5309e-03,\n",
      "         5.8795e-02, -4.1582e-02, -1.1429e-01,  4.4337e-02,  4.9057e-02,\n",
      "         2.3316e-01, -5.4120e-03,  2.2068e-01, -6.1863e-02, -8.5107e-02,\n",
      "        -3.1348e-02, -2.7989e-01, -6.9805e-02, -3.2184e-02,  5.3019e-01,\n",
      "         6.0548e-02,  6.7593e-03,  1.2622e-01, -1.8254e-01, -8.1565e-02,\n",
      "        -8.6093e-02, -2.3267e-01,  4.6932e-02, -3.2162e-02,  2.4495e-01,\n",
      "         2.5558e-01,  5.0255e-01, -2.5316e-02, -3.6687e-01,  2.2747e-01,\n",
      "         5.0061e-02, -1.2963e-01, -2.1340e-01,  9.5692e-02,  2.7034e-02,\n",
      "        -5.4885e-02,  5.5318e-01, -2.0798e-01,  5.0818e-02,  1.3626e-01,\n",
      "         1.3049e-02, -3.2431e-02, -6.9347e-02,  7.0783e-02,  4.9543e-02,\n",
      "        -3.9123e-02,  1.8604e-02, -3.6797e-03,  8.5189e-03,  7.0020e-02,\n",
      "        -7.5778e-02,  6.8072e-02, -5.3249e-02, -8.0798e-02,  7.4431e-02,\n",
      "         1.0796e-02,  2.2235e-02, -5.5298e-02,  7.1134e-02, -5.4067e-04,\n",
      "        -6.0916e-02,  8.4149e-03,  1.3752e-01,  6.6982e-02, -9.2467e-02,\n",
      "         5.1882e-02,  2.4361e-02, -3.3606e-03,  8.4729e-02,  2.5358e-02,\n",
      "         1.4694e-02, -1.5144e-01,  3.7966e-02,  1.6177e-01, -1.8741e-01,\n",
      "        -8.5923e-03,  2.7428e-01,  3.3856e-02, -2.0304e-02, -1.6035e-01,\n",
      "         2.0482e-03, -9.5496e-03,  1.1258e-01,  1.5282e-01,  1.2751e-02,\n",
      "         7.9705e-02, -2.5191e-01,  2.3550e-01,  7.9441e-02, -6.3057e-02,\n",
      "         2.1313e-02,  1.7430e-02, -2.2683e-03,  4.0468e-02, -1.5839e-03,\n",
      "         6.4937e-02,  9.8034e-02, -1.7004e-01,  2.0189e-02, -9.0661e-02,\n",
      "         1.9658e-02, -4.5015e-02,  9.7072e-02,  3.0764e-02, -6.6245e-02,\n",
      "         1.2761e-01,  7.0814e-02, -1.0238e-01,  1.2989e-01, -2.4561e-02,\n",
      "         2.1737e-01, -1.6135e-01,  6.5427e-02, -7.6824e-02,  7.9966e-02,\n",
      "        -4.6252e-02,  2.8396e-02, -1.1198e-01,  8.9733e-02,  3.8474e-01,\n",
      "         1.8499e-01, -2.7100e-02, -2.9213e-01, -5.4801e-02, -1.6829e-02,\n",
      "        -3.5231e-02, -2.2332e-01,  7.3873e-02,  5.4839e-02,  4.7137e-02,\n",
      "        -1.7509e-02, -1.3758e-01,  5.9413e-03,  7.6854e-02,  5.5625e-02,\n",
      "        -9.9308e-03,  7.9010e-02,  4.2863e-02,  2.5118e-01,  8.9045e-02,\n",
      "         2.6088e-02, -8.3321e-03,  2.2562e-02, -4.2538e-03,  7.5752e-03,\n",
      "         1.7112e-01, -7.9615e-02,  4.5891e-03,  8.5617e-02, -1.7610e-02,\n",
      "         6.0429e-04,  2.8586e-02,  4.5820e-02, -2.6550e-03,  9.8456e-02,\n",
      "        -2.4627e-02,  7.4707e-02, -2.9370e-02,  2.3638e-02,  1.3269e-02,\n",
      "         1.6711e-01,  6.1365e-02,  2.8617e-02,  5.5908e-02, -3.7099e-02,\n",
      "        -1.2796e-02,  1.8125e-02,  5.8034e-03,  1.5593e-01, -5.2561e-02,\n",
      "        -3.0197e-02,  3.7546e-02,  8.5734e-02, -9.3404e-02, -3.8528e-02,\n",
      "         1.0367e-01, -2.0236e-01,  1.6954e-01,  6.2403e-02, -1.8047e+00,\n",
      "         2.4675e-01,  6.9425e-02,  1.8497e-01, -8.8293e-02, -1.0199e-01,\n",
      "        -5.7190e-02,  1.2573e-02,  9.7483e-02, -1.3511e-01, -1.1754e-01,\n",
      "         1.8633e-02, -5.7925e-02,  5.6421e-02,  3.2659e-02,  1.4989e-01,\n",
      "         1.2130e-02, -2.7263e-02,  1.8826e-01,  1.6246e-02, -4.9329e-02,\n",
      "        -6.0463e-03, -1.0281e-01,  1.0114e-01,  5.9820e-02, -3.1658e-02,\n",
      "         7.8425e-02,  7.2835e-03,  2.8511e-01, -8.4731e-02, -1.9459e-01,\n",
      "         5.4539e-02, -2.0807e-02,  1.6733e-02, -2.8325e-02,  2.7284e-02,\n",
      "        -1.0682e-01, -5.0297e-03, -1.6867e-01,  1.9812e-01, -7.2723e-02,\n",
      "         1.8639e-02, -4.6564e-03, -4.0458e-02, -9.1415e-03, -7.4359e-02,\n",
      "         9.1014e-02,  5.7868e-02,  8.9513e-03,  1.1860e-01,  1.9229e-02,\n",
      "         3.9841e-01,  4.7134e-02,  3.1253e-02, -2.4444e-02,  2.4999e-01,\n",
      "         1.1170e-01, -9.2409e-02,  6.2507e-02,  8.7236e-02, -1.0357e-02,\n",
      "         9.8041e-02,  3.3857e-02,  6.5257e-02,  1.8561e-03, -1.9411e-01,\n",
      "         1.5613e-02, -4.7633e-02, -1.7851e-01,  7.2832e-02,  1.1964e-02,\n",
      "         1.4555e-01, -4.8431e-02,  1.9237e-01, -4.5269e-01, -1.6514e-01,\n",
      "         7.8996e-02,  8.0928e-03, -2.8966e-02,  2.3738e-02,  1.1376e-01,\n",
      "         2.7740e-02, -5.9431e-03, -2.8955e-02, -6.4975e-03,  3.1046e-02,\n",
      "         5.8191e-02,  1.5227e-01, -3.1833e-02,  4.8402e-02,  6.8929e-02,\n",
      "        -9.3154e-03, -1.7511e-01, -1.6900e-02,  4.9224e-02, -5.9610e-02,\n",
      "         6.1293e-02,  5.1188e-02,  8.4511e-03,  1.8330e-01,  2.1173e-02,\n",
      "         2.1221e-03,  1.9966e-02,  2.8255e-02, -2.1994e-02, -3.1737e-02,\n",
      "         4.0418e-02, -1.9635e-02,  1.5352e-02,  1.3582e-01,  7.3412e-02,\n",
      "        -3.9804e-01,  1.0786e-01, -4.1575e-02,  4.1133e-02,  2.8202e-01,\n",
      "        -1.0221e-02,  4.0303e-02,  1.5112e-02,  2.3905e-01,  3.4231e-02,\n",
      "         3.0276e-03, -1.1887e-01,  2.7624e-01, -2.3594e-02,  7.4831e-03,\n",
      "        -5.2664e-05,  8.7785e-02, -3.1065e-01, -6.3949e+00, -1.1753e-01,\n",
      "         8.9575e-02, -5.1689e-02, -6.3833e-03,  4.1457e-02,  2.0768e-02,\n",
      "        -8.6080e-02,  2.4768e-02,  7.1886e-03,  2.7720e-02, -2.0868e-01,\n",
      "        -3.0131e-01, -9.6434e-02, -5.1740e-02,  5.6395e-02, -5.8891e-02,\n",
      "        -1.2151e-01, -7.5581e-02, -1.1867e-01, -1.6069e-02,  2.5488e-01,\n",
      "         1.5255e-01,  2.4698e-01, -6.3964e-03, -5.8133e-02,  1.2341e-01,\n",
      "        -2.6330e-01,  2.5516e-02, -3.5178e-02, -2.2890e-02,  1.0771e-01,\n",
      "         1.8501e-02, -7.1061e-02, -9.8473e-02,  1.6186e-01,  2.8563e-02,\n",
      "         1.6956e-02,  8.3897e-02, -4.9195e-02, -7.3826e-03,  3.6757e-01,\n",
      "         1.1895e-01,  4.5679e-02,  6.4562e-02, -6.6259e-01, -4.1774e-02,\n",
      "        -3.1691e-03,  3.3952e-02, -1.6494e-02,  9.7343e-02, -1.9789e-02,\n",
      "        -3.7514e-01,  9.8490e-02, -1.0209e-02,  6.3634e-02,  1.5980e-02,\n",
      "        -7.0773e-02,  1.3583e-01, -5.5422e-02, -7.0889e-02,  2.7789e-01,\n",
      "        -2.5170e-02, -2.4611e-02, -2.6224e-03,  1.3204e-01, -7.3427e-02,\n",
      "         9.1178e-03,  9.5661e-03,  3.1418e-02,  9.1940e-02,  1.2282e-02,\n",
      "         1.3611e-02, -3.7636e-01, -1.3032e-02, -2.6195e-02, -2.9706e-02,\n",
      "         1.9886e-02,  1.4154e-01,  4.0824e-02, -4.1875e-02,  1.1753e-01,\n",
      "         2.0157e-02,  7.1114e-02,  3.1028e-02, -1.1749e-01,  1.0721e-01,\n",
      "         9.7428e-02, -2.6909e-02,  4.8492e-02, -7.0157e-02, -1.3937e-01,\n",
      "        -1.1992e-01,  1.0982e-01,  3.1423e-02, -1.2474e-01,  1.9070e-01,\n",
      "         1.4994e-02,  3.5320e-01, -4.8319e-02, -5.2421e-02,  1.0609e-02,\n",
      "        -2.2504e-02,  4.4416e-02,  3.7229e-02,  1.6691e-01, -3.6423e-01,\n",
      "         1.0220e-01,  5.7988e-02,  1.4290e-02, -6.2541e-03, -3.7062e-02,\n",
      "         2.4746e-02, -4.5159e-03,  8.3932e-02,  8.2037e-03,  1.6159e-01,\n",
      "        -8.8906e-02,  6.8317e-02, -9.1005e-02, -6.0667e-02,  3.3309e-03,\n",
      "        -2.9820e-02, -1.6690e-02,  2.0314e-01, -6.9913e-02, -9.8421e-03,\n",
      "         8.3902e-02, -3.2085e-02,  4.3511e-02, -6.7732e-03, -3.3847e-02,\n",
      "        -4.1723e-01, -6.8805e-02, -5.5763e-02, -2.6070e-01,  6.6649e-02,\n",
      "        -3.7704e-02,  3.1528e-02,  6.6136e-02,  1.0235e-01,  3.2210e-02,\n",
      "        -1.7885e-02, -1.4049e-02,  4.1689e-02, -2.9458e-02, -2.8573e-01,\n",
      "        -6.1202e-02, -1.8036e-02,  5.1365e-03, -4.8980e-02, -1.3509e-01,\n",
      "        -3.6649e-02,  1.7785e+01, -1.0623e-02, -2.8398e-02, -9.3434e-02,\n",
      "        -2.4803e-02, -6.1269e-02,  9.2440e-03,  8.0764e-02, -2.9279e-01,\n",
      "         3.1731e-02,  2.1005e-03,  6.5169e-03, -2.6161e-01, -1.0054e-01,\n",
      "        -6.6918e-02,  1.8201e-01,  9.0479e-02, -7.6951e-02,  1.4720e-01,\n",
      "         5.7690e-02,  5.3797e-02,  9.0995e-02, -8.1658e-02, -1.1206e-01,\n",
      "        -6.3816e-03,  8.0413e-02, -7.0411e-02], device='cuda:0') tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#print(len(X))\n",
    "#print(X.shape)  # Should print: torch.Size([length_of_sequence])\n",
    "#print(type(X))#X_Stack = torch.stack(X)\n",
    "#print(X_Stack.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=32\n",
    ")\n",
    "print(X[0],y[0])\n",
    "#print(X_train.shape,X_test.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0476e700-6699-4284-b7f3-2a3d2aa949f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test,y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01f5cd32-3543-48dc-83ab-d3ae276296d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Input to hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model2 = SimpleNN(input_size=768, hidden_size=128, output_size=3)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c63f9d96-6da0-4e71-a263-58753ffdd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 268.48it/s, loss=0.734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training loss: 0.9336, Training accuracy: 0.5608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 435.51it/s, loss=0.803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8362, Test accuracy: 0.6299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 251.57it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Training loss: 0.7432, Training accuracy: 0.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 372.58it/s, loss=0.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7761, Test accuracy: 0.6798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 284.96it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Training loss: 0.6548, Training accuracy: 0.7266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 461.75it/s, loss=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7434, Test accuracy: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 293.65it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Training loss: 0.6026, Training accuracy: 0.7535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 458.22it/s, loss=0.845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7668, Test accuracy: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 279.84it/s, loss=0.936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Training loss: 0.5588, Training accuracy: 0.7758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 452.83it/s, loss=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7724, Test accuracy: 0.6719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 297.13it/s, loss=0.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Training loss: 0.5137, Training accuracy: 0.7916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 496.41it/s, loss=0.788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7745, Test accuracy: 0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 300.70it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Training loss: 0.4992, Training accuracy: 0.7823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 422.99it/s, loss=0.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7911, Test accuracy: 0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 288.48it/s, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Training loss: 0.4723, Training accuracy: 0.8055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 534.44it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8052, Test accuracy: 0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 352.97it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Training loss: 0.4201, Training accuracy: 0.8268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 678.96it/s, loss=0.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8416, Test accuracy: 0.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 350.29it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Training loss: 0.4044, Training accuracy: 0.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 658.79it/s, loss=0.866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8364, Test accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model2.parameters(), lr=2e-3)\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Initialize tqdm progress bar for training\n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data and labels to device (CPU or GPU)\n",
    "        data, labels = batch\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model2(data)  # Shape will be (batch_size, 3)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Correct way to get predicted class\n",
    "        #print(preds)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Update tqdm description with current loss\n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Test phase\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    correct_test_predictions = 0\n",
    "    total_test_predictions = 0\n",
    "    \n",
    "    test_progress_bar = tqdm(test_dataloader, desc=f\"Test Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress_bar:\n",
    "            data, labels = batch\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model2(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate test accuracy\n",
    "            preds = torch.argmax(outputs, dim=-1)  # Correct way to get predicted class\n",
    "            #print(preds)\n",
    "            correct_test_predictions += (preds == labels).sum().item()\n",
    "            total_test_predictions += labels.size(0)\n",
    "            \n",
    "            test_progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    test_accuracy = correct_test_predictions / total_test_predictions\n",
    "    \n",
    "    print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
