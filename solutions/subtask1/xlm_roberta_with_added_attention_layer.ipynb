{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfQVFtXBiMWy"
   },
   "source": [
    "### This file contains the same code as xlm-roberta-span-token.ipynb, but with a small addition in the last code part. After applying XLM-RoBERTa, attention layer is added on top of the last hidden layer, in order to enhance NEs' local context semantic representation.\n",
    "#### Embedded tokens representing span-text are Queries, and the embedding tokens from the rest of the input text are Keys and Values for attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYvFwavZiMW0",
    "outputId": "aa93dcbb-1aa7-4de6-de70-d918ebb5ddfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "drive/My Drive/Colab Notebooks/semeval_data/subtask1.parquet\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# sub1 = 'drive/My Drive/Colab Notebooks/semeval_data/subtask1.parquet'\n",
    "# print(sub1)\n",
    "\n",
    "from pathlib import Path\n",
    "wd = Path.cwd()\n",
    "wd = wd.parent.parent\n",
    "wd = wd / 'merged_data' \n",
    "sub1 = str(wd) + '/subtask1.parquet'\n",
    "print(sub1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BMEITI9CiMW2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(sub1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ozEIv93FiMW3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def labelNum(row):\n",
    "    if row['class1'] == 'Antagonist':\n",
    "        return int(0)\n",
    "    if row['class1'] == 'Innocent':\n",
    "        return int(1)\n",
    "    if row['class1'] == 'Protagonist':\n",
    "        return int(2)\n",
    "def cleanText(row):\n",
    "    text = str(row['text'])\n",
    "    #text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('\\n',' ').replace('  ', ' ')\n",
    "    return text\n",
    "df['label'] = df.apply(labelNum,axis=1)\n",
    "df['input'] = df.apply(cleanText,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnshZfqsiMW4",
    "outputId": "b296ae43-093a-43cd-edc9-a2e33359c93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang                                                            BG\n",
      "art_name                                                BG_670.txt\n",
      "entity                                                       Запад\n",
      "start                                                          152\n",
      "end                                                            156\n",
      "class1                                                  Antagonist\n",
      "classes2              [Conspirator, Instigator, Foreign Adversary]\n",
      "text             Опитът на колективния Запад да „обезкърви Руси...\n",
      "label                                                            0\n",
      "input            Опитът на колективния Запад да „обезкърви Руси...\n",
      "new_start_end                                           (151, 156)\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def find_all_substring_start_end(text, substring):\n",
    "    # Use re.finditer to find all occurrences of the substring in the text\n",
    "    matches = re.finditer(re.escape(substring), text)\n",
    "\n",
    "    # Collect the start and end indices of all matches\n",
    "    positions = [(match.start(), match.end()) for match in matches]\n",
    "\n",
    "    return positions\n",
    "def adjust_start_end(row):\n",
    "    org_text,cl_text,start,end,entity = str(row['text']),str(row['input']),int(row['start']),int(row['end']),str(row['entity'])\n",
    "    ss1 = find_all_substring_start_end(org_text,entity)\n",
    "    ss2 = find_all_substring_start_end(cl_text,entity)\n",
    "    #print(ss1,ss2)\n",
    "    #print(row['text'][start:end])\n",
    "    a = 0\n",
    "    for i in range(len(ss1)):\n",
    "        if abs((ss1[i][0] - start) + (ss1[i][1] - end) ) <= 2:\n",
    "            a = i\n",
    "            break\n",
    "    if org_text[ss1[a][0]:ss1[a][1]] != cl_text[ss2[a][0]:ss2[a][1]]:\n",
    "        print(\"ERROR!\")\n",
    "    return ss2[a][0],ss2[a][1]\n",
    "df['new_start_end'] = df.apply(adjust_start_end,axis=1)\n",
    "print(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s-PdTAcViMW5"
   },
   "outputs": [],
   "source": [
    "def addTokensToInput(row):\n",
    "    inp = row['input']\n",
    "    start,end = row['new_start_end']\n",
    "    #print(start,end)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    token_input = inp[:start] + \"[SPAN_START] \" + inp[start:end] + \" [SPAN_END]\" + inp[end:]\n",
    "    return token_input\n",
    "\n",
    "df['span_input'] = df.apply(addTokensToInput,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yobaQPRoiMW5"
   },
   "outputs": [],
   "source": [
    "def upStartEnd(row):\n",
    "    start,end = row['new_start_end']\n",
    "    start += len(\"[SPAN_START] \")\n",
    "    end += len(\"[SPAN_START] \")\n",
    "    return start,end\n",
    "\n",
    "df['new_start_end'] = df.apply(upStartEnd,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PwT_XHKmiMW5",
    "outputId": "4e1c0bbd-dcd6-4b9a-d3d7-50c37bb44d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3).to(device)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['span_input'], padding=True, truncation=True,max_length=8192,return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZDwWW9hiMW6",
    "outputId": "7dfc6354-e6e2-48f8-f77a-e37ee637f96c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(250004, 768, padding_idx=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTokens = {\n",
    "    \"additional_special_tokens\": [\"[SPAN_START]\", \"[SPAN_END]\"]\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(extraTokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gMWI8j1ZiMW6"
   },
   "outputs": [],
   "source": [
    "data = df.loc[ : , ['span_input','label','new_start_end','entity']]\n",
    "data['tokenized']=data.apply(preprocess_function,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cBcBQZegiMW7"
   },
   "outputs": [],
   "source": [
    "def indexes(row):\n",
    "    off_mask = row['tokenized']['offset_mapping']\n",
    "    start,end = row['new_start_end'][0],row['new_start_end'][1]\n",
    "    inds = list()\n",
    "    for p in range(len(off_mask)):\n",
    "        if off_mask[p][0] >= start and off_mask[p][1] <= end:\n",
    "            if p != len(off_mask)-1:\n",
    "                inds.append(p)\n",
    "    #if len(inds) > 1:\n",
    "        #print(\"GREATER THAN 1\")\n",
    "    if len(inds) == 0:\n",
    "        print(start,end)\n",
    "    return inds\n",
    "data['indexes'] = data.apply(indexes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwd1VtkRiMW8",
    "outputId": "d001d787-6d47-49ab-b0f8-27804a51304b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2535 2535 2535\n"
     ]
    }
   ],
   "source": [
    "data['list'] = data['tokenized'].apply(lambda x: x['input_ids'])\n",
    "data['attention'] = data['tokenized'].apply(lambda x: x['attention_mask'])\n",
    "ids = data['list']\n",
    "att = data['attention']\n",
    "indexes = data['indexes']\n",
    "tids = list()\n",
    "tatt = list()\n",
    "print(len(ids),len(att),len(indexes))\n",
    "for i in range(len(ids)):\n",
    "    tids.append(torch.tensor(ids[i]))\n",
    "    tatt.append(torch.tensor(att[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Hawj2WEWiMW8"
   },
   "outputs": [],
   "source": [
    "sliced_ids = list()\n",
    "sliced_ntids = list()\n",
    "sliced_att = list()\n",
    "key_inds = list()\n",
    "key_ids = list()\n",
    "\n",
    "def slices(index,size,context_size):\n",
    "    if (size<context_size):\n",
    "        return 0,size\n",
    "    lower_c = int(context_size/2-1)\n",
    "    upper_c = int(context_size/2)\n",
    "    #print(lower_c,upper_c)\n",
    "    if index < lower_c:\n",
    "        return 0,context_size\n",
    "    elif index >= lower_c:\n",
    "        if index + upper_c > size:\n",
    "            return index-(context_size-(size-index)), size\n",
    "        else:\n",
    "            return index-lower_c,index+upper_c+1\n",
    "\n",
    "\n",
    "for i in range(len(tids)):\n",
    "    slower,supper = slices(indexes[i][0],len(tids[i]),510)\n",
    "    #key_tid = tids[i][indexes[i][0]]\n",
    "    pid = ids[i][slower:supper]\n",
    "    key_inds.append([])\n",
    "    for j in indexes[i]:\n",
    "        key_id = ids[i][j]\n",
    "        if key_id not in pid:\n",
    "           print(len(ids[i]),key_id,slower,supper,indexes[i])\n",
    "        key_inds[i].append(pid.index(key_id))\n",
    "    apid = tids[i][slower:supper]\n",
    "    apatt = tatt[i][slower:supper]\n",
    "    if 0 not in pid:\n",
    "        apid = torch.cat((torch.tensor([0]),apid),dim=0)\n",
    "        apatt = torch.cat((torch.tensor([1]),apatt),dim=0)\n",
    "    if 2 not in pid:\n",
    "        apid = torch.cat((apid,torch.tensor([2])),dim=0)\n",
    "        apatt = torch.cat((apatt,torch.tensor([1])),dim=0)\n",
    "    sliced_ids.append(apid)\n",
    "    sliced_att.append(apatt)\n",
    "\n",
    "Min = 10000\n",
    "Max = 0\n",
    "ind2 = 0\n",
    "for i in range(len(indexes)):\n",
    "    if len(sliced_ids[i]) < Min:\n",
    "        Min = len(sliced_ids[i])\n",
    "        ind2 = i\n",
    "\n",
    "    if len(sliced_ids[i]) > Max:\n",
    "        Max = len(sliced_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xFCUO_GXiMW8"
   },
   "outputs": [],
   "source": [
    "input_ids = list()\n",
    "att_mask = list()\n",
    "for ten,att in zip(sliced_ids,sliced_att):\n",
    "    if len(ten) < 512:\n",
    "        padding_length = 512 - len(ten)\n",
    "        padding_tensor = torch.full((padding_length,), tokenizer.pad_token_id, dtype=ten.dtype)\n",
    "        padding_tensor2 = torch.full((padding_length,), 0, dtype=att.dtype)\n",
    "        ten = torch.cat((ten,padding_tensor),dim=0)\n",
    "        att = torch.cat((att,padding_tensor2),dim=0)\n",
    "    input_ids.append(ten)\n",
    "    att_mask.append(att)\n",
    "inputIds = torch.stack(input_ids)\n",
    "attMask = torch.stack(att_mask)\n",
    "\n",
    "inputIds_np = inputIds.numpy()\n",
    "attMask_np = attMask.numpy()\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sZvBhiA-iMW8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "    inputIds_np, attMask_np, y, test_size=0.2, random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2Nos0gEBiMW8"
   },
   "outputs": [],
   "source": [
    "X_train_ids = torch.tensor(X_train_ids, dtype=torch.long).to(device)\n",
    "X_test_ids = torch.tensor(X_test_ids, dtype=torch.long).to(device)\n",
    "X_train_mask = torch.tensor(X_train_mask, dtype=torch.long).to(device)\n",
    "X_test_mask = torch.tensor(X_test_mask, dtype=torch.long).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GTxQJLCliMW9"
   },
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
    "test_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YES435MWiMW9"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=8e-6)\n",
    "classifier = nn.Linear(model.config.hidden_size * 2, 3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ms3jToMlm-4I",
    "outputId": "4bca71a9-d34a-49bc-feb7-1ae397102f4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 127/127 [03:11<00:00,  1.50s/it, loss=0.861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training loss: 1.0080, Training accuracy: 0.4941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/10: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.9233, Test accuracy: 0.5266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Training loss: 0.8156, Training accuracy: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/10: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, loss=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7778, Test accuracy: 0.6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Training loss: 0.7103, Training accuracy: 0.7130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/10: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7110, Test accuracy: 0.7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Training loss: 0.6360, Training accuracy: 0.7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/10: 100%|██████████| 32/32 [00:13<00:00,  2.33it/s, loss=0.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6381, Test accuracy: 0.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Training loss: 0.5555, Training accuracy: 0.8097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/10: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6546, Test accuracy: 0.7732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Training loss: 0.4884, Training accuracy: 0.8481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 6/10: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, loss=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6044, Test accuracy: 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Training loss: 0.4362, Training accuracy: 0.8679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 7/10: 100%|██████████| 32/32 [00:13<00:00,  2.33it/s, loss=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5671, Test accuracy: 0.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Training loss: 0.3852, Training accuracy: 0.8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 8/10: 100%|██████████| 32/32 [00:13<00:00,  2.33it/s, loss=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5717, Test accuracy: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Training loss: 0.3510, Training accuracy: 0.9093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 9/10: 100%|██████████| 32/32 [00:13<00:00,  2.33it/s, loss=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6121, Test accuracy: 0.8107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 127/127 [03:10<00:00,  1.50s/it, loss=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Training loss: 0.3057, Training accuracy: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 10/10: 100%|██████████| 32/32 [00:13<00:00,  2.33it/s, loss=0.426]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5690, Test accuracy: 0.8383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        span_start_token_id = tokenizer.convert_tokens_to_ids('[SPAN_START]')\n",
    "        span_end_token_id = tokenizer.convert_tokens_to_ids('[SPAN_END]')\n",
    "\n",
    "        start_mask = (input_ids == span_start_token_id)\n",
    "        end_mask = (input_ids == span_end_token_id)\n",
    "\n",
    "        entity_representations = []\n",
    "\n",
    "        start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
    "        end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # check that span is valid and has non-zero length\n",
    "        valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
    "\n",
    "        valid_start_indices = start_indices[valid_spans]\n",
    "        valid_end_indices = end_indices[valid_spans]\n",
    "\n",
    "        # to handle spans with different lengths by padding\n",
    "        max_span_length = (valid_end_indices - valid_start_indices).max().item() + 1\n",
    "\n",
    "        # extract entity tokens for every sample in batch\n",
    "        for i in range(batch_size):\n",
    "            if valid_spans[i]:\n",
    "                entity_tokens = hidden_states[i, valid_start_indices[i]:valid_end_indices[i]]\n",
    "\n",
    "                # padding - to have consistent dimensions for tensors\n",
    "                if entity_tokens.size(0) < max_span_length:\n",
    "                    padding_length = max_span_length - entity_tokens.size(0)\n",
    "                    padding = torch.zeros(padding_length, entity_tokens.size(1), device=device)\n",
    "                    entity_tokens = torch.cat([entity_tokens, padding], dim=0)\n",
    "\n",
    "                entity_representations.append(entity_tokens)\n",
    "\n",
    "\n",
    "        #[batch_size, max_span_length, hidden_size]\n",
    "        entity_representations = torch.stack(entity_representations, dim=0) if entity_representations else \\\n",
    "            torch.empty((batch_size, max_span_length, hidden_states.size(-1)), device=device)\n",
    "\n",
    "        # Calculate NE-specific context with attention\n",
    "        # Entity tokens are Q, and hidden_states are K and V\n",
    "        attention_scores = torch.softmax(torch.matmul(entity_representations, hidden_states[0].T), dim=-1)\n",
    "        context_vector = torch.matmul(attention_scores, hidden_states[0])\n",
    "\n",
    "        # Pooling over the span length (mean pooling)\n",
    "        context_vector_pooled = context_vector.mean(dim=1)\n",
    "\n",
    "        representation = torch.cat([entity_representations.mean(dim=1), context_vector_pooled], dim=-1)\n",
    "\n",
    "        logits = classifier(representation) \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test_predictions = 0\n",
    "    total_test_predictions = 0\n",
    "\n",
    "    test_progress_bar = tqdm(test_dataloader, desc=f\"Test Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress_bar:\n",
    "\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "            span_start_token_id = tokenizer.convert_tokens_to_ids('[SPAN_START]')\n",
    "            span_end_token_id = tokenizer.convert_tokens_to_ids('[SPAN_END]')\n",
    "\n",
    "            start_mask = (input_ids == span_start_token_id)\n",
    "            end_mask = (input_ids == span_end_token_id)\n",
    "\n",
    "            entity_representations = []\n",
    "\n",
    "            start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
    "            end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
    "\n",
    "            valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
    "\n",
    "            valid_start_indices = start_indices[valid_spans]\n",
    "            valid_end_indices = end_indices[valid_spans]\n",
    "\n",
    "            max_span_length = (valid_end_indices - valid_start_indices).max().item() + 1 \n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if valid_spans[i]:\n",
    "                    entity_tokens = hidden_states[i, valid_start_indices[i]:valid_end_indices[i]]\n",
    "\n",
    "                    if entity_tokens.size(0) < max_span_length:\n",
    "                        padding_length = max_span_length - entity_tokens.size(0)\n",
    "                        padding = torch.zeros(padding_length, entity_tokens.size(1), device=device)\n",
    "                        entity_tokens = torch.cat([entity_tokens, padding], dim=0)\n",
    "\n",
    "                    entity_representations.append(entity_tokens)\n",
    "\n",
    "            entity_representations = torch.stack(entity_representations, dim=0) if entity_representations else \\\n",
    "                torch.empty((batch_size, max_span_length, hidden_states.size(-1)), device=device)\n",
    "\n",
    "            attention_scores = torch.softmax(torch.matmul(entity_representations, hidden_states[0].T), dim=-1)\n",
    "            context_vector = torch.matmul(attention_scores, hidden_states[0])\n",
    "\n",
    "            context_vector_pooled = context_vector.mean(dim=1)\n",
    "\n",
    "            representation = torch.cat([entity_representations.mean(dim=1), context_vector_pooled], dim=-1)\n",
    "            logits = classifier(representation)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct_test_predictions += (preds == labels).sum().item()\n",
    "            total_test_predictions += labels.size(0)\n",
    "\n",
    "            test_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    test_accuracy = correct_test_predictions / total_test_predictions\n",
    "\n",
    "    print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
