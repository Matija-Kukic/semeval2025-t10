{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfQVFtXBiMWy"
      },
      "source": [
        "### This file contains the same code as xlm-roberta-span-token.ipynb, but does classification on [SPAN_START] token instead of default [CLS] \n",
        "#### This approach showed the best results so far in the test accuracy and faster convergence of the test-loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvFwavZiMW0",
        "outputId": "b310a23d-9fb7-4440-c6f9-3c1135767494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive/My Drive/Colab Notebooks/semeval_data/subtask1.parquet\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# sub1 = 'drive/My Drive/Colab Notebooks/semeval_data/subtask1.parquet'\n",
        "# print(sub1)\n",
        "\n",
        "from pathlib import Path\n",
        "wd = Path.cwd()\n",
        "wd = wd.parent.parent\n",
        "wd = wd / 'merged_data'\n",
        "sub1 = str(wd) + '/subtask1.parquet'\n",
        "print(sub1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BMEITI9CiMW2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(sub1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ozEIv93FiMW3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def labelNum(row):\n",
        "    if row['class1'] == 'Antagonist':\n",
        "        return int(0)\n",
        "    if row['class1'] == 'Innocent':\n",
        "        return int(1)\n",
        "    if row['class1'] == 'Protagonist':\n",
        "        return int(2)\n",
        "def cleanText(row):\n",
        "    text = str(row['text'])\n",
        "    #text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = text.replace('\\n',' ').replace('  ', ' ')\n",
        "    return text\n",
        "df['label'] = df.apply(labelNum,axis=1)\n",
        "df['input'] = df.apply(cleanText,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnshZfqsiMW4",
        "outputId": "b93e557b-2e21-4f24-c552-4980a25ea130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lang                                                            BG\n",
            "art_name                                                BG_670.txt\n",
            "entity                                                       Запад\n",
            "start                                                          152\n",
            "end                                                            156\n",
            "class1                                                  Antagonist\n",
            "classes2              [Conspirator, Instigator, Foreign Adversary]\n",
            "text             Опитът на колективния Запад да „обезкърви Руси...\n",
            "label                                                            0\n",
            "input            Опитът на колективния Запад да „обезкърви Руси...\n",
            "new_start_end                                           (151, 156)\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def find_all_substring_start_end(text, substring):\n",
        "    # Use re.finditer to find all occurrences of the substring in the text\n",
        "    matches = re.finditer(re.escape(substring), text)\n",
        "\n",
        "    # Collect the start and end indices of all matches\n",
        "    positions = [(match.start(), match.end()) for match in matches]\n",
        "\n",
        "    return positions\n",
        "def adjust_start_end(row):\n",
        "    org_text,cl_text,start,end,entity = str(row['text']),str(row['input']),int(row['start']),int(row['end']),str(row['entity'])\n",
        "    ss1 = find_all_substring_start_end(org_text,entity)\n",
        "    ss2 = find_all_substring_start_end(cl_text,entity)\n",
        "    #print(ss1,ss2)\n",
        "    #print(row['text'][start:end])\n",
        "    a = 0\n",
        "    for i in range(len(ss1)):\n",
        "        if abs((ss1[i][0] - start) + (ss1[i][1] - end) ) <= 2:\n",
        "            a = i\n",
        "            break\n",
        "    if org_text[ss1[a][0]:ss1[a][1]] != cl_text[ss2[a][0]:ss2[a][1]]:\n",
        "        print(\"ERROR!\")\n",
        "    return ss2[a][0],ss2[a][1]\n",
        "df['new_start_end'] = df.apply(adjust_start_end,axis=1)\n",
        "print(df.loc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s-PdTAcViMW5"
      },
      "outputs": [],
      "source": [
        "def addTokensToInput(row):\n",
        "    inp = row['input']\n",
        "    start,end = row['new_start_end']\n",
        "    #print(start,end)\n",
        "    start = int(start)\n",
        "    end = int(end)\n",
        "    token_input = inp[:start] + \"[SPAN_START] \" + inp[start:end] + \" [SPAN_END]\" + inp[end:]\n",
        "    return token_input\n",
        "\n",
        "df['span_input'] = df.apply(addTokensToInput,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yobaQPRoiMW5"
      },
      "outputs": [],
      "source": [
        "def upStartEnd(row):\n",
        "    start,end = row['new_start_end']\n",
        "    start += len(\"[SPAN_START] \")\n",
        "    end += len(\"[SPAN_START] \")\n",
        "    return start,end\n",
        "\n",
        "df['new_start_end'] = df.apply(upStartEnd,axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwT_XHKmiMW5",
        "outputId": "626478ce-d9cb-400b-89f4-db63b58ce451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3).to(device)\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['span_input'], padding=True, truncation=True,max_length=8192,return_offsets_mapping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZDwWW9hiMW6",
        "outputId": "0a20c263-2028-4ab4-e2e4-e7f86107ff45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Embedding(250004, 768, padding_idx=1)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extraTokens = {\n",
        "    \"additional_special_tokens\": [\"[SPAN_START]\", \"[SPAN_END]\"]\n",
        "}\n",
        "num_added_toks = tokenizer.add_special_tokens(extraTokens)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gMWI8j1ZiMW6"
      },
      "outputs": [],
      "source": [
        "data = df.loc[ : , ['span_input','label','new_start_end','entity']]\n",
        "data['tokenized']=data.apply(preprocess_function,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cBcBQZegiMW7"
      },
      "outputs": [],
      "source": [
        "def indexes(row):\n",
        "    off_mask = row['tokenized']['offset_mapping']\n",
        "    start,end = row['new_start_end'][0],row['new_start_end'][1]\n",
        "    inds = list()\n",
        "    for p in range(len(off_mask)):\n",
        "        if off_mask[p][0] >= start and off_mask[p][1] <= end:\n",
        "            if p != len(off_mask)-1:\n",
        "                inds.append(p)\n",
        "    #if len(inds) > 1:\n",
        "        #print(\"GREATER THAN 1\")\n",
        "    if len(inds) == 0:\n",
        "        print(start,end)\n",
        "    return inds\n",
        "data['indexes'] = data.apply(indexes,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwd1VtkRiMW8",
        "outputId": "f11114eb-166f-47c9-b6b5-453975abe01e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2902 2902 2902\n"
          ]
        }
      ],
      "source": [
        "data['list'] = data['tokenized'].apply(lambda x: x['input_ids'])\n",
        "data['attention'] = data['tokenized'].apply(lambda x: x['attention_mask'])\n",
        "ids = data['list']\n",
        "att = data['attention']\n",
        "indexes = data['indexes']\n",
        "tids = list()\n",
        "tatt = list()\n",
        "print(len(ids),len(att),len(indexes))\n",
        "for i in range(len(ids)):\n",
        "    tids.append(torch.tensor(ids[i]))\n",
        "    tatt.append(torch.tensor(att[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Hawj2WEWiMW8"
      },
      "outputs": [],
      "source": [
        "sliced_ids = list()\n",
        "sliced_ntids = list()\n",
        "sliced_att = list()\n",
        "key_inds = list()\n",
        "key_ids = list()\n",
        "\n",
        "def slices(index,size,context_size):\n",
        "    if (size<context_size):\n",
        "        return 0,size\n",
        "    lower_c = int(context_size/2-1)\n",
        "    upper_c = int(context_size/2)\n",
        "    #print(lower_c,upper_c)\n",
        "    if index < lower_c:\n",
        "        return 0,context_size\n",
        "    elif index >= lower_c:\n",
        "        if index + upper_c > size:\n",
        "            return index-(context_size-(size-index)), size\n",
        "        else:\n",
        "            return index-lower_c,index+upper_c+1\n",
        "\n",
        "\n",
        "for i in range(len(tids)):\n",
        "    slower,supper = slices(indexes[i][0],len(tids[i]),510)\n",
        "    #key_tid = tids[i][indexes[i][0]]\n",
        "    pid = ids[i][slower:supper]\n",
        "    key_inds.append([])\n",
        "    for j in indexes[i]:\n",
        "        key_id = ids[i][j]\n",
        "        if key_id not in pid:\n",
        "           print(len(ids[i]),key_id,slower,supper,indexes[i])\n",
        "        key_inds[i].append(pid.index(key_id))\n",
        "    apid = tids[i][slower:supper]\n",
        "    apatt = tatt[i][slower:supper]\n",
        "    if 0 not in pid:\n",
        "        apid = torch.cat((torch.tensor([0]),apid),dim=0)\n",
        "        apatt = torch.cat((torch.tensor([1]),apatt),dim=0)\n",
        "    if 2 not in pid:\n",
        "        apid = torch.cat((apid,torch.tensor([2])),dim=0)\n",
        "        apatt = torch.cat((apatt,torch.tensor([1])),dim=0)\n",
        "    sliced_ids.append(apid)\n",
        "    sliced_att.append(apatt)\n",
        "\n",
        "Min = 10000\n",
        "Max = 0\n",
        "ind2 = 0\n",
        "for i in range(len(indexes)):\n",
        "    if len(sliced_ids[i]) < Min:\n",
        "        Min = len(sliced_ids[i])\n",
        "        ind2 = i\n",
        "\n",
        "    if len(sliced_ids[i]) > Max:\n",
        "        Max = len(sliced_ids[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xFCUO_GXiMW8"
      },
      "outputs": [],
      "source": [
        "input_ids = list()\n",
        "att_mask = list()\n",
        "for ten,att in zip(sliced_ids,sliced_att):\n",
        "    if len(ten) < 512:\n",
        "        padding_length = 512 - len(ten)\n",
        "        padding_tensor = torch.full((padding_length,), tokenizer.pad_token_id, dtype=ten.dtype)\n",
        "        padding_tensor2 = torch.full((padding_length,), 0, dtype=att.dtype)\n",
        "        ten = torch.cat((ten,padding_tensor),dim=0)\n",
        "        att = torch.cat((att,padding_tensor2),dim=0)\n",
        "    input_ids.append(ten)\n",
        "    att_mask.append(att)\n",
        "inputIds = torch.stack(input_ids)\n",
        "attMask = torch.stack(att_mask)\n",
        "\n",
        "inputIds_np = inputIds.numpy()\n",
        "attMask_np = attMask.numpy()\n",
        "y = data['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sZvBhiA-iMW8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
        "    inputIds_np, attMask_np, y, test_size=0.2, random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2Nos0gEBiMW8"
      },
      "outputs": [],
      "source": [
        "X_train_ids = torch.tensor(X_train_ids, dtype=torch.long).to(device)\n",
        "X_test_ids = torch.tensor(X_test_ids, dtype=torch.long).to(device)\n",
        "X_train_mask = torch.tensor(X_train_mask, dtype=torch.long).to(device)\n",
        "X_test_mask = torch.tensor(X_test_mask, dtype=torch.long).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GTxQJLCliMW9"
      },
      "outputs": [],
      "source": [
        "# Create TensorDatasets\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
        "test_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YES435MWiMW9"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=8e-6)\n",
        "classifier = nn.Linear(model.config.hidden_size, 3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms3jToMlm-4I",
        "outputId": "6285b92d-a49d-49dc-8ce8-69433eff2f4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/6: 100%|██████████| 146/146 [03:47<00:00,  1.56s/it, loss=0.542]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "Training loss: 0.8517, Training accuracy: 0.6105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test Epoch 1/6: 100%|██████████| 37/37 [00:16<00:00,  2.25it/s, loss=0.584]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.6101, Test accuracy: 0.7745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2/6: 100%|██████████| 146/146 [03:45<00:00,  1.54s/it, loss=0.31]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/6\n",
            "Training loss: 0.5346, Training accuracy: 0.7932\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test Epoch 2/6: 100%|██████████| 37/37 [00:16<00:00,  2.27it/s, loss=0.586]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.4836, Test accuracy: 0.8365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3/6: 100%|██████████| 146/146 [03:45<00:00,  1.55s/it, loss=0.982]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/6\n",
            "Training loss: 0.3646, Training accuracy: 0.8703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test Epoch 3/6: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5190, Test accuracy: 0.8158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4/6: 100%|██████████| 146/146 [03:45<00:00,  1.55s/it, loss=0.00909]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/6\n",
            "Training loss: 0.2549, Training accuracy: 0.9078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test Epoch 4/6: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.568]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.4894, Test accuracy: 0.8434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5/6: 100%|██████████| 146/146 [03:45<00:00,  1.55s/it, loss=0.0482]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/6\n",
            "Training loss: 0.1652, Training accuracy: 0.9500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test Epoch 5/6: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.555]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5174, Test accuracy: 0.8313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6/6: 100%|██████████| 146/146 [03:45<00:00,  1.55s/it, loss=0.0689]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/6\n",
            "Training loss: 0.1148, Training accuracy: 0.9660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test Epoch 6/6: 100%|██████████| 37/37 [00:16<00:00,  2.26it/s, loss=0.253]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5184, Test accuracy: 0.8606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 6\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for batch in train_progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
        "\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "        span_start_token_id = tokenizer.convert_tokens_to_ids('[SPAN_START]')\n",
        "        span_end_token_id = tokenizer.convert_tokens_to_ids('[SPAN_END]')\n",
        "\n",
        "        start_mask = (input_ids == span_start_token_id)\n",
        "        end_mask = (input_ids == span_end_token_id)\n",
        "\n",
        "        entity_representations = []\n",
        "\n",
        "        start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
        "        end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # check that span is valid and has non-zero length\n",
        "        valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
        "\n",
        "        valid_start_indices = start_indices[valid_spans]\n",
        "        valid_end_indices = end_indices[valid_spans]\n",
        "\n",
        "        # extract entity tokens for every sample in batch\n",
        "        for i in range(batch_size):\n",
        "            entity_tokens = hidden_states[i, valid_start_indices[i]]\n",
        "            entity_representations.append(entity_tokens)\n",
        "\n",
        "        entity_representations = torch.stack(entity_representations, dim=0)\n",
        "\n",
        "        logits = classifier(entity_representations)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        correct_predictions += (preds == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct_test_predictions = 0\n",
        "    total_test_predictions = 0\n",
        "\n",
        "    test_progress_bar = tqdm(test_dataloader, desc=f\"Test Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_progress_bar:\n",
        "\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
        "\n",
        "            hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "            span_start_token_id = tokenizer.convert_tokens_to_ids('[SPAN_START]')\n",
        "            span_end_token_id = tokenizer.convert_tokens_to_ids('[SPAN_END]')\n",
        "\n",
        "            start_mask = (input_ids == span_start_token_id)\n",
        "            end_mask = (input_ids == span_end_token_id)\n",
        "\n",
        "            entity_representations = []\n",
        "\n",
        "            start_indices = start_mask.nonzero(as_tuple=True)[1]\n",
        "            end_indices = end_mask.nonzero(as_tuple=True)[1]\n",
        "\n",
        "            valid_spans = (start_indices != -1) & (end_indices != -1) & (start_indices <= end_indices)\n",
        "\n",
        "            valid_start_indices = start_indices[valid_spans]\n",
        "            valid_end_indices = end_indices[valid_spans]\n",
        "\n",
        "            # extract entity tokens for every sample in batch\n",
        "            for i in range(batch_size):\n",
        "                entity_tokens = hidden_states[i, valid_start_indices[i]]\n",
        "                entity_representations.append(entity_tokens)\n",
        "\n",
        "            entity_representations = torch.stack(entity_representations, dim=0)\n",
        "\n",
        "            logits = classifier(entity_representations)\n",
        "            loss = criterion(logits, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            correct_test_predictions += (preds == labels).sum().item()\n",
        "            total_test_predictions += labels.size(0)\n",
        "\n",
        "            test_progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_dataloader)\n",
        "    test_accuracy = correct_test_predictions / total_test_predictions\n",
        "\n",
        "    print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
