{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77690264-7675-43bb-bcbf-dcadcd4deaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matijak/Documents/programiranje/projects/semeval/merged_data/subtask1.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "wd = Path.cwd()\n",
    "wd = wd.parent.parent\n",
    "wd = wd / 'merged_data' \n",
    "sub1 = str(wd) + '/subtask1.parquet'\n",
    "print(sub1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fc1c7e-731e-42dc-b4aa-03f098994bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>art_name</th>\n",
       "      <th>entity</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>class1</th>\n",
       "      <th>classes2</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>Запад</td>\n",
       "      <td>152</td>\n",
       "      <td>156</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Conspirator, Instigator, Foreign Adversary]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>САЩ</td>\n",
       "      <td>530</td>\n",
       "      <td>532</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>НАТО</td>\n",
       "      <td>535</td>\n",
       "      <td>538</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>Украйна</td>\n",
       "      <td>578</td>\n",
       "      <td>584</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Foreign Adversary]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG</td>\n",
       "      <td>BG_670.txt</td>\n",
       "      <td>украински войници</td>\n",
       "      <td>633</td>\n",
       "      <td>649</td>\n",
       "      <td>Innocent</td>\n",
       "      <td>[Victim]</td>\n",
       "      <td>Опитът на колективния Запад да „обезкърви Руси...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang    art_name             entity start  end      class1  \\\n",
       "0   BG  BG_670.txt              Запад   152  156  Antagonist   \n",
       "1   BG  BG_670.txt                САЩ   530  532  Antagonist   \n",
       "2   BG  BG_670.txt               НАТО   535  538  Antagonist   \n",
       "3   BG  BG_670.txt            Украйна   578  584  Antagonist   \n",
       "4   BG  BG_670.txt  украински войници   633  649    Innocent   \n",
       "\n",
       "                                       classes2  \\\n",
       "0  [Conspirator, Instigator, Foreign Adversary]   \n",
       "1                                  [Instigator]   \n",
       "2                                  [Instigator]   \n",
       "3                           [Foreign Adversary]   \n",
       "4                                      [Victim]   \n",
       "\n",
       "                                                text  \n",
       "0  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "1  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "2  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "3  Опитът на колективния Запад да „обезкърви Руси...  \n",
       "4  Опитът на колективния Запад да „обезкърви Руси...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(sub1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca443c1-8559-4eef-b75c-0ba0f9d48431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang                                                       EN\n",
       "art_name                                     EN_UA_103861.txt\n",
       "entity                                                Chinese\n",
       "start                                                     791\n",
       "end                                                       797\n",
       "class1                                             Antagonist\n",
       "classes2                                                [Spy]\n",
       "text        The World Needs Peacemaker Trump Again \\n\\n by...\n",
       "label                                                       0\n",
       "input       The World Needs Peacemaker Trump Again  by Jef...\n",
       "Name: 448, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def labelNum(row):\n",
    "    if row['class1'] == 'Antagonist':\n",
    "        return int(0)\n",
    "    if row['class1'] == 'Innocent':\n",
    "        return int(1)\n",
    "    if row['class1'] == 'Protagonist':\n",
    "        return int(2)\n",
    "def cleanText(row):\n",
    "    text = str(row['text'])\n",
    "    #text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('\\n',' ').replace('  ', ' ')\n",
    "    return text\n",
    "df['label'] = df.apply(labelNum,axis=1)\n",
    "df['input'] = df.apply(cleanText,axis=1)\n",
    "df.loc[448]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d957f8-b18f-4a85-8905-9ce1a7433c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang                                                            EN\n",
      "art_name                                          EN_UA_103861.txt\n",
      "entity                                                     Chinese\n",
      "start                                                          791\n",
      "end                                                            797\n",
      "class1                                                  Antagonist\n",
      "classes2                                                     [Spy]\n",
      "text             The World Needs Peacemaker Trump Again \\n\\n by...\n",
      "label                                                            0\n",
      "input            The World Needs Peacemaker Trump Again  by Jef...\n",
      "new_start_end                                           (785, 792)\n",
      "Name: 448, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def find_all_substring_start_end(text, substring):\n",
    "    # Use re.finditer to find all occurrences of the substring in the text\n",
    "    matches = re.finditer(re.escape(substring), text)\n",
    "    \n",
    "    # Collect the start and end indices of all matches\n",
    "    positions = [(match.start(), match.end()) for match in matches]\n",
    "    \n",
    "    return positions\n",
    "def adjust_start_end(row):\n",
    "    org_text,cl_text,start,end,entity = str(row['text']),str(row['input']),int(row['start']),int(row['end']),str(row['entity'])\n",
    "    ss1 = find_all_substring_start_end(org_text,entity)\n",
    "    ss2 = find_all_substring_start_end(cl_text,entity)\n",
    "    #print(ss1,ss2)\n",
    "    #print(row['text'][start:end])\n",
    "    a = 0\n",
    "    for i in range(len(ss1)):\n",
    "        if abs((ss1[i][0] - start) + (ss1[i][1] - end) ) <= 2:\n",
    "            a = i\n",
    "            break\n",
    "    if org_text[ss1[a][0]:ss1[a][1]] != cl_text[ss2[a][0]:ss2[a][1]]:\n",
    "        print(\"ERROR!\")\n",
    "    return ss2[a][0],ss2[a][1]\n",
    "df['new_start_end'] = df.apply(adjust_start_end,axis=1)\n",
    "print(df.loc[448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be496b1-8ddb-40fd-8fc8-85b5488b717f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7f1038ab7f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained models for each language\n",
    "nlp_en = spacy.load('en_core_web_sm')  # English model\n",
    "nlp_bg = spacy.load('xx_ent_wiki_sm')  # Bulgarian model\n",
    "nlp_hi = spacy.load('xx_ent_wiki_sm')  # Hindi model\n",
    "nlp_pt = spacy.load('xx_ent_wiki_sm')  # Portuguese model\n",
    "\n",
    "nlp_en.add_pipe('sentencizer')\n",
    "nlp_bg.add_pipe('sentencizer')\n",
    "nlp_hi.add_pipe('sentencizer')\n",
    "nlp_pt.add_pipe('sentencizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a0bfb4-7baf-4f7a-90dd-6acc6630cfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang                                                            BG\n",
       "art_name                                                BG_670.txt\n",
       "entity                                                       Запад\n",
       "start                                                          152\n",
       "end                                                            156\n",
       "class1                                                  Antagonist\n",
       "classes2              [Conspirator, Instigator, Foreign Adversary]\n",
       "text             Опитът на колективния Запад да „обезкърви Руси...\n",
       "label                                                            0\n",
       "input            Опитът на колективния Запад да „обезкърви Руси...\n",
       "new_start_end                                           (151, 156)\n",
       "sentences        [Опитът на колективния Запад да „обезкърви Рус...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ovo se vrti minutu i pol da se ne cudite\n",
    "def sentencizer(row):\n",
    "    sentences = 0\n",
    "    if row['lang'] == 'EN':\n",
    "        doc = nlp_en(row['input'])\n",
    "        sentences = [sentence.text for sentence in doc.sents]\n",
    "    if row['lang'] == 'HI':\n",
    "        doc = nlp_hi(row['input'])\n",
    "        sentences = [sentence.text for sentence in doc.sents]\n",
    "    if row['lang'] == 'BG':\n",
    "        doc = nlp_bg(row['input'])\n",
    "        sentences = [sentence.text for sentence in doc.sents]\n",
    "    if row['lang'] == 'PT':\n",
    "        doc = nlp_pt(row['input'])\n",
    "        sentences = [sentence.text for sentence in doc.sents]\n",
    "    return sentences\n",
    "df['sentences'] = df.apply(sentencizer,axis=1)\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb9e828-6f01-4efb-a43a-1541e26557f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang                                                            EN\n",
      "art_name                                          EN_UA_103861.txt\n",
      "entity                                                     Chinese\n",
      "start                                                          791\n",
      "end                                                            797\n",
      "class1                                                  Antagonist\n",
      "classes2                                                     [Spy]\n",
      "text             The World Needs Peacemaker Trump Again \\n\\n by...\n",
      "label                                                            0\n",
      "input            The World Needs Peacemaker Trump Again  by Jef...\n",
      "new_start_end                                           (785, 792)\n",
      "sentences        [The World Needs Peacemaker Trump Again  by Je...\n",
      "new_ind                                                          5\n",
      "Name: 448, dtype: object\n",
      "TRUTH LIVES on at https://sgtreport.tv/ There has been an astounding 6,300% increase in the number of Chinese nationals illegally entering the country in the last few years.\n"
     ]
    }
   ],
   "source": [
    "def sent_id(row):\n",
    "    sents = row['sentences']\n",
    "    inp = row['input']\n",
    "    start = row['new_start_end'][0]\n",
    "    ind = 0\n",
    "    ind_s = -1\n",
    "    for s in range(len(sents)):\n",
    "        for i in range(len(sents[s])):\n",
    "            ind+=1\n",
    "            if ind == start:\n",
    "                ind_s = s\n",
    "                break\n",
    "        if ind_s > -1:\n",
    "            break\n",
    "\n",
    "\n",
    "    if ind_s >= len(sents):\n",
    "        print(\"WTF\",ind_s,len(sents))\n",
    "    return ind_s\n",
    "\n",
    "df['new_ind'] = df.apply(sent_id,axis=1)\n",
    "print(df.loc[448])\n",
    "nid = df['new_ind'].loc[448]\n",
    "sent = df['sentences'].loc[448]\n",
    "print(sent[nid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917cd08c-ea33-40bd-8f25-7ac678b15e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this rate continues, this year’s total will easily top the 24,125 Chinese nationals who illegally entered the country last year.TRUTH LIVES on at https://sgtreport.tv/ There has been an astounding 6,300% increase in the number of Chinese nationals illegally entering the country in the last few years.Since China is a communist nation, these individuals are not freely traveling to the United States for a “better life.” TRUTH LIVES on at https://sgtreport.tv/ There has been an astounding 6,300% increase in the number of Chinese nationals illegally entering the country in the last few years.\n"
     ]
    }
   ],
   "source": [
    "def sent_input(row):\n",
    "    nind = row['new_ind']\n",
    "    sents = row['sentences']\n",
    "    inp = \"\"\n",
    "    if nind - 1 >= 0:\n",
    "        inp+=sents[nind-1]\n",
    "    inp+=sents[nind]\n",
    "    if nind + 1 < len(sents):\n",
    "        inp+=sents[nind+1]\n",
    "    return inp\n",
    "def sent_input_short(row):\n",
    "    nind = int(row['new_ind'])\n",
    "    sents = row['sentences']\n",
    "    inp = \"\"\n",
    "    if nind >= len(sents) :\n",
    "        print(nind,len(sents))\n",
    "    inp+=sents[nind]\n",
    "    return inp\n",
    "df['context'] = df.apply(sent_input,axis=1)\n",
    "df['no_context'] = df.apply(sent_input_short,axis=1)\n",
    "\n",
    "print(df['context'].loc[448],\n",
    "    df['no_context'].loc[448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04890d81-4977-43c5-bc9e-42e28232fb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814     Biden is a compromised puppet politician who h...\n",
       " 937     मास्को और वॉशिंगटन के बीच संतुलन बनाना अमेरिका...\n",
       " 728     The former head of German intelligence has rev...\n",
       " 579     Speaking in the Estonian parliament on January...\n",
       " 2392    O membro do alto escalão do Daesh (organização...\n",
       " Name: no_context, dtype: object,\n",
       " 2109    O porta-voz do Kremlin (presidência russa), Dm...\n",
       " 676     But his unconvincing 'no' will not stop Ayatol...\n",
       " 946     इस संचार युद्ध में रूस कटघरे में खड़ा हो चुका है।\n",
       " 1       Подкрепата на САЩ, НАТО и други западни съюзни...\n",
       " 1697    यूक्रेन के रूसी आक्रमण के 42 वें दिन जारी की ग...\n",
       " Name: no_context, dtype: object,\n",
       " 814     0\n",
       " 937     0\n",
       " 728     0\n",
       " 579     2\n",
       " 2392    0\n",
       " Name: label, dtype: int64,\n",
       " 2109    0\n",
       " 676     0\n",
       " 946     2\n",
       " 1       0\n",
       " 1697    1\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[\"no_context\"]\n",
    "y = df[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=22\n",
    ")\n",
    "X_train.head(), X_test.head(), y_train.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2400d14-5c0c-4e2b-ac25-1f665a1c176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71343425-53fc-4d3b-8c55-a02aebe8c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cea88d08-6bbc-45dc-8731-e2e90424ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3).to(device)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65fc1e5f-313c-4748-9c32-e381b7d05857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas DataFrame/Series to Hugging Face Dataset\n",
    "train_data = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "test_data = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7fbb381-de38-43de-8119-a8e32473f718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9016c2e16b0488497851994cbd24a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79be5eccb5a448e8837c16eeeb8f3acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71e5f1f2-ef14-4d60-aa32-d63ce6de5169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2154\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112c7c98-41d7-4a43-a902-0419deb5c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c462ec-f82e-4f7d-8a02-4f60a2da4de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:27<00:00,  1.22it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss: 1.0441, Training accuracy: 0.4777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.90it/s, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0120, Test accuracy: 0.5039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:27<00:00,  1.21it/s, loss=0.976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Training loss: 0.9546, Training accuracy: 0.5747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.94it/s, loss=0.788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8456, Test accuracy: 0.6352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:28<00:00,  1.21it/s, loss=0.572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Training loss: 0.8373, Training accuracy: 0.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 3/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  4.00it/s, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8318, Test accuracy: 0.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:27<00:00,  1.22it/s, loss=0.748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Training loss: 0.7659, Training accuracy: 0.6871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 4/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  4.01it/s, loss=0.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8306, Test accuracy: 0.6273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:27<00:00,  1.22it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Training loss: 0.6580, Training accuracy: 0.7405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5/5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.97it/s, loss=0.704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8012, Test accuracy: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Initialize tqdm progress bar for training\n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        #print(preds)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Update tqdm description with current loss\n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Test phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test_predictions = 0\n",
    "    total_test_predictions = 0\n",
    "    \n",
    "    # Initialize tqdm progress bar for test\n",
    "    test_progress_bar = tqdm(test_dataloader, desc=f\"Test Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate test accuracy\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct_test_predictions += (preds == labels).sum().item()\n",
    "            total_test_predictions += labels.size(0)\n",
    "            \n",
    "            # Update tqdm description with current loss\n",
    "            test_progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    test_accuracy = correct_test_predictions / total_test_predictions\n",
    "    \n",
    "    print(f\"Test loss: {avg_test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
